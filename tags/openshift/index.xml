<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenShift on Frank's Notes</title><link>https://blog.yylin.io/tags/openshift/</link><description>Recent content in OpenShift on Frank's Notes</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><lastBuildDate>Sat, 24 Sep 2022 21:40:40 +0800</lastBuildDate><atom:link href="https://blog.yylin.io/tags/openshift/index.xml" rel="self" type="application/rss+xml"/><item><title>單節點 OpenShift 部署</title><link>https://blog.yylin.io/openshift/sno-installer/</link><pubDate>Sat, 24 Sep 2022 21:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/sno-installer/</guid><description>&lt;p>OpenShift 4.9 開始正式推出提供單節點部署（Single Node OpenShift，SNO），以支援小型、全功能的企業級Kubernetes叢集的應用，常見客戶 POC 應用需求如資料中心伺服器資源有限情況下，單節點 OpenShift 部署型態能夠更容易處理，可協助企業擴充既有應用程式開發與部署規模，以及管理相關工作流程，更支援邊緣資料資料中心的執行需求。&lt;/p>
&lt;p>單節點部署可以使用 RHACM 或者在線的安裝引導進行安装，此篇以安裝引導進行 SNO 部署說明：&lt;/p>
&lt;h2 id="1-準備好本地的安裝環境">1. 準備好本地的安裝環境：&lt;/h2>
&lt;ul>
&lt;li>本文章使用 &lt;a class="link" href="https://access.redhat.com/zh_CN/content/4218151" target="_blank" rel="noopener"
>Red Hat Virtualization&lt;/a>虛擬化環境，部署 SNO 叢集的最低配置要求如 &lt;code>Prerequisite&lt;/code> 下表資訊&lt;/li>
&lt;li>生成 SSH 密鑰(用於SSH登陸)&lt;/li>
&lt;li>配置部署的虛擬機 IP 或使用DHCP配置叢集網路及 DNS 伺服器解析&lt;/li>
&lt;/ul>
&lt;h3 id="prerequisite">Prerequisite&lt;/h3>
&lt;p>&lt;strong>主機資源要求&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;th>Disk&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8vCPU&lt;/td>
&lt;td>16GB&lt;/td>
&lt;td>120GB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>Requirements for installing OpenShift on a single node - &lt;a class="link" href="https://docs.openshift.com/container-platform/4.10/installing/installing_sno/install-sno-preparing-to-install-sno.html#install-sno-requirements-for-installing-on-a-single-node_install-sno-preparing" target="_blank" rel="noopener"
>minimum resource requirements&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h3 id="dns-設定">&lt;strong>DNS 設定&lt;/strong>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Usage&lt;/th>
&lt;th>FQDN&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kubernetes API&lt;/td>
&lt;td>api.&amp;lt;cluster_name&amp;gt;.&amp;lt;base_domain&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Internal API&lt;/td>
&lt;td>api-int.&amp;lt;cluster_name&amp;gt;.&amp;lt;base_domain&amp;gt;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ingress route&lt;/td>
&lt;td>*.apps.&amp;lt;cluster_name&amp;gt;.&amp;lt;base_domain&amp;gt;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br/>
&lt;h2 id="2-登入安裝-consoleredhatcomhttpsconsoleredhatcom-創建-openshift-單節點部署">2. 登入安裝 &lt;a class="link" href="https://console.redhat.com/" target="_blank" rel="noopener"
>console.redhat.com&lt;/a>, 創建 OpenShift 單節點部署&lt;/h2>
&lt;h3 id="installation---assisted-install">Installation - Assisted Install&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>開啟 &lt;a class="link" href="https://console.redhat.com" target="_blank" rel="noopener"
>https://console.redhat.com&lt;/a>，選擇 OpenShift&lt;/strong>
&lt;img src="https://i.imgur.com/Qzz1mj4.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>點擊上方 Create Cluster&lt;/strong>
&lt;img src="https://i.imgur.com/mNKB6GQ.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>選擇 Datacenter 並點擊 Create Cluster&lt;/strong>
&lt;img src="https://i.imgur.com/tGSYnXs.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>輸入 Cluster 相關參數，勾選 Install single node OpenShift&lt;/strong>
&lt;img src="https://i.imgur.com/oPwZ5kx.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Host network 部分選擇 Static network configuration&lt;/strong>
&lt;img src="https://i.imgur.com/wV0rcAv.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>輸入相關網路參數&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>此部分範例設置為 192.168.10.0/24 網段&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://i.imgur.com/SgPDkJ5.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/X9XE6Ev.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>點選 Add host&lt;/strong>
&lt;img src="https://i.imgur.com/vhYpStz.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>選擇 Minimal image file，並輸入對應 ssh public key 內容&lt;/strong>
&lt;img src="https://i.imgur.com/zJwFOr9.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>下載 Discovery ISO&lt;/strong>
&lt;img src="https://i.imgur.com/1N1LkT7.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>啟動 VM 並掛載該 ISO，等待其自動安裝與設定，完成後應該看到 Host Inventory 顯示如下&lt;/strong>
&lt;img src="https://i.imgur.com/QngzQ70.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>針對錯誤部分進行修改，此處問題為 hostname 不能為 localhost&lt;/strong>
&lt;img src="https://i.imgur.com/vEYp79z.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/TI0hOOP.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>確認沒問題後即可進行下一步&lt;/strong>
&lt;img src="https://i.imgur.com/yzBx0w6.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>最後檢查一遍設定，確認無誤後點擊 Install cluster&lt;/strong>
&lt;img src="https://i.imgur.com/sxKA3O1.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>等待安裝完畢&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>單節點安裝時間大約 40 分鐘&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://i.imgur.com/rE8Ykdw.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;img src="https://i.imgur.com/5hl2Gqc.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>安裝完成後即可根據以下連線資訊使用 Single Node OCP&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i.imgur.com/Bvn1ERs.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>最終登入 OCP 查看狀態&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i.imgur.com/tZG1QkE.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/BidpalP.jpg"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>補充 Youtube 安裝參考：
&lt;div class="video-wrapper">
&lt;iframe loading="lazy"
src="https://www.youtube.com/embed/QFf0yVAHQKc"
allowfullscreen
title="YouTube Video"
>
&lt;/iframe>
&lt;/div>
&lt;/li>
&lt;/ul>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://www.youtube.com/watch?v=QFf0yVAHQKc&amp;amp;ab_channel=OpenShift" target="_blank" rel="noopener"
>Demo: How to try out single-node OpenShift from Red Hat&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.openshift.com/container-platform/4.10/installing/installing_sno/install-sno-preparing-to-install-sno.html" target="_blank" rel="noopener"
>Preparing to install on a single node&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.openshift.com/container-platform/4.10/installing/installing_sno/install-sno-installing-sno.html" target="_blank" rel="noopener"
>OpenShift 4.10 - Installing OpenShift on a single node&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.youtube.com/watch?v=PE8W8OKJoXc&amp;amp;ab_channel=OpenShift" target="_blank" rel="noopener"
>OpenShift Virtualization on a Single Node Cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.redhat.com/en/blog/meet-single-node-openshift-our-smallest-openshift-footprint-edge-architectures" target="_blank" rel="noopener"
>Meet single node OpenShift: Our newest small OpenShift footprint for edge architectures&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lees07/tech-docs/blob/master/e1-sno-by-assisted-installer.md" target="_blank" rel="noopener"
>https://github.com/lees07/tech-docs/blob/master/e1-sno-by-assisted-installer.md&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>OpenShift 啟用 GPU Operator Dashboard</title><link>https://blog.yylin.io/openshift/gpu-utilization/</link><pubDate>Tue, 28 Jun 2022 21:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/gpu-utilization/</guid><description>&lt;h2 id="先決條件">先決條件：&lt;/h2>
&lt;ul>
&lt;li>Bastion 安裝 Helm&lt;/li>
&lt;li>當前 OpenShift 版本為 4.10+&lt;/li>
&lt;li>Nvidia GPU Operator 已經完成安裝&lt;/li>
&lt;/ul>
&lt;h2 id="啟用-nvidia-gpu-operator-usage-資訊">啟用 NVIDIA GPU Operator Usage 資訊&lt;/h2>
&lt;ol>
&lt;li>添加 helm repo:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ helm repo add rh-ecosystem-edge https://rh-ecosystem-edge.github.io/console-plugin-nvidia-gpu
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="2">
&lt;li>更新 repo:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">$ helm repo update
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="3">
&lt;li>安裝 &lt;code>helm chart&lt;/code> 於預設 NVIDIA GPU Operator namespace:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ helm install -n nvidia-gpu-operator console-plugin-nvidia-gpu rh-ecosystem-edge/console-plugin-nvidia-gpu
$ kubectl -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu
# 啟用 plugin 執行以下 command:
$ kubectl patch consoles.operator.openshift.io cluster --patch &amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/plugins/-&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;console-plugin-nvidia-gpu&amp;#34; }]&amp;#39; --type=json
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="4">
&lt;li>查看部署的資源：&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="5">
&lt;li>驗證 plugins 是否已指定&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc get consoles.operator.openshift.io cluster --output=jsonpath=&amp;#34;{.spec.plugins}&amp;#34;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>如果未指定，則運行以下 command 以啟用 plugin：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc patch consoles.operator.openshift.io cluster --patch &amp;#39;{ &amp;#34;spec&amp;#34;: { &amp;#34;plugins&amp;#34;: [&amp;#34;console-plugin-nvidia-gpu&amp;#34;] } }&amp;#39; --type=merge
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>如果指定，則運行以下 command 以啟用 plugin：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc patch consoles.operator.openshift.io cluster --patch &amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/plugins/-&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;console-plugin-nvidia-gpu&amp;#34; }]&amp;#39; --type=json
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在 OCP Web Console 頁面中（Home &amp;gt; Overview）就可以查閱 GPU utilization:&lt;/p>
&lt;p>&lt;img src="https://docs.nvidia.com/datacenter/cloud-native/_images/gpu_overview_dashboard1.png"
loading="lazy"
>&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/enable-gpu-op-dashboard.html" target="_blank" rel="noopener"
>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/enable-gpu-op-dashboard.html&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>在 ACM 透過 OpenShift-GitOps/ArgoCD 管理應用程式</title><link>https://blog.yylin.io/openshift/argocd-and-acm/</link><pubDate>Sat, 25 Jun 2022 09:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/argocd-and-acm/</guid><description>&lt;h2 id="概述">概述&lt;/h2>
&lt;p>從 ACM 2.5 版開始正式 GA 功能，您可以透過 ApplicationSets 來配置 ArgoCD / OpenShift-GitOps ，通過單一管理平台以可擴展的方式管理您的所有 GitOps Applications。&lt;/p>
&lt;p>ApplicationSet Controller 是一個 Kubernetes Controller，它增加對 ApplicationSet CustomResourceDefinition (CRD) 的支持。&lt;/p>
&lt;p>ApplicationSet Controller 在與 Argo CD 一起安裝時，通過添加額外的功能來支持以叢集管理員為中心的場景來補充它。&lt;/p>
&lt;p>ApplicationSet Ccontroller 提供：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>使用單個 Kubernetes 清單通過 Argo CD 從一個或多個 Git 存儲庫部署多個 Application 的能力&lt;/p>
&lt;/li>
&lt;li>
&lt;p>改進了對 &lt;a class="link" href="https://blog.maxkit.com.tw/2017/09/monorepos.html" target="_blank" rel="noopener"
>monorepos&lt;/a> 的支持：在 Argo CD 的上下文中，monorepo 是在單個 Git 存儲庫中定義的多個 Argo CD Application 資源&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>我們將介紹如何將 ACM 與 OpenShift GitOps 的 ApplicationSets 連接起來，以便在託管 Cluster 中配置和部署 OpenShift GitOps Application 和 ApplicationSet。&lt;/p>
&lt;h2 id="環境配置-先決條件">環境配置-先決條件&lt;/h2>
&lt;ul>
&lt;li>我們需要使用 Operator Hub 在 ACM Hub 那座 OpenShift Cluster 中安裝 OpenShift GitOps：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ until oc apply -k https://github.com/RedHat-EMEA-SSA-Team/ns-gitops/tree/bootstrap/bootstrap ; do sleep 2; done
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>
&lt;p>參考 &lt;a class="link" href="https://docs.openshift.com/container-platform/4.9/cicd/gitops/installing-openshift-gitops.html" target="_blank" rel="noopener"
>OpenShift GitOps 的官方文件說明&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>另一方面，我們需要配置管理不同的 Cluster (e.g. Public Cloud)。在我的例子中，我使用我環境中部署 2 座 OCP Cluster 叢集，並將在這篇文章中用於部署我的 Application。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="於-openshift-gitops--argocd-配置託管叢集">於 OpenShift GitOps / ArgoCD 配置託管叢集&lt;/h2>
&lt;p>要在 ACM 中配置和鏈接 OpenShift GitOps，我們可以將一組一個或多個託管 Cluster 註冊到 Argo CD 或 OpenShift GitOps Operator 的實例。&lt;/p>
&lt;p>註冊後，我們可以使用從 ACM Hub Application Controller 的 Application 和 ApplicationSets 將我們需要部署的應用程式部署到這些叢集。然後，我們可以設置一個連續的 GitOps 環境，以在開發、暫存和生產環境中跨叢集自動化配置 Application 的一致性。&lt;/p>
&lt;ul>
&lt;li>首先，我們需要創建 cluster sets 並將託管 clusters 添加到這些 cluster sets：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ cat acmgitops/managedclusterset.yaml
apiVersion: cluster.open-cluster-management.io/v1alpha1
kind: ManagedClusterSet
metadata:
name: all-openshift-clusters
spec: {}
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>
&lt;p>將託管叢集作為導入 Cluster 添加到 ClusterSet。您可以使用 &lt;a class="link" href="https://github.com/open-cluster-management/rhacm-docs/blob/2.4_stage/clusters/managedclustersets.adoc#creating-a-managedclustersetbinding-by-using-the-console" target="_blank" rel="noopener"
>ACM Console&lt;/a> 或 &lt;a class="link" href="https://github.com/open-cluster-management/rhacm-docs/blob/2.4_stage/clusters/managedclustersets.adoc#adding-clusters-to-a-managedclusterset-by-using-the-command-line" target="_blank" rel="noopener"
>CLI&lt;/a> 導入：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>創建託管 Cluster 綁定到部署 Argo CD 或 OpenShift GitOps 的 namespace&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ cat managedclustersetbinding.yaml
apiVersion: cluster.open-cluster-management.io/v1alpha1
kind: ManagedClusterSetBinding
metadata:
name: all-openshift-clusters
namespace: openshift-gitops
spec:
clusterSet: all-openshift-clusters
$ oc apply -f managedclustersetbinding.yaml
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>在託管叢集綁定中使用的 namespace 中，創建放置自定義資源以選擇一組託管叢集以註冊到 ArgoCD 或 OpenShift GitOps Operator instance：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiVersion: cluster.open-cluster-management.io/v1alpha1
kind: Placement
metadata:
name: all-openshift-clusters
namespace: openshift-gitops
spec:
predicates:
- requiredClusterSelector:
labelSelector:
matchExpressions:
- key: vendor
operator: &amp;#34;In&amp;#34;
values:
- OpenShift
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>注意：只有 OpenShift 叢集註冊到 Argo CD 或 GitOps Operator instance，而不是其他 Kubernetes 叢集。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>創建一個 GitOpsCluster 自定義資源以將託管叢集從放置註冊到 Argo CD 或 OpenShift GitOps 的指定的 instance：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiVersion: apps.open-cluster-management.io/v1alpha1
kind: GitOpsCluster
metadata:
name: argo-acm-clusters
namespace: openshift-gitops
spec:
argoServer:
cluster: local-cluster
argoNamespace: openshift-gitops
placementRef:
kind: Placement
apiVersion: cluster.open-cluster-management.io/v1alpha1
name: all-openshift-clusters
namespace: openshift-gitops
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>這使 Argo CD instance 能夠將 Application 部署到任何 ACM Hub 託管叢集中。&lt;/p>
&lt;p>正如我們從前面的示例中看到的，placementRef.name 被定義為 &lt;code>all-openshift-clusters&lt;/code>，並被指定為安裝在 argoNamespace：openshift-gitops 中的 GitOps 實例的目標叢集。&lt;/p>
&lt;p>另一方面，argoServer.cluster 規範需要 &lt;code>local-cluster&lt;/code> 值，因為將使用部署在 OpenShift 叢集中的 OpenShift GitOps，該叢集也是安裝 ACM Hub 的位置。&lt;/p>
&lt;ul>
&lt;li>幾分鐘後，我們在 ACM Hub 中生成了 GitOps Cluster CRD，我們將能夠直接從 Application 部分的 ACM Hub 控制台定義 Application 和ApplicationSet。&lt;/li>
&lt;/ul>
&lt;h2 id="從-acm-hub-部署-argocdopenshift-gitops-applicationset">從 ACM Hub 部署 ArgoCD/OpenShift GitOps ApplicationSet&lt;/h2>
&lt;p>一旦我們通過 ACM Hub 中的 GitOps Cluster CRD 啟用了 OpenShift GitOps 和 ACM 之間的整合，我們就可以直接在 ACM Hub 中部署 ApplicationSet，在一個單一的頁面中管理所有 ArgoCD Application。&lt;/p>
&lt;p>另一方面，我們還將受益於&lt;a class="link" href="https://argocd-applicationset.readthedocs.io/en/stable/Generators/" target="_blank" rel="noopener"
>ArgoCD ApplicationSets 的不同生成器的特性&lt;/a>。&lt;/p>
&lt;p>使用這些生成方式，我們可以從不同叢集中的單個 Repository 部署多個 Application，利用 ApplicationSet 為每個託管的 Cluster 的從Repository 中的配置不同對象及要部署的 Application。&lt;/p>
&lt;p>讓我們在 ACM Hub 中生成 ApplicationSet。&lt;/p>
&lt;ul>
&lt;li>使用 UI 為Application 集生成一個 ApplicationSet 示例：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappA.png"
loading="lazy"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
name: acm-appsets
namespace: openshift-gitops
spec:
generators:
- clusterDecisionResource:
configMapRef: acm-placement
labelSelector:
matchLabels:
cluster.open-cluster-management.io/placement: acm-appsets-placement
requeueAfterSeconds: 180
template:
metadata:
name: &amp;#39;acm-appsets-&amp;#39;
spec:
destination:
namespace: bgdk
server: &amp;#39;&amp;#39;
project: default
source:
path: apps/bgd/overlays/bgdk
repoURL: &amp;#39;https://github.com/yylin1/ns-apps/&amp;#39;
targetRevision: single-app
syncPolicy:
automated:
prune: true
selfHeal: true
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意：目標 namespace 可以是 openshift-gitops。BGDK 可能會更改，但它會以這種方式離開，因為我們需要放置一個目標 namespace，即ApplicationSet 本身不需要它（Applicatio bgdk 也不需要）&lt;/p>
&lt;ul>
&lt;li>結果是在 OpenShift GitOps 中生成但由 ACM Hub 管理的 ApplicationSet：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappB.png"
loading="lazy"
>&lt;/p>
&lt;p>正如我們所看到的，被分配到兩個不同的 Cluster ，&lt;code>bm-germany&lt;/code> 這 &lt;code>local-cluster&lt;/code> 將是 Application 部署的地方，由 ApplicationSet 管理&lt;/p>
&lt;p>Application 在之前定義 ApplicationSet 期間為每個叢集生成了與定義為 acm-appsets-placement 的 Placement 匹配的 ApplicationSet。還可以匹配 Cluster 進行 label，而不僅僅依賴於 Placement 對象。&lt;/p>
&lt;ul>
&lt;li>在生成的 Application 中，每個Application都有自己的Application、Placement 和 Cluster，我們可以檢查：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappC.png"
loading="lazy"
>&lt;/p>
&lt;p>因為我們可以檢查 ArgoCD Application 是否正確部署並由 BM-Germany 叢集中的 ACM AppSets 的 ApplicationSet 自動管理。此外，另一個 ArgoCD Application 將用於在與 Placement 匹配的另一個 Cluster 中部署另一個 Application 。&lt;/p>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappD.png"
loading="lazy"
>&lt;/p>
&lt;p>正如我們之前所描述的，兩個 ArgoCD Application 是由與定義的 Placement 匹配的 ApplicationSet 生成的。&lt;/p>
&lt;ul>
&lt;li>在 OpenShift GitOps / ArgoCD argo-controller 實例中，ACM 生成的 ApplicationSet 也生成了兩個 Argo Application ，並且為與 Placement 匹配的 ClusterSet 中管理的每個 Cluster 生成了每個 ArgoCD Application ：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappE.png"
loading="lazy"
>&lt;/p>
&lt;blockquote>
&lt;p>注意：檢查指向在早期步驟中定義的不同託管 Cluster 的目標。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>每個 Argo ApplicationSet 管理每個託管 Cluster 中的Application ，例如在 BM-Germany Cluster 中部署 BGDK Application 。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappF.png"
loading="lazy"
>&lt;/p>
&lt;p>此 Application 將在託管 Cluster 中部署Application 清單，在這種情況下部署 bgdk Application 清單（路由、服務、部署等）。&lt;/p>
&lt;ul>
&lt;li>在 ArgoCD/OpenShift GitOps 的設置中，在 ACM 使用 ClusterSet 管理的這些叢集。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/acmappG.png"
loading="lazy"
>&lt;/p>
&lt;p>這些是由 ACM Hub 中生成的 GitOps CRD 自動生成和管理的，它與託管 Cluster 對應。&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/RedHat-EMEA-SSA-Team/ns-gitops/" target="_blank" rel="noopener"
>https://github.com/RedHat-EMEA-SSA-Team/ns-gitops/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://rcarrata.com/openshift/argo-and-acm/" target="_blank" rel="noopener"
>https://rcarrata.com/openshift/argo-and-acm/&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>CI/CD: Tekton Pipeline 實戰</title><link>https://blog.yylin.io/openshift/pipelines/</link><pubDate>Tue, 21 Jun 2022 21:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/pipelines/</guid><description>&lt;p>OpenShift Pipelines 是一個基於 Kubernetes 資源的雲塊的持續和持續交付（持續集成和持續交付，簡稱 CI/CD）的解決方案。它通過執行執行的細節，使用 Tekton 進行跨平台的自動部署。Tekton 引入了多種標準的自定義資源定義 (CRD)，定義可跨 Kubernetes 分佈用於 CI/CD 管道。&lt;/p>
&lt;p>&lt;img src="https://www.redhat.com/cms/managed-files/container-platforms-pipelines.png"
loading="lazy"
>&lt;/p>
&lt;h2 id="主要特性">主要特性&lt;/h2>
&lt;ul>
&lt;li>OpenShift Pipelines 是一個無服務器的 CI/CD 系統，它在獨立的容器中運行 Pipelines，以及所有需要的依賴組件。&lt;/li>
&lt;li>OpenShift Pipelines 是為開發微服務架構的非中心化團隊設計的。&lt;/li>
&lt;li>OpenShift Pipelines 使用標準 CI（pipeline）定義，這些與現有的 Kubernetes 工具集成擴展可擴展和擴展，可讓您定義和擴展 Kubernetes。&lt;/li>
&lt;li>您可以通過 OpenShift Pipelines 使用 Kubernetes （如 Source-to-Image (S2I)、Buildah、Buildpacks 和 Kaniko）構建鏡像，這些工具可以移植到任何 Kubernetes 平台。&lt;/li>
&lt;li>您可以使用 OpenShift Container Platform 開發運行（Developer Console）來創建 Tekton 資源，查看 Pipeline 的日誌，並管理 OpenShift Container Platform 設計空間中的管道。&lt;/li>
&lt;/ul>
&lt;p>在 Tekton pipeline 中有以下幾個主要的組成要素，分別是：&lt;/p>
&lt;ul>
&lt;li>PipelineResource&lt;/li>
&lt;li>Task &amp;amp; ClusterTask&lt;/li>
&lt;li>TaskRun&lt;/li>
&lt;li>Pipeline&lt;/li>
&lt;li>PipelineRun&lt;/li>
&lt;/ul>
&lt;h2 id="pipelineresource">PipelineResource&lt;/h2>
&lt;p>PipelineResource 簡單來說可以作為 task 的 input or output，而每個 task 可以有多個 input &amp;amp; output。&lt;/p>
&lt;h2 id="syntax">Syntax&lt;/h2>
&lt;p>PipelineResource 的定義中會有以下必要資訊：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>apiVersion：目前固定是 tekton.dev/v1alpha1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kind：因為這是 CRD，所以是 PipelineResource&lt;/p>
&lt;/li>
&lt;li>
&lt;p>metadata：用來辨識此 TaskRun 用的資訊，例如 name&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sepc：使用 resource 的詳細資訊(例如：路徑、位址)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>type：用來指定 resource type，目前支援 git, pullRequest, image, cluster, storage, cloudevent … 等等&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>其他選填項目：&lt;/p>
&lt;ul>
&lt;li>params：不同的 resource type 可能會有的不同額外參數資訊&lt;/li>
&lt;/ul>
&lt;h2 id="resource-type">Resource Type&lt;/h2>
&lt;p>有了以上概念後，接著要知道的是 PipelineResources 共有以下幾種類型：&lt;/p>
&lt;ul>
&lt;li>Git Resource&lt;/li>
&lt;li>Pull Request Resource&lt;/li>
&lt;li>Image Resource&lt;/li>
&lt;li>Cluster Resource&lt;/li>
&lt;li>Storage Resource&lt;/li>
&lt;li>Cloud Event Resource&lt;/li>
&lt;/ul>
&lt;p>以下就針對比較常用的 Git &amp;amp; Image resource 說明，其他的部份可以參考官網的詳細文件。&lt;/p>
&lt;h3 id="git-resource">Git Resource&lt;/h3>
&lt;p>一般的 git repository，作為 task input 時，Tekton 執行 task 前會將程式碼 clone 回來，因此這邊就必須注意 git repository 存取的權限問題，若是 private repository 就要額外提供 credential 資訊才可以正常運作&lt;/p>
&lt;p>以下是一個標準的 Git PipelineResource 的定義：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
name: wizzbang-git
namespace: default
spec:
type: git
params:
- name: url
value: https://github.com/wizzbangcorp/wizzbang.git
# 可用 branch, tag, commit SHA or ref
# 沒指定就會拉 master branch
- name: revision
value: master
# value: some_awesome_feature
# value: refs/pull/52525/head
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="task--clustertask">Task &amp;amp; ClusterTask&lt;/h2>
&lt;p>&lt;code>Task&lt;/code>(&amp;amp; &lt;code>ClusterTask&lt;/code>) 中包含了一連串的 step，通常是使用者要用來執行 CI flow，而這些工作會在單一個 pod 中以多個 container 的形式逐一完成。&lt;/p>
&lt;p>Task &amp;amp; ClusterTask 兩者的不同在於 Task 是屬於 namespace level，而 ClusterTask 是屬於 cluster level&lt;/p>
&lt;p>而在 Task 的定義中，最重要的部份有以下三個項目：&lt;/p>
&lt;ul>
&lt;li>Input&lt;/li>
&lt;li>Output&lt;/li>
&lt;li>Steps&lt;/li>
&lt;/ul>
&lt;p>以下是一個 task 的標準定義內容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
name: deploy-using-kubectl
spec:
inputs:
resources:
- name: source
type: git
- name: image
type: image
params:
- name: path
type: string
description: Path to the manifest to apply
- name: yamlPathToImage
type: string
description:
The path to the image to replace in the yaml manifest (arg to yq)
steps:
# step 中可以定義多個執行工作，會依照順序執行
- name: replace-image
image: mikefarah/yq
command: [&amp;#34;yq&amp;#34;]
args:
- &amp;#34;w&amp;#34;
- &amp;#34;-i&amp;#34;
- &amp;#34;$(inputs.params.path)&amp;#34;
- &amp;#34;$(inputs.params.yamlPathToImage)&amp;#34;
- &amp;#34;$(inputs.resources.image.url)&amp;#34;
- name: run-kubectl
image: lachlanevenson/k8s-kubectl
command: [&amp;#34;kubectl&amp;#34;]
args:
- &amp;#34;apply&amp;#34;
- &amp;#34;-f&amp;#34;
- &amp;#34;$(inputs.params.path)&amp;#34;
# 此 volume 在 task 中沒有用到，只是一個範例而已
# 用以表示可以在 task 中定義 volume 並使用
volumes:
- name: example-volume
emptyDir: {}
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="taskrun">TaskRun&lt;/h2>
&lt;p>定義了 task 之後，Tekton 並不會主動執行任何 task，這時候就必須要搭配 TaskRun 才可以讓 task 真正的執行指定工作。&lt;/p>
&lt;p>以下是一個標準的 TaskRun 定義：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback"># 以下幾個(apiVersion, kind, metadata, spec)是必要資訊
apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
name: build-docker-image-from-git-source-task-run
spec:
serviceAccount: robot-docker-basic
# 指定到已經預先定義好的 Task
taskRef:
name: build-docker-image-from-git-source
inputs:
resources:
- name: docker-source
# 指定到已經預先定義好的 PipelineResource
resourceRef:
name: git-tekton-test
params:
- name: pathToDockerFile
value: Dockerfile
- name: pathToContext
value: /workspace/docker-source/examples/microservices/leeroy-web
outputs:
resources:
- name: builtImage
resourceRef:
name: image-tekton-test
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="pipeline">Pipeline&lt;/h2>
&lt;p>Pipeline 其實可以把它簡單思考為前面 Task 的集合，有順序性的排列，並透過之後介紹的 PipelineRun 來運作。&lt;/p>
&lt;p>成功運行 Pipeline 結果:&lt;/p>
&lt;p>&lt;img src="https://www.redhat.com/architect/sites/default/files/styles/embed_large/public/2022-06/4-pipeline.png?itok=Eex7RxG9"
loading="lazy"
>&lt;/p>
&lt;p>失敗運行 Pipeline 結果：&lt;/p>
&lt;p>&lt;img src="https://www.redhat.com/architect/sites/default/files/styles/embed_large/public/2022-06/5-ci-dev-pipeline-failed.png?itok=dzFYWfKl"
loading="lazy"
>&lt;/p>
&lt;h2 id="結語">結語&lt;/h2>
&lt;p>以上內容(PipelineResource, Task, TaskRun, Pipeline, PipelineRun) 是 Tekton 中執行工作的必要元素，實際上執行的 CI/CD 工作都會與這幾個部份有關。&lt;/p>
&lt;p>Tekton 將所有的基本元素拆分成一個一個的 k8s CRD(Custom Resource Definition)，如果是稍微複雜一點的 CI/CD 工作，可能就會需要定義不少個 CRD 才能完成，而且在設計上相對於其他的 CI server(例如：GitLab CI, Drone CI)可能不是這麼直覺；但這樣的設計提供了以下優點：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>原生的 k8s 使用經驗，不需要額外學習其他語法&lt;/p>
&lt;/li>
&lt;li>
&lt;p>定義好的 CRD(PipelineResource, Task, Pipeline) 可以被重複利用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>原生整合 k8s&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>若是未來有考慮 workload 都跑在 k8s 上的使用者，在選擇 CI/CD 的工具時或許可以將 Tekton 考慮進行。&lt;/p>
&lt;p>接著可能會面臨到的問題可能是，如果希望作到 GitOps，光是以上項目好像不夠還有相關元件支援性。&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/tektoncd/pipeline/tree/main/examples" target="_blank" rel="noopener"
>pipeline/examples&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/tektoncd/pipeline/blob/main/docs/resources.md" target="_blank" rel="noopener"
>PipelineResources&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>從零開始快速建置 - Microsoft Azure Red Hat OpenShift (ARO)</title><link>https://blog.yylin.io/openshift/aro/</link><pubDate>Sun, 13 Mar 2022 21:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/aro/</guid><description>&lt;p>ARO4 深入探討 - Microsoft Azure Red Hat OpenShift 4&lt;/p>
&lt;p>讓我們深入挖掘！&lt;/p>
&lt;p>Microsoft Azure Red Hat OpenShift 服務支持部署完全託管的 OpenShift 集群。&lt;/p>
&lt;p>Azure Red Hat OpenShift 由 Red Hat 和 Microsoft 聯合設計、運營和支持，以提供集成的支持體驗。沒有虛擬機可以運行，也不需要打補丁。主節點、基礎架構和應用程序節點由 Red Hat 和 Microsoft 代表您進行修補、更新和監控。您的 Azure Red Hat OpenShift 集群已部署到您的 Azure 訂閱中，並包含在您的 Azure 賬單中。&lt;/p>
&lt;p>在 OpenShift 4 上部署 Azure Red Hat 時，整個集群都包含在一個虛擬網絡中。在這個虛擬網絡中，您的主節點和工作節點都位於各自的子網中。每個子網都使用一個內部負載均衡器和一個公共負載均衡器。&lt;/p>
&lt;p>這是有關 Azure Red Hat OpenShift 4 的官方圖表（可在 ARO4 Microsoft 頁面中找到）：&lt;/p>
&lt;p>&lt;img src="https://rcarrata.com/images/aro4-networking-diagram.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>關於 ARO4 部署和管理的網絡和資源的更多詳細信息，請查看&lt;a class="link" href="https://docs.microsoft.com/en-us/azure/openshift/concepts-networking#networking-components" target="_blank" rel="noopener"
>ARO 圖詳細信息 - 官方文檔&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>讓我們安裝我們的第一個 ARO4 集群！&lt;/p>
&lt;h2 id="azure-帳戶先決條件">Azure 帳戶先決條件&lt;/h2>
&lt;p>首先，我們需要在 Azure 帳戶中設置幾項內容，例如生成 ServicePrincipals、增加限制以及定義要使用的區域。&lt;/p>
&lt;ul>
&lt;li>按照&lt;a class="link" href="https://docs.openshift.com/container-platform/latest/installing/installing_azure/installing-azure-account.html" target="_blank" rel="noopener"
>配置 Azure 帳戶先決條件&lt;/a>來定義和分配適當的 RBAC 權限並增加對 Azure 帳戶的限制。&lt;/li>
&lt;/ul>
&lt;h2 id="為-aro-安裝配置-azure-基礎結構先決條件">為 ARO 安裝配置 Azure 基礎結構先決條件&lt;/h2>
&lt;p>當我們準備好上一步後，就可以在 Azure 中為我們的 ARO4 集群生成基礎資源先決條件了：&lt;/p>
&lt;ul>
&lt;li>定義 ARO4 安裝的基本參數：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">export LOCATION=eastus
export RESOURCEGROUP=aro-rg
export CLUSTER=rcarrata
export VNET_CIDR=&amp;#34;10.0.0.0/22&amp;#34;
export MASTER_SUBNET_CIDR=&amp;#34;10.0.0.0/23&amp;#34;
export WORKER_SUBNET_CIDR=&amp;#34;10.0.2.0/23&amp;#34;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>使用 az cli 登錄到 Azure：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az login
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意：當登錄彈出時，您需要在 Azure Dashboard 中使用您的憑據進行身份驗證。&lt;/p>
&lt;ul>
&lt;li>註冊資源提供者：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az provider register -n Microsoft.RedHatOpenShift --wait
az provider register -n Microsoft.Compute --wait
az provider register -n Microsoft.Storage --wait
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="建立-azure-red-hat-openshift-4-叢集">建立 Azure Red Hat OpenShift 4 叢集&lt;/h1>
&lt;h2 id="開始之前">開始之前&lt;/h2>
&lt;p>Azure Red Hat OpenShift 至少需要 40 個核心，才能建立和執行 OpenShift 叢集。 新 Azure 訂用帳戶的預設 Azure 資源配額不符合這項需求。 若要要求增加資源限制，請參閱標準配額：&lt;a class="link" href="https://docs.microsoft.com/zh-tw/azure/azure-portal/supportability/per-vm-quota-requests" target="_blank" rel="noopener"
>VM 系列的增加限制&lt;/a>。&lt;/p>
&lt;ul>
&lt;li>例如，若要檢查最小支援的虛擬機器系列 SKU 「標準 DSv3」的目前訂用帳戶配額：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">LOCATION=eastus
az vm list-usage -l $LOCATION \
--query &amp;#34;[?contains(name.value, &amp;#39;standardDSv3Family&amp;#39;)]&amp;#34; \
-o table
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">CurrentValue Limit LocalName
-------------- ------- --------------------------
40 1000 Standard DSv3 Family vCPUs
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="驗證權限">驗證權限&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">Azure CLI quickstart:
export GUID=hzdnk
export CLIENT_ID=bdf5480a-c661-451f-a2ce-81e448ce0ba8
export PASSWORD=Zz842xzC.7GWZS_xVZ3oQvg1~7PtoIcKVA
export TENANT=1ce7852f-dcf3-42bc-afe6-3bf81ab984fb
export SUBSCRIPTION=ede7f891-835c-4128-af5b-0e53848e54e7
export RESOURCEGROUP=openenv-hzdnk
curl -L https://aka.ms/InstallAzureCli | bash
az login --service-principal -u $CLIENT_ID -p $PASSWORD --tenant $TENANT
[
{
&amp;#34;cloudName&amp;#34;: &amp;#34;AzureCloud&amp;#34;,
&amp;#34;homeTenantId&amp;#34;: &amp;#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb&amp;#34;,
&amp;#34;id&amp;#34;: &amp;#34;ede7f891-835c-4128-af5b-0e53848e54e7&amp;#34;,
&amp;#34;isDefault&amp;#34;: true,
&amp;#34;managedByTenants&amp;#34;: [
{
&amp;#34;tenantId&amp;#34;: &amp;#34;b5ce0030-ec42-4a62-bc94-3025993e790c&amp;#34;
}
],
&amp;#34;name&amp;#34;: &amp;#34;RHPDS Subscription - OpenTLC Tenant&amp;#34;,
&amp;#34;state&amp;#34;: &amp;#34;Enabled&amp;#34;,
&amp;#34;tenantId&amp;#34;: &amp;#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb&amp;#34;,
&amp;#34;user&amp;#34;: {
&amp;#34;name&amp;#34;: &amp;#34;bdf5480a-c661-451f-a2ce-81e448ce0ba8&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;servicePrincipal&amp;#34;
}
}
]
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>創建資源組為 ARO4 對象（如 vnet 和子網，以及自己的 ARO4 對象）分配資源：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az group create --name $RESOURCEGROUP --location $LOCATION
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>創建虛擬網絡：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az network vnet create --resource-group $RESOURCEGROUP \
--name aro-vnet --address-prefixes $VNET_CIDR
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>為主節點添加一個空子網：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az network vnet subnet create \
--resource-group $RESOURCEGROUP \
--vnet-name aro-vnet \
--name master-subnet \
--address-prefixes $MASTER_SUBNET_CIDR \
--service-endpoints Microsoft.ContainerRegistry
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>為工作節點添加一個空子網：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az network vnet subnet create \
--resource-group $RESOURCEGROUP \
--vnet-name aro-vnet \
--name worker-subnet \
--address-prefixes $WORKER_SUBNET_CIDR \
--service-endpoints Microsoft.ContainerRegistry
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>在主子網上禁用子網專用終結點策略：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">az network vnet subnet update \
--name master-subnet \
--resource-group $RESOURCEGROUP \
--vnet-name aro-vnet \
--disable-private-link-service-network-policies true
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="安裝-aro4">安裝 ARO4&lt;/h2>
&lt;ul>
&lt;li>使用 azure cli 創建 ARO 集群：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">echo &amp;#34;Creating ARO Cluster... Please wait 40mins&amp;#34;
az aro create --resource-group $RESOURCEGROUP \
--name $CLUSTER --vnet aro-vnet \
--master-subnet master-subnet \
--worker-subnet worker-subnet \
--pull-secret @pull-secret.txt
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意：安裝需要一個有效的 pull-secret。請訪問您的&lt;a class="link" href="https://rcarrata.com/openshift/aro4/cloud.redhat.com/openshift/" target="_blank" rel="noopener"
>Cloud OpenShift Doshboard&lt;/a>並獲取您的 pull-secret 令牌。&lt;/p>
&lt;ul>
&lt;li>然後 az aro cli 將在 Azure Dashboard 中預配一個 ARO 對象：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i.imgur.com/I1PyWMK.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>自動地，它使用用於安裝的 Azure 對象創建了一個額外的 Azure 資源組，由 RH 和 MSFT 的 ARO SRE 管理：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i.imgur.com/wjJ4OUi.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>在此資源組中，我們生成了提供和配置 ARO4 集群所需的資源：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://rcarrata.com/images/aro4_3.png"
loading="lazy"
>&lt;/p>
&lt;h2 id="訪問-api-和控制台">訪問 API 和控制台&lt;/h2>
&lt;ul>
&lt;li>大約 40 分鐘後。我們將使用控制台和 API 準備好 ARO4 集群：&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://rcarrata.com/images/aro4_4.png" target="_blank" rel="noopener"
>&lt;/a>&lt;/p>
&lt;ul>
&lt;li>要訪問集群，請列出集群的憑據：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">echo &amp;#34;List credentials for ARO Cluster&amp;#34;
az aro list-credentials \
--name $CLUSTER \
--resource-group $RESOURCEGROUP
echo &amp;#34;&amp;#34;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>要顯示 ARO4 控制台：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">echo &amp;#34;List console for ARO cluster&amp;#34;
az aro show \
--name $CLUSTER \
--resource-group $RESOURCEGROUP \
--query &amp;#34;consoleProfile.url&amp;#34; -o tsv
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>檢查 ARO4 API：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">apiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv)
echo &amp;#34;This is the API for your cluster: $apiServer&amp;#34;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>查詢 ARO Cluster 狀態&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@bastion ~]# az aro list -o table
Name ResourceGroup Location ProvisioningState WorkerCount URL
--------- --------------- ---------- ------------------- ------------- -----------------------------------------------------------------
aro-demo openenv-hdpnp eastus Succeeded 3 https://console-openshift-console.apps.yylin.io/
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>NVIDIA GPU Operator on OpenShift4</title><link>https://blog.yylin.io/openshift/gpu-operator/</link><pubDate>Mon, 28 Feb 2022 21:40:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/gpu-operator/</guid><description>&lt;p>Nvidia GPU Operator v1.9 on OpenShift 4.9.9 包含以上版本，安裝不用再進行額外權限配置]&lt;/p>
&lt;h2 id="openshift-499-或更高的版本-1">OpenShift 4.9.9 或更高的版本 [1]&lt;/h2>
&lt;p>針對 driver toolkit 取消必要安裝要求:&lt;/p>
&lt;ul>
&lt;li>Set up an entitlement&lt;/li>
&lt;li>Mirror the RPM packages in a disconnected environment&lt;/li>
&lt;li>Configure a proxy to access the package repository&lt;/li>
&lt;/ul>
&lt;p>[1] &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions" target="_blank" rel="noopener"
>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions&lt;/a>&lt;/p>
&lt;h2 id="openshift-498-與以下版本-2">OpenShift 4.9.8 與以下版本 [2]&lt;/h2>
&lt;p>需手動操作獲取 OCP 憑證，創建 MachineConfig 來認證 OCP 叢集，擴大授權 Images 使用權限範圍，來安裝 Nvidia Operator :&lt;/p>
&lt;ol>
&lt;li>從 Red Hat Customer Portal 下載 Red Hat OpenShift Container Platform 訂閱憑證 (啟用權限需要登入 OCP 憑證）。&lt;/li>
&lt;li>創建一個 MachineConfig 啟用訂閱管理平台並提供有效訂閱憑證。等待 MachineConfigOperator 重啟節點並完成 MachineConfig。&lt;/li>
&lt;li>驗證叢集所有節點更新權限是否正常。&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>補充 - NVIDIA GPU Operator 安裝會部署幾個 Pod 服務，用於管理和啟用 GPU 在 OpenShift 中運作。其中一些 Pod 需要 OpenShift 使用一些非 Universal Base Image (UBI) 默認授權的 Images。必須在 OpenShift Cluster 中啟用信任的授權 Images，來啟動 NVIDIA GPU 驅動程式的容器運行。&lt;/p>
&lt;/blockquote>
&lt;p>[1] &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions" target="_blank" rel="noopener"
>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions&lt;/a>
[2] &lt;a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/cluster-entitlement.html#enabling-a-cluster-wide-entitlemenent" target="_blank" rel="noopener"
>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/cluster-entitlement.html#enabling-a-cluster-wide-entitlemenent&lt;/a>&lt;/p>
&lt;h2 id="install-operator---node-feature-discovery">Install Operator - Node Feature Discovery&lt;/h2>
&lt;p>&lt;img src="https://i.imgur.com/3J8zoZU.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/NDepxYJ.png"
loading="lazy"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[lab-user@bastion ~]$ oc get no
NAME STATUS ROLES AGE VERSION
ip-10-0-135-51.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f
ip-10-0-142-219.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f
ip-10-0-167-35.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f
ip-10-0-186-251.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f
ip-10-0-213-103.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://i.imgur.com/456JizI.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>要驗證實例是否已創建，請運行：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[lab-user@bastion ~]$ oc get pods -n openshift-nfd
NAME READY STATUS RESTARTS AGE
nfd-controller-manager-6f65f47cf6-tg6gj 2/2 Running 0 24m
nfd-master-d7cqw 1/1 Running 0 35s
nfd-master-j42m9 1/1 Running 0 35s
nfd-master-r64nv 1/1 Running 0 35s
nfd-worker-24tzn 1/1 Running 0 35s
nfd-worker-5rsg2 1/1 Running 0 35s
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>成功的部署會顯示一個Running狀態。&lt;/li>
&lt;/ul>
&lt;h2 id="installing-the-nvidia-gpu-operator">Installing the NVIDIA GPU Operator&lt;/h2>
&lt;p>With the Node Feature Discovery Operator installed you can continue with the final step and install the NVIDIA GPU Operator.&lt;/p>
&lt;p>As a cluster administrator, you can install the NVIDIA GPU Operator using the OpenShift Container Platform CLI or the web console.&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/zSZnOQc.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/H5mLoWH.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/gV3G6kq.png"
loading="lazy"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">kind: ClusterPolicy
apiVersion: nvidia.com/v1
metadata:
name: gpu-cluster-policy
spec:
dcgmExporter:
config:
name: &amp;#39;&amp;#39;
dcgm:
enabled: true
daemonsets: {}
devicePlugin: {}
driver:
enabled: true
use_ocp_driver_toolkit: true
repoConfig:
configMapName: &amp;#39;&amp;#39;
certConfig:
name: &amp;#39;&amp;#39;
licensingConfig:
nlsEnabled: false
configMapName: &amp;#39;&amp;#39;
virtualTopology:
config: &amp;#39;&amp;#39;
gfd: {}
migManager:
enabled: true
nodeStatusExporter:
enabled: true
operator:
defaultRuntime: crio
deployGFD: true
initContainer: {}
mig:
strategy: single
toolkit:
enabled: true
validator:
plugin:
env:
- name: WITH_WORKLOAD
value: &amp;#39;true&amp;#39;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Create the ClusterPolicy custom resource. This CRD will create several OCP resources. It will evaluate all the labels for the each node in the cluster and look for this:&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/k4xdsx8.png"
loading="lazy"
>&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/45ygkxL.png"
loading="lazy"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc project nvidia-gpu-operator
$ oc get pod -o wide
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="validating-the-gpu-availability">Validating the GPU availability&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[lab-user@bastion ~]$ oc get pod | grep nvidia-device-plugin-daemonset
nvidia-device-plugin-daemonset-bspfh 1/1 Running 0 21m
nvidia-device-plugin-daemonset-n62dm 1/1 Running 0 21m
[lab-user@bastion ~]$ oc exec -ti nvidia-device-plugin-daemonset-bspfh -- nvidia-smi
Defaulted container &amp;#34;nvidia-device-plugin-ctr&amp;#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init)
Fri Jan 28 06:47:21 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|===============================+======================+======================|
| 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 |
| N/A 32C P0 23W / 300W | 0MiB / 16160MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes: |
| GPU GI CI PID Type Process name GPU Memory |
| ID ID Usage |
|=============================================================================|
| No running processes found |
+-----------------------------------------------------------------------------+
...
...
$ oc exec -ti nvidia-device-plugin-daemonset-n62dm -- nvidia-smi
Defaulted container &amp;#34;nvidia-device-plugin-ctr&amp;#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init)
Fri Jan 28 06:47:44 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|===============================+======================+======================|
| 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 |
| N/A 26C P0 24W / 300W | 0MiB / 16160MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes: |
| GPU GI CI PID Type Process name GPU Memory |
| ID ID Usage |
|=============================================================================|
| No running processes found |
+-----------------------------------------------------------------------------+
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="running-a-sample-gpu-application">Running a sample GPU Application&lt;/h2>
&lt;p>Run a simple CUDA VectorAdd sample, which adds two vectors together to ensure the GPUs have bootstrapped correctly.&lt;/p>
&lt;ol>
&lt;li>Run the following:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">cat &amp;lt;&amp;lt; EOF | oc create -f -
apiVersion: v1
kind: Pod
metadata:
name: cuda-vectoradd
spec:
restartPolicy: OnFailure
containers:
- name: cuda-vectoradd
image: &amp;#34;nvidia/samples:vectoradd-cuda11.2.1&amp;#34;
resources:
limits:
nvidia.com/gpu: 1
EOF
pod/cuda-vectoradd created
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="2">
&lt;li>Check the logs of the container:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[lab-user@bastion ~]$ oc logs cuda-vectoradd
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="getting-information-about-the-gpu">Getting information about the GPU¶&lt;/h2>
&lt;p>The nvidia-smi shows memory usage, GPU utilization and the temperature of the GPU. Test the GPU access by running the popular nvidia-smi command within the pod.&lt;/p>
&lt;p>To view GPU utilization, run nvidia-smi from a pod in the GPU Operator daemonset.&lt;/p>
&lt;ol>
&lt;li>Change to the nvidia-gpu-operator project:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc project nvidia-gpu-operator
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="2">
&lt;li>Run the following command to view these new pods:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc get pod -owide -lopenshift.driver-toolkit=true
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
nvidia-driver-daemonset-49.84.202201102104-0-gl557 2/2 Running 0 26m 10.131.0.106 ip-10-0-167-35.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
nvidia-driver-daemonset-49.84.202201102104-0-k9sg5 2/2 Running 0 26m 10.128.2.17 ip-10-0-135-51.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="3">
&lt;li>Run the nvidia-smi command within the pod:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ oc exec -it nvidia-driver-daemonset-48.84.202110270303-0-9df9j -- nvidia-smi
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Red Hat OpenShift Platform on AWS (ROSA) 快速部署上手</title><link>https://blog.yylin.io/openshift/rosa/</link><pubDate>Thu, 30 Sep 2021 14:54:40 +0800</pubDate><guid>https://blog.yylin.io/openshift/rosa/</guid><description>&lt;h2 id="事前準備">事前準備：&lt;/h2>
&lt;p>開始前，需要先準備以下資訊與要求。&lt;/p>
&lt;h3 id="啟用-rosa">啟用 ROSA&lt;/h3>
&lt;blockquote>
&lt;p>開始配置 AWS 服務前，使用者必須以 &lt;a class="link" href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-aws-prereqs.html#rosa-policy-iam_prerequisites" target="_blank" rel="noopener"
>IAM&lt;/a> 身份(參考文件: &lt;a class="link" href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-aws-prereqs.html#rosa-customer-requirements_prerequisites" target="_blank" rel="noopener"
>Customer Requirements&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;p>登入 AWS 管理控制台，確認當前使用者&lt;code>已啟用 Red Hat OpenShift&lt;/code>服務
&lt;img src="https://i.imgur.com/lbpYgG6.png"
loading="lazy"
>&lt;/p>
&lt;h3 id="下載-aws-cli-與-openshift-cli-工具">下載 AWS CLI 與 OpenShift CLI 工具:&lt;/h3>
&lt;p>&lt;strong>URL Dowload&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>選擇當前環境下載對應&lt;code>CLI&lt;/code> ( &lt;a class="link" href="https://console.redhat.com/openshift/downloads" target="_blank" rel="noopener"
>Command-line interface (CLI) tools&lt;/a>)&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/c64k4mB.png"
loading="lazy"
>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>or&lt;/p>
&lt;p>&lt;strong>Command Dowload&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>下載 &lt;code>AWS/OpenShift CLI&lt;/code> 工具，並解壓縮檔案到 /usr/local/bin/ 底下:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="c1"># CLI&lt;/span>
$ wget -c https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/rosa/latest/rosa-linux.tar.gz -O - &lt;span class="p">|&lt;/span> tar -xz
$ mv rosa /usr/local/bin/
$ rosa version
1.1.1
$ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - &lt;span class="p">|&lt;/span> tar -xz
$ mv &lt;span class="o">{&lt;/span>oc,kubectl&lt;span class="o">}&lt;/span> /usr/local/bin/
$ oc version
Client Version: 4.8.2
$ oc version
Client Version: 4.8.10
Server Version: 4.8.12
Kubernetes Version: v1.21.1+d8043e1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>創建 Red Hat 帳號
OpenShift 由 Red Hat SaaS 提供，因此您需要創建一個 Red Hat 帳戶。
&lt;a class="link" href="https://cloud.redhat.com/" target="_blank" rel="noopener"
>https://cloud.redhat.com/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="透過-rosa-指令檢查憑證與資源配置">透過 ROSA 指令檢查憑證與資源配置&lt;/h2>
&lt;p>這邊以 Red Hat 部署 ROSA 文件部署為主 (&lt;a class="link" href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-creating-cluster.html" target="_blank" rel="noopener"
>Red Hat OpenShift Service on AWS - Creating a ROSA cluster&lt;/a>)&lt;/p>
&lt;p>首先，使用 &lt;code>ROSA CLI&lt;/code> 檢查 AWS 憑證&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa verify permissions
I: Validating SCP policies...
I: AWS SCP policies ok
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第一次登錄時，你會需要一個 Token 來登入你的 Red Hat 帳號&lt;/p>
&lt;blockquote>
&lt;p>需要創建一組 Red Hat 帳號&lt;/p>
&lt;/blockquote>
&lt;p>確認登入訊息：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa login
I: Logged in as &lt;span class="s1">&amp;#39;xxxx@mail.com&amp;#39;&lt;/span> on &lt;span class="s1">&amp;#39;https://api.openshift.com&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>配置完成後可以透過 &lt;code>rosa whoami&lt;/code> 來檢查你目前所有登錄憑證與狀態&lt;/p>
&lt;blockquote>
&lt;p>Red Hat 帳號會附帶 OpenShift Cluster Manager (OCM) 資訊與你的 AWS 帳戶顯示。&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ rosa whoami
AWS Account ID: xxxxxxxxxxxx
AWS Default Region: us-west-2
AWS ARN: arn:aws:iam::xxxxxxxxxxxx:user/user
OCM API: https://api.openshift.com
OCM Account ID: xxxxxxxxxxxxxxxxxxxxxx
OCM Account Name: xxxxxx
OCM Account Username: xxxxx@xxxxx.com
OCM Account Email: xxxxx@xxxxx.com
OCM Organization ID: xxxxxxxxxxxxxxxxxxxxxx
OCM Organization Name: Red Hat
OCM Organization External ID: xxxxxxxx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>以上檢查沒問題後，接下運行 &lt;code>rosa init&lt;/code> 運行指令來確保 ROSA 配置與相關資源狀態沒問題&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ rosa init
I: Logged in as &amp;#39;xxxxxx@mail.com&amp;#39; on &amp;#39;https://api.openshift.com&amp;#39;
I: Validating AWS credentials...
I: AWS credentials are valid!
I: Validating SCP policies...
I: AWS SCP policies ok
I: Validating AWS quota...
I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html
I: Ensuring cluster administrator user &amp;#39;osdCcsAdmin&amp;#39;...
I: Admin user &amp;#39;osdCcsAdmin&amp;#39; already exists!
I: Validating SCP policies for &amp;#39;osdCcsAdmin&amp;#39;...
I: AWS SCP policies ok
I: Validating cluster creation...
I: Cluster creation valid
I: Verifying whether OpenShift command-line tool is available...
I: Current OpenShift Client Version: 4.8.10
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>需要驗證當前環境，是否已經登入 AWS並確認透過&lt;code>rosa init&lt;/code> 來確認 AWS 資源與配置是否能足夠創建 OpenShift&lt;/p>
&lt;/blockquote>
&lt;h3 id="申請-aws-帳戶配額">申請 AWS 帳戶配額&lt;/h3>
&lt;p>在配置 ROSA 時作為預先檢查 &lt;code>$ rosa verify quota --region=${cluster}&lt;/code> 叢集名稱。如果您在剛剛部署但未執行任何操作的帳戶上運行它，您將收到錯誤訊息 EC2 &lt;code>quota 配置不足&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa verify quota --region&lt;span class="o">=&lt;/span>us-west-2
I: Validating AWS quota...
E: Insufficient AWS quotas
E: Service quota is insufficient &lt;span class="k">for&lt;/span> the following service quota codes:
- Service ec2 quota code L-1216C47A Running On-Demand Standard &lt;span class="o">(&lt;/span>A, C, D, H, I, M, R, T, Z&lt;span class="o">)&lt;/span> instances not valid, expected quota of at least 100, but got &lt;span class="m">5&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>這邊需要額外申請 AWS EC2 限制增加配額
&lt;img src="https://i.imgur.com/X2PtefK.png"
loading="lazy"
>&lt;/p>
&lt;p>需要提供申請需求與相關資訊，才能申請限制配額成功&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/fMprN3I.png"
loading="lazy"
>&lt;/p>
&lt;p>相關配額資訊參考&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html" target="_blank" rel="noopener"
>AWS service quotas&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html" target="_blank" rel="noopener"
>Required AWS service quotas&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i.imgur.com/4K3BWLp.png"
loading="lazy"
>&lt;/p>
&lt;p>以上完成申請後，再次驗證&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa verify quota --region&lt;span class="o">=&lt;/span>us-west-2
I: Validating AWS quota...
I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.htm
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="透過-rosa-指令快速創建-red-hat-openshift">透過 ROSA 指令快速創建 Red Hat OpenShift&lt;/h2>
&lt;ul>
&lt;li>透過 &lt;code>rosa create cluster&lt;/code> 開始部署 OpenShift Cluster (部署Cluster 大約需要30-40分鐘)&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa create cluster --cluster-name&lt;span class="o">=&lt;/span>rosacluster
I: Creating cluster &lt;span class="s1">&amp;#39;rosacluster&amp;#39;&lt;/span>
I: To view a list of clusters and their status, run &lt;span class="s1">&amp;#39;rosa list clusters&amp;#39;&lt;/span>
I: Cluster &lt;span class="s1">&amp;#39;rosacluster&amp;#39;&lt;/span> has been created.
I: Once the cluster is installed you will need to add an Identity Provider before you can login into the cluster. See &lt;span class="s1">&amp;#39;rosa create idp --help&amp;#39;&lt;/span> &lt;span class="k">for&lt;/span> more information.
I: To determine when your cluster is Ready, run &lt;span class="s1">&amp;#39;rosa describe cluster -c newsblogcluster&amp;#39;&lt;/span>.
I: To watch your cluster installation logs, run &lt;span class="s1">&amp;#39;rosa logs install -c newsblogcluster --watch&amp;#39;&lt;/span>.
Name: newsblogcluster
ID: XXXXXXXXXXXXXXXXXXXX
External ID:
OpenShift Version:
Channel Group: stable
DNS: newsblogcluster.phnh.p1.openshiftapps.com
AWS Account: &lt;span class="m">123456789012&lt;/span>
API URL:
Console URL:
Region: xxxxxxxxxxx
Multi-AZ: &lt;span class="nb">false&lt;/span>
Nodes:
- Master: &lt;span class="m">3&lt;/span>
- Infra: &lt;span class="m">2&lt;/span>
- Compute: &lt;span class="m">2&lt;/span> &lt;span class="o">(&lt;/span>m5.xlarge&lt;span class="o">)&lt;/span>
Network:
- Service CIDR: 172.30.0.0/16
- Machine CIDR: 10.0.0.0/16
- Pod CIDR: 10.128.0.0/14
- Host Prefix: /23
State: pending &lt;span class="o">(&lt;/span>Preparing account&lt;span class="o">)&lt;/span>
Private: No
Created: xxxxxxxxxxxxxxxx
Details Page: https://cloud.redhat.com/openshift/details/XXXXXXXXXXXXXXXXXXXX
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>執行命令後，您可以透過 &lt;code>rosa describe cluster&lt;/code> 或 &lt;code>rosa logs install&lt;/code> 指令檢查 OpenShift Cluster 狀態。您還可以從 URL 訪問 OpenShift Console 進行確認。&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/DvBaXFH.png"
loading="lazy"
>
&lt;img src="https://i.imgur.com/KkZJ3zn.png"
loading="lazy"
>&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/LUfY9n9.png"
loading="lazy"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">$ rosa create admin -c rosacluster
W: It is recommended to add an identity provider to login to this cluster. See &amp;#39;rosa create idp --help&amp;#39; for more information.
I: Admin account has been added to cluster &amp;#39;rosacluster&amp;#39;.
I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user.
I: To login, run the following command:
oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z
I: It may take up to a minute for the account to become active.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z
Login successful.
You have access to &lt;span class="m">87&lt;/span> projects, the list has been suppressed. You can list all projects with &lt;span class="s1">&amp;#39;oc projects&amp;#39;&lt;/span>
Using project &lt;span class="s2">&amp;#34;default&amp;#34;&lt;/span>.
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ oc get no
NAME STATUS ROLES AGE VERSION
ip-10-0-153-95.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1
ip-10-0-171-119.us-west-2.compute.internal Ready infra,worker 17m v1.21.1+d8043e1
ip-10-0-175-165.us-west-2.compute.internal Ready infra,worker 18m v1.21.1+d8043e1
ip-10-0-202-234.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1
ip-10-0-239-231.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1
ip-10-0-242-134.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1
ip-10-0-252-208.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="刪除-cluster">刪除 Cluster&lt;/h2>
&lt;p>&lt;code>rosa delete cluster&lt;/code> 可以使用指令刪除使用 ROSA 創建的cluster。順便說一下，集群的刪除在大約 10 分鐘內完成。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ rosa delete cluster --cluster&lt;span class="o">=&lt;/span>rosacluster
? Are you sure you want to delete cluster rosacluster? Yes
I: Cluster &lt;span class="s1">&amp;#39;rosacluster&amp;#39;&lt;/span> will start uninstalling now
I: To watch your cluster uninstallation logs, run &lt;span class="err">&amp;#39;&lt;/span>rosa logs uninstall -c rosacluster --watch
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>rosaworkshop.io&lt;/li>
&lt;/ul></description></item></channel></rss>