[{"content":"","date":"2022-03-13T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/aro/","title":"從零開始快速建置 - Microsoft Azure Red Hat OpenShift (ARO)"},{"content":"本文件如何安裝 OpenShift 4.8 版本至虛擬機器上，這邊將以 RHEL 環境來進行測試。\n RHEL 8 or Fedora machine with podman v3.3 installed Fully qualified domain name for the Quay service (must resolve via DNS, or at least /etc/hosts) Passwordless sudo access on the target host (rootless install tbd) Key-based SSH connectivity on the target host (will be set up automatically for local installs, in case of remote hosts see here) make (only if compiling your own installer)  當我們的基礎設施處於離線環境時，需要創建容器鏡像倉庫(Registry)來託管安裝 OpenShift Container Platform 所需的相關鏡像檔，來正常部署 OpenShift 叢集。\n這是有關 mirror registry for Red Hat OpenShift OpenShift 4 的官方流程圖（可在 Red Hat Blog 頁面中找到）：\n事前準備  Reference  https://cloud.redhat.com/blog/introducing-mirror-registry-for-red-hat-openshift https://access.redhat.com/support/policy/updates/openshift#omr https://www.youtube.com/watch?v=j5e4OT71N0A https://cloud.redhat.com/blog/introducing-mirror-registry-for-red-hat-openshift  ","date":"2022-03-07T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/mirror-registry-for-ocp4-install/","title":"Disconnected Environment - Introducing Mirror Registry for Red Hat OpenShift"},{"content":"2021 年有非常多的轉變與抉擇，是讓自己很深刻的一年，雖然年初有一些個人因素，影響後續許多計畫與安排，但之後從個人職涯選擇以及決定，為自己嘗試更多各種不同的挑戰，雖然過程很不容易，但能能充實的過完這一年。\n2021 回顧  協助前公司部分產品 Containerized 建置與Kuberentes - IT 初期工作流程基礎規劃，學習很多寶貴的維運經驗。 下半年決定為自己職涯嘗試新的挑戰，轉換職務，雖然過程花很多時間為認知與技能做轉換，過程參與事件活動非常多，但忙得有意義，非常充實。 在 COSCUP 2021 協助 CNTUG 社群議程軌-開源運河上的雲原生號擔任主持人：首次 COSCUP 線上虛擬舉行，體會主持人如何引導議程進行時，與講者會眾互動 Q\u0026amp;A ，並完成整個整個議程軌活動，難得且充實的年會體驗。 擔任 HPC Summit 2021 講者：有機會分享抽空研究DeepOps 的專案，如何對於 AI/ML Infra 建置的部署工具與企業場景實際應用分析(簡報連結)。 擔任 Kuberentes Summit 2021 - WorkShop 講者: 學習如何用會眾角度規劃整個工作坊設計。 回歸山岳攝影-登山健行: 持續撰寫登山系列文章，跟一群好友組了登山興趣小組，開始嘗試(台灣中級山系列、台北大縱走路線完成4/7段[持續進行中])，脫離城市的大自然體驗。 自己動手做一顆手工肥皂，感謝 SU VIDA x 私 生活 開設的工作坊 攀岩(抱石) 維持 LV2 等級都順利完攀，持續往下一級前進。 終於換了新一台相機 Sony A7 IV  2022 展望  增強專業能力部分，提升各方面不足點 花更多時間在技術文章撰寫 完成先前未完成的證照 回歸開源專案貢獻，積極參與 Conference / Meetup 分享 持續健身運動維持飲食控制，為百岳做後續準備  Related Posts  2020-年度回顧 (舊 Blog 搬家中)   2022 繼續爬山   Photography Blog (https://medium.com/yiyang-lins-life)  ","date":"2021-12-31T13:33:28+08:00","permalink":"https://blog.yylin.io/year/2021-retrospect-and-prospect/","title":"2021 年度回顧"},{"content":"事前準備： 開始前，需要先準備以下資訊與要求。\n啟用 ROSA  開始配置 AWS 服務前，使用者必須以 IAM 身份(參考文件: Customer Requirements)\n 登入 AWS 管理控制台，確認當前使用者已啟用 Red Hat OpenShift服務 下載 AWS CLI 與 OpenShift CLI 工具: URL Dowload\n  選擇當前環境下載對應CLI ( Command-line interface (CLI) tools)\n  or\nCommand Dowload\n 下載 AWS/OpenShift CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:  1 2 3 4 5 6 7 8 9 10 11 12 13 14  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/rosa/latest/rosa-linux.tar.gz -O - | tar -xz $ mv rosa /usr/local/bin/ $ rosa version 1.1.1 $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 $ oc version Client Version: 4.8.10 Server Version: 4.8.12 Kubernetes Version: v1.21.1+d8043e1    創建 Red Hat 帳號 OpenShift 由 Red Hat SaaS 提供，因此您需要創建一個 Red Hat 帳戶。 https://cloud.redhat.com/\n  透過 ROSA 指令檢查憑證與資源配置 這邊以 Red Hat 部署 ROSA 文件部署為主 (Red Hat OpenShift Service on AWS - Creating a ROSA cluster)\n首先，使用 ROSA CLI 檢查 AWS 憑證\n1 2 3  $ rosa verify permissions I: Validating SCP policies... I: AWS SCP policies ok   第一次登錄時，你會需要一個 Token 來登入你的 Red Hat 帳號\n 需要創建一組 Red Hat 帳號\n 確認登入訊息：\n1 2  $ rosa login I: Logged in as \u0026#39;xxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39;   配置完成後可以透過 rosa whoami 來檢查你目前所有登錄憑證與狀態\n Red Hat 帳號會附帶 OpenShift Cluster Manager (OCM) 資訊與你的 AWS 帳戶顯示。\n 1 2 3 4 5 6 7 8 9 10 11 12  $ rosa whoami AWS Account ID: xxxxxxxxxxxx AWS Default Region: us-west-2 AWS ARN: arn:aws:iam::xxxxxxxxxxxx:user/user OCM API: https://api.openshift.com OCM Account ID: xxxxxxxxxxxxxxxxxxxxxx OCM Account Name: xxxxxx OCM Account Username: xxxxx@xxxxx.com OCM Account Email: xxxxx@xxxxx.com OCM Organization ID: xxxxxxxxxxxxxxxxxxxxxx OCM Organization Name: Red Hat OCM Organization External ID: xxxxxxxx   以上檢查沒問題後，接下運行 rosa init 運行指令來確保 ROSA 配置與相關資源狀態沒問題\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ rosa init I: Logged in as \u0026#39;xxxxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39; I: Validating AWS credentials... I: AWS credentials are valid! I: Validating SCP policies... I: AWS SCP policies ok I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html I: Ensuring cluster administrator user \u0026#39;osdCcsAdmin\u0026#39;... I: Admin user \u0026#39;osdCcsAdmin\u0026#39; already exists! I: Validating SCP policies for \u0026#39;osdCcsAdmin\u0026#39;... I: AWS SCP policies ok I: Validating cluster creation... I: Cluster creation valid I: Verifying whether OpenShift command-line tool is available... I: Current OpenShift Client Version: 4.8.10    需要驗證當前環境，是否已經登入 AWS並確認透過rosa init 來確認 AWS 資源與配置是否能足夠創建 OpenShift\n 申請 AWS 帳戶配額 在配置 ROSA 時作為預先檢查 $ rosa verify quota --region=${cluster} 叢集名稱。如果您在剛剛部署但未執行任何操作的帳戶上運行它，您將收到錯誤訊息 EC2 quota 配置不足。\n1 2 3 4 5  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... E: Insufficient AWS quotas E: Service quota is insufficient for the following service quota codes: - Service ec2 quota code L-1216C47A Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances not valid, expected quota of at least 100, but got 5   這邊需要額外申請 AWS EC2 限制增加配額 需要提供申請需求與相關資訊，才能申請限制配額成功\n相關配額資訊參考\n AWS service quotas Required AWS service quotas  以上完成申請後，再次驗證\n1 2 3  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.htm    透過 ROSA 指令快速創建 Red Hat OpenShift  透過 rosa create cluster 開始部署 OpenShift Cluster (部署Cluster 大約需要30-40分鐘)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  $ rosa create cluster --cluster-name=rosacluster I: Creating cluster \u0026#39;rosacluster\u0026#39; I: To view a list of clusters and their status, run \u0026#39;rosa list clusters\u0026#39; I: Cluster \u0026#39;rosacluster\u0026#39; has been created. I: Once the cluster is installed you will need to add an Identity Provider before you can login into the cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: To determine when your cluster is Ready, run \u0026#39;rosa describe cluster -c newsblogcluster\u0026#39;. I: To watch your cluster installation logs, run \u0026#39;rosa logs install -c newsblogcluster --watch\u0026#39;. Name: newsblogcluster ID: XXXXXXXXXXXXXXXXXXXX External ID: OpenShift Version: Channel Group: stable DNS: newsblogcluster.phnh.p1.openshiftapps.com AWS Account: 123456789012 API URL: Console URL: Region: xxxxxxxxxxx Multi-AZ: false Nodes: - Master: 3 - Infra: 2 - Compute: 2 (m5.xlarge) Network: - Service CIDR: 172.30.0.0/16 - Machine CIDR: 10.0.0.0/16 - Pod CIDR: 10.128.0.0/14 - Host Prefix: /23 State: pending (Preparing account) Private: No Created: xxxxxxxxxxxxxxxx Details Page: https://cloud.redhat.com/openshift/details/XXXXXXXXXXXXXXXXXXXX   執行命令後，您可以透過 rosa describe cluster 或 rosa logs install 指令檢查 OpenShift Cluster 狀態。您還可以從 URL 訪問 OpenShift Console 進行確認。\n1 2 3 4 5 6 7 8 9  $ rosa create admin -c rosacluster W: It is recommended to add an identity provider to login to this cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: Admin account has been added to cluster \u0026#39;rosacluster\u0026#39;. I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user. I: To login, run the following command: oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z I: It may take up to a minute for the account to become active.   1 2 3 4 5 6  $ oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z Login successful. You have access to 87 projects, the list has been suppressed. You can list all projects with \u0026#39;oc projects\u0026#39; Using project \u0026#34;default\u0026#34;.   1 2 3 4 5 6 7 8 9  $ oc get no NAME STATUS ROLES AGE VERSION ip-10-0-153-95.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1 ip-10-0-171-119.us-west-2.compute.internal Ready infra,worker 17m v1.21.1+d8043e1 ip-10-0-175-165.us-west-2.compute.internal Ready infra,worker 18m v1.21.1+d8043e1 ip-10-0-202-234.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-239-231.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-242-134.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-252-208.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1   刪除 Cluster rosa delete cluster 可以使用指令刪除使用 ROSA 創建的cluster。順便說一下，集群的刪除在大約 10 分鐘內完成。\n1 2 3 4  $ rosa delete cluster --cluster=rosacluster ? Are you sure you want to delete cluster rosacluster? Yes I: Cluster \u0026#39;rosacluster\u0026#39; will start uninstalling now I: To watch your cluster uninstallation logs, run \u0026#39;rosa logs uninstall -c rosacluster --watch   Reference  rosaworkshop.io  ","date":"2021-09-30T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/rosa/","title":"Red Hat OpenShift Platform on AWS (ROSA) 快速部署上手"},{"content":"延伸上一篇 OCP UPI 部署(OpenShift 4.8.x UPI install on Bare metal)過程遇到問題彙整。\n 備註: 由於資源有限情況下，透過單一伺服器(主機)透過 VM 模擬實際節點硬體資源環境，這邊主要透過檢測與除錯，來確保 OpenShift 部署成功。\n 部署常見問題整理 (持續更新) 超時等待 bootstrap 節點啟動  發現特定 Pod 持續RunningNotReady 狀態  1 2 3  W0830 11:16:37.087497 35214 reflector.go:436] k8s.io/client-go/tools/watch/informerwatcher.go:146: watch of *v1.ConfigMap ended with: very short watch: k8s.io/client-go/tools/watch/informerwatcher.go:146: Unexpected watch close - watch lasted less than a second and no items received apiserver - \u0026gt; RunningNotReady Aug 30 03:42:37 bootstrap.lab.yiylin.internal bootkube.sh[644199]: Pod Status:openshift-kube-apiserver/kube-apiserver RunningNotReady    此情境請透過收集bootstrap 節點日誌，來確認環境問題\n 1 2 3 4  $ ./openshift-install gather bootstrap --dir=\u0026lt;installation_directory\u0026gt; ... INFO Pulling debug logs from the bootstrap machine INFO Bootstrap gather logs captured here \u0026#34;\u0026lt;installation_directory\u0026gt;/log-bundle-\u0026lt;timestamp\u0026gt;.tar.gz\u0026#34;    蒐集包含 bootstrap process / bootstrap 上所有 Pod/Container 的 log 下載完後解壓縮 .tar.gz 檔案，針對可能部署或未啟動的Pod檢查log錯誤狀態，來找到對應的錯誤資訊   補充說明 bootstrap 主要分成兩個動作\n 安裝自己跟建立自己的 cluster(Pod) 引導安裝 master x 3 master bootstrap 全部安裝完，然後下 wait for complete 確認最終才可移除 bootstrap node   Reference:  Gathering logs from a failed installation Waiting for the bootstrap process to complete  Master Node 部署不起來，檢查並驗證 etcd 運行之硬碟是否正常運作 ?  大部分 master 無法建置情境，通常可能會有原因是硬碟速度問題(硬碟壞軌)   造成叢集有時候部署成功有時候會失敗的情況\n 透過簡單運行 fio 來測試 etcd benchamrk performance 檢測\n1 2 3 4  $ oc debug node/\u0026lt;master_node\u0026gt; [...] sh-4.4# chroot /host bash [root@\u0026lt;master_node\u0026gt; /]# podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf   Reference:  How to Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance in OCP  [參考影片說明] OpenShift 4 如何使用 fio 驗證 etcd 運行之硬碟是否可以正常運作 / Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance Hardware requirements and recommendations  補充|檢查 - 除錯流程   部署過程都透過 bootkube 確認部署狀態資訊，log 會不斷顯示錯誤然後 loop 確認環節，嘗試能不能從他錯誤訊息找出規則(也就是他跳錯誤的地方，通常是在 log restart之前)\n  嘗試 SSH 到 Master Node，透過 netstat -plnt 檢查三個服務，這三個通常只要好，這台 Master 就沒問題 (6443| 22623|2379|2380)，分別是 OpenShift API/Kubernetes API/ETCD/Machine Config Operator\n  如果以上幾個 Pod 有問題，透過 crictl ps 跟 crictl log 去查看 Container Log，問題有可能是連續的錯誤，所以可能是 API 有問題，可能是 Controller/Scheduler 引起，細節需要透過上面提到方式，下載節點所有 Log 日誌來比對檢查，找到正確的錯誤資訊來排除錯誤\n  Reference:  OpenShift Troubleshooting Resources  ","date":"2021-09-29T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/install-troubleshooting/","title":"OpenShift Intsall Troubleshooting Notes"},{"content":" [註記] 由於硬體環境資源有限，本實驗是透過 Proxmox Environment 單一伺服器節點來模擬部署 OpenShift 叢集，並依據官網配置建議硬體需求(Server sizing)資源。 [共同撰寫編輯]: Kyle Bai\n 部署版本說明：  Proxmox Environment v6.1-7  CPU: 8-Core, 16-Thread Unlocked / Memory: 128 GB   OpenShift v4.8.x.  Reference  參考官網文件 - Installing a user-provisioned cluster on bare metal  事前準備 開始前，需要先準備以下資訊與要求。\nDownload URLs  User-provisioned infrastructure: 可下載 OpenShift Installer、CLI 與 Pull secrets(需要登入 Red Hat 註冊帳號權限) Public OpenShift 4.8 Mirror: 可下載 OpenShift Installer、CLI 與 OPM。  openshift-client-linux.tar.gz: 包含 oc 與 kubectl CLI 工具。 openshift-install-linux.tar.gz: 包含 openshift-install 工具。用於建立 OpenShift auth、igntion 等設定檔案。 opm-linux.tar.gz: 用於管理 Operator Hub 與 mirror 相關 images。   Public RHCOS 4.8 Mirror: 可下載 RHCOS 4.8 相關 VM images。  若手動安裝以下載 rhcos-live.x86_64.iso 為主。若想要控管使用版本，請選擇有標示版本的 live iso。  FET vSphere 安裝請以這方式進行。   若以 PXE 方式安裝，則下載以下映像檔:  rhcos-live-initramfs.x86_64.img rhcos-live-kernel-x86_64 rhcos-live-rootfs.x86_64.img       過程中會透過 wget 下載，因此在有網路狀況下，不需要預先下載。\n  Network and Hosts Info  OpenShift subnet CIDR: 192.168.101.0/24 OpenShift Pod Network(CNI): 10.128.0.0/14 OpenShift Service Network: 172.3.0.0/16 Domain: lab.yiylin.internal     Hosts IP CPU RAM     bashtion 192.168.101.9 2 vCore 4G   bootstrap 192.168.101.10 4 vCore 8G   master-1 192.168.101.21 4 vCore 8G   master-2 192.168.191.22 4 vCore 8G   master-3 192.168.191.23 4 vCore 8G   worker-1 192.168.191.31 4 vCore 8G   worker-2 192.168.191.32 4 vCore 8G   infra-1 192.168.191.41 4 vCore 8G   infra-2 192.168.191.42 4 vCore 8G   infra-3 192.168.191.43 4 vCore 8G     這裡為了方便測試，將 PXE / HAProxy / DNS Server 等都放在 bastion上。 [額外參考資訊]各元件節點最小資源可參考 Minimum resource requirements。 請先透過 Proxmox Environment 開啟對應 hardware VM。\n 補充 Network 說明  其中 OpenShift subnet CIDR 請修改為自己當前 Proxmox Environment 網路配置)  Bastion 透過 Live CD 安裝 RHEL 8 版本(或使用 CentOS 8)，並確保網路設定正確。\n RHEL 需要確認 subscription 驗證，確保 repo 源可正常取得\n 事前準備 進入 bastion 機器，安裝 jq 與 wget 工具等:\n1 2  $ sudo su - $ yum install -y wget jq vim   關閉 bastion 防火牆:\n1 2  $ systemctl stop firewalld $ systemctl disable firewalld   \u0026gt; 這邊方便測試，所以直接關閉 :D。\n設定與關閉 SELinux:\n1 2 3 4  $ vim /etc/selinux/config SELINUX=disabled $ setenforce 0    這邊方便測試實驗，所以直接關閉。\n 建立 SSH Keys:\n1  $ ssh-keygen -t rsa    這邊方便測試，以 root 來建立。\n 導入以下環境變數:\n1 2  export DOMAIN=lab.yiylin.internal export OCP_NETWORK_CIDR=192.168.101.0/24   下載 OpenShift Installer 與 CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:\n1 2 3 4 5 6 7 8 9 10 11  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 # Installer $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-install-linux.tar.gz -O - | tar -xz $ mv openshift-install /usr/local/bin/ $ openshift-install version openshift-install 4.8.2   安裝與設定 DNS Server 安裝 DNS server 套件，這邊使用 dnsmasq 來提供服務:\n1  $ yum -y install dnsmasq bind-utils    這部分也可參考 OCP 官網安裝 BIND，關於不同 DNS Server 相關配置方法，會額外補充於後續文章介紹。\n 編輯 /etc/dnsmasq.d/openshift.conf 設定檔，以設定 A Record 與 PTR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/dnsmasq.d/openshift.conf server=8.8.8.8 domain=${DOMAIN},${OCP_NETWORK_CIDR},local # A Records(api) host-record=api.${DOMAIN}, 192.168.101.9 host-record=api-int.${DOMAIN}, host-record=api-int.${DOMAIN},192.168.101.9 # A Records(hosts) host-record=bastion.${DOMAIN},192.168.101.9 host-record=bootstrap.${DOMAIN},192.168.101.10 host-record=master-1.${DOMAIN},192.168.101.21 host-record=master-2.${DOMAIN},192.168.101.22 host-record=master-3.${DOMAIN},192.168.101.23 host-record=worker-1.${DOMAIN},192.168.101.31 host-record=worker-2.${DOMAIN},192.168.101.32 host-record=infra-1.${DOMAIN},192.168.101.41 host-record=infra-2.${DOMAIN},192.168.101.42 host-record=infra-3.${DOMAIN},192.168.101.43 # A Records(etcd) host-record=etcd-0.${DOMAIN},192.168.101.21 host-record=etcd-1.${DOMAIN},192.168.101.22 host-record=etcd-2.${DOMAIN},192.168.101.23 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-0.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-1.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-2.${DOMAIN},2380,0,10 # PTRs address=/apps.${DOMAIN}/192.168.101.9 address=/.apps.${DOMAIN}/192.168.101.9 address=/api.${DOMAIN}/192.168.101.9 address=/api-int.${DOMAIN}/192.168.101.9 address=/bastion.${DOMAIN}/192.168.101.9 address=/bootstrap.${DOMAIN}/192.168.101.10 address=/master-1.${DOMAIN}/192.168.101.21 address=/master-2.${DOMAIN}/192.168.101.22 address=/master-3.${DOMAIN}/192.168.101.23 address=/etcd-0.${DOMAIN}/192.168.101.21 address=/etcd-1.${DOMAIN}/192.168.101.22 address=/etcd-2.${DOMAIN}/192.168.101.23 address=/worker-1.${DOMAIN}/192.168.101.31 address=/worker-2.${DOMAIN}/192.168.101.32 address=/infra-1.${DOMAIN}/192.168.101.41 address=/infra-2.${DOMAIN}/192.168.101.42 address=/infra-3.${DOMAIN}/192.168.101.43 EOF   啟動 dnsmasq 服務，並檢測 dnsmasq 設定是否正確:\n1 2 3  $ systemctl enable dnsmasq; systemctl start dnsmasq $ dnsmasq --test dnsmasq: syntax check OK.   編輯 /etc/resolv.conf 檔案，修改 nameserver:\n1 2 3  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/resolv.conf nameserver 127.0.0.1 EOF   驗證 DNS server 正反解:\n1 2 3 4 5  # dig domain @dns_server $ dig bastion.${DOMAIN} @192.168.101.9 # dig -x ip @dns_server $ dig -x 192.168.101.9 @192.168.101.9    請驗證填寫自己對應的 ip address\n 安裝與設定 HTTP Server 安裝與設定 httpd 套件:\n1  $ yum -y install httpd   編輯 /etc/httpd/conf/httpd.conf 設定檔:\n1 2  #Listen 12.34.56.78:80 Listen 8080\t   因為 80 是 Ingress-http 使用，除非將 HAProxy 拆成不同節點或用 External LB。\n 啟動 httpd 服務:\n1  $ systemctl enable httpd; systemctl start httpd   產生 OpenShift Ignition fils 建立放置 OpenShift Config 檔案的目錄:\n1  $ cd $HOME ; mkdir $HOME/ocp4/ignition;cd $HOME/ocp4/ignition   建立與編輯 install-config.yaml 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v1baseDomain:yiylin.internalcompute:- hyperthreading:Enabledname:workerreplicas:0controlPlane:hyperthreading:Enabledname:masterreplicas:3metadata:name:labnetworking:clusterNetworks:- cidr:10.128.0.0/14hostPrefix:24networkType:OpenShiftSDNserviceNetwork:- 172.3.0.0/16platform:none:{}pullSecret:\u0026#39;{\u0026#34;auths\u0026#34;:{ YOUR_PULL_SECRETE }}\u0026#39;sshKey:\u0026#34;SSH_PUB_KEY\u0026#34;    Pull secret 可在 Create Cluster by user provisioned 取得。 Proxy 設定，參考 Configuring the cluster-wide proxy during installation。 SSH Key 請自行於 bastion 創建填寫，這裡不多做說明   產生 manifests 相關檔案:\n1 2 3 4  $ cp install-config.yaml install-config.yaml.bak $ openshift-install create manifests --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition manifests openshift   編輯 manifests/cluster-scheduler-02-config.yml 檔案，設定 mastersSchedulable 值，以確保 master 不作為工作負載節點:\n1 2 3 4  ...spec:mastersSchedulable:false...  產生 OCP 使用的 Ignition files:\n1 2 3  $ openshift-install create ignition-configs --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition auth bootstrap.ign master.ign metadata.json worker.ign   撰寫方面安裝 RHCOS 的腳本 $HOME/ocp4/ignition/coreos-install.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/bin/bash  set -eu export DEVICE_NAME=${DEVICE_NAME:-/dev/sda} export IGNITION_FILE=${IGNITION_FILE:-bootstrap.ign} # bootstrap.ign master.ign worker.ign  export IGNITION_URL=${IGNITION_URL:-http://192.168.101.9:8080/ignition/${IGNITION_FILE}} sudo coreos-installer install ${DEVICE_NAME} \\  --insecure-ignition \\  --ignition-url=${IGNITION_URL} \\  --firstboot-args rd.neednet=1 \\  --copy-network   複製 *.ign 與腳本 到 /var/www/html/ignition 目錄:\n1 2 3 4 5 6  $ mkdir /var/www/html/ignition $ cp -rp $HOME/ocp4/ignition/{*.ign,coreos-install.sh} /var/www/html/ignition $ ls /var/www/html/ignition bootstrap.ign master.ign worker.ign coreos-install.sh $ chmod 664 -R /var/www/html/ignition/*   Bootstrap 透過 PVE 建立 Bootstrap 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   設定網卡資訊:\n 確保網卡 IP address 設定為 xx.xx.xx.xx/24，若沒有設定 Network mask 的話，就會用預設 xx.xx.xx.xx/8。\n 重新啟動網卡設定:\n透過以下指令檢查設定結果:\n1  $ ip -4 a    確認網路與 DNS 都能夠被解析到。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Bootstrap 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Bootstrap 開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be INFO Waiting up to 20m0s for the Kubernetes API at https://api.lab.yiylin.internal:6443... INFO API v1.21.1+051ac4f up INFO Waiting up to 30m0s for bootstrapping to complete... # 確認 openshift-api-server 以及 machine-config-server LB 可以通 $ curl -k https://api-int.${DOMAIN}:6443 $ curl -k https://api-int.${DOMAIN}:22623/config/master | jq .   當看到此訊息時，就能並往下進行安裝 Masters。若等待過久，可以透過 bastion 進入 bootstrap 節點查看狀況:\n1 2 3 4 5  $ ssh core@bootstrap.lab.yiylin.internal core $ sudo crictl pods core $ sudo crictl ps -a core $ sudo journalctl -b -f -u bootkube.service   正常情況 Pods 要如以下:\n1 2 3 4 5 6 7 8  POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 8101e41b16f9e 52 seconds ago Ready bootstrap-kube-scheduler-localhost kube-system 0 (default) 907d0a933771a 52 seconds ago Ready bootstrap-kube-controller-manager-localhost kube-system 0 (default) a8b709f7caeaa 52 seconds ago Ready bootstrap-kube-apiserver-localhost kube-system 0 (default) 4eb793b3d1844 52 seconds ago Ready cloud-credential-operator-localhost openshift-cloud-credential-operator 0 (default) 49a1cdb2b25f9 52 seconds ago Ready bootstrap-cluster-version-operator-localhost openshift-cluster-version 0 (default) c5098e42b5aa5 About a minute ago Ready bootstrap-machine-config-operator-localhost default 0 (default) e7a3dbc6e0bc0 About a minute ago Ready etcd-bootstrap-member-localhost openshift-etcd 0 (default)   Maters 透過 vCenter 建立 Master 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   \u0026gt; 網路參考 Bootstrap 設定流程。\n確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Masters 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=master.ign $ ./coreos-install.sh # 安裝完後 $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Masters 都開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug ... INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Bootstrap status: complete INFO It is now safe to remove the bootstrap resources INFO Time elapsed: 0s   在 bastion 執行以下指令來查看叢集狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ cp $HOME/ocp4/ignition/auth/kubeconfig $HOME/ocp4/kubeconfig $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-9xqqd 25m kubernetes.io/kubelet-serving system:node:etcd-1 Approved,Issued csr-gzvwr 27m kubernetes.io/kubelet-serving system:node:etcd-0 Approved,Issued csr-j6npt 25m kubernetes.io/kubelet-serving system:node:etcd-2 Approved,Issued csr-mxqkf 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-tth9f 28m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-zx9q8 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued system:openshift:openshift-authenticator 25m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-authentication-operator:authentication-operator Approved,Issued $ oc get no NAME STATUS ROLES AGE VERSION master-1.lab.yiylin.internal Ready master 31m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 15m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 10m v1.21.1+051ac4f   完成後，即可進行 Workers 與 Infras 部署，並移除 Bootstrap。若等待過久，可以透過 bastion 進入 masters 節點查看狀況:\n1  $ ssh core@maste-1.lab.yiylin.internal   Workers \u0026amp; Infras 透過 vCenter 建立 Workers \u0026amp; Infras 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui    網路參考 Bootstrap 設定流程。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Workers \u0026amp; Infras 節點:\n1 2 3 4 5 6 7 8  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=worker.ign $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Workers \u0026amp; Infras 都開啟完成後，就可以回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ openshift-install wait-for install-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be DEBUG Loading Install Config... DEBUG Loading SSH Key... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Cluster Name... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Networking... DEBUG Loading Platform... DEBUG Loading Pull Secret... DEBUG Loading Platform... DEBUG Using Install Config loaded from state file INFO Waiting up to 40m0s for the cluster at https://api.lab.yiylin.internal:6443 to initialize... DEBUG Still waiting for the cluster to initialize: Some cluster operators are still updating: authentication, console, ingress, monitoring    出現 DEBUG 即可往下操作，因為這部分會需要下載 Images 會比較久\n 開啟一個新的 Terminal，在 bastion 節點 Approve workers CSR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr -ojson | jq -r .items[] | select(.status == {} ) | .metadata.name | xargs oc adm certificate approve # 確認機器都起來 $ oc get no NAME STATUS ROLES AGE VERSION infra0 Ready worker 12m v1.21.1+051ac4f infra1 Ready worker 12m v1.21.1+051ac4f infra2 Ready worker 12m v1.21.1+051ac4f master0 Ready master 11h v1.21.1+051ac4f master1 Ready master 11h v1.21.1+051ac4f master2 Ready master 11h v1.21.1+051ac4f worker0 Ready worker 12m v1.21.1+051ac4f worker1 Ready worker 12m v1.21.1+051ac4f worker2 Ready worker 12m v1.21.1+051ac4f worker3 Ready worker 12m v1.21.1+051ac4f   搬移資源至 Infra nodes 接著要將 Worker 與 Infra 角色做拆分，確保系統與 Operator 服務不影響業務服務的負載，參考 Creating infrastructure machine sets\n設定 infra nodes 的 labels:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ for i in {1..2}; do oc label node worker-${i} node-role.kubernetes.io/app= --overwrite done $ for i in {1..3}; do oc label node infra-${i} node-role.kubernetes.io/infra= --overwrite oc label node infra-${i} node-role.kubernetes.io/worker- done $ oc get no NAME STATUS ROLES AGE VERSION infra-1.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-2.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-3.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f master-1.lab.yiylin.internal Ready master 156m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 140m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 135m v1.21.1+051ac4f worker-1.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f worker-2.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f   編輯 Scheduler 物件，並調整預設 NodeSelector:\n1 2 3 4 5  $ oc edit scheduler cluster ... spec: defaultNodeSelector: node-role.kubernetes.io/app= ...   建立 Infra 機器的 Machine Config Pool 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ cat \u0026amp;lt;\u0026amp;lt;EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - key: machineconfiguration.openshift.io/role operator: In values: - worker - infra nodeSelector: matchLabels: node-role.kubernetes.io/infra: EOF $ oc get mcp    這時 Infra 節點會重新啟動。\n 搬移 Router 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10  $ oc edit IngressController default -n openshift-ingress-operator ... spec: nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/infra: replicas: 2 $ oc -n openshift-ingress get pod -o wide   搬移 Image Registry 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8  $ oc edit configs.imageregistry.operator.openshift.io/cluster ... spec: nodeSelector: node-role.kubernetes.io/infra: ... $ oc -n openshift-image-registry get pods -o wide   搬移 Monitoring 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: grafana: nodeSelector: node-role.kubernetes.io/infra: k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: telemeterClient: nodeSelector: node-role.kubernetes.io/infra: openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: EOF $ watch -n 0.1 oc -n openshift-monitoring get pod -o wide   \u0026gt; 其他資源請參考 Moving OpenShift Logging resources 搬移。\n叢集初始化會需要一點時間，完成後就可以透過 Console 查看整體狀況。\nWeb Console 透過 oc 取得 Web console 的 route URL:\n1 2  $ oc -n openshift-console get route console -o=jsonpath={.spec.host}{\\n} console-openshift-console.apps.lab.yiylin.internal   取得 OpenShift 的 kubeadmin 密碼:\n1  $ cat $HOME/ocp4/ignition/auth/kubeadmin-password    Terminal Login 開啟一個新的 Terminal，在 bastion 節點配置 KUBECONFIG\n1 2  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get no   References  https://github.com/openshift/machine-config-operator/blob/master/docs/custom-pools.md https://gist.github.com/kairen/894348c6281f33a981b528e69c3d06bf https://docs.openshift.com/container-platform/4.8/installing/installing_vsphere/installing-restricted-networks-vsphere.html Networking Guide - BIND (Berkeley Internet Name Domain) 參考 IPI Prerequisites  ","date":"2021-09-26T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/ocp4-install/","title":"OpenShift 4.8.x UPI install on Bare metal"}]