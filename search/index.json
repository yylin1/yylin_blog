[{"content":"/概述/ 為什麼有這樣需求？ 近期剛好接觸客戶需求，希望能透過「單一叢集OpenSihft 運作於跨網段區域」上架構需求，希望有基於能限制不同角色，訪問流量進入 OpenShift 叢集只允許使用特定功能，包含以下需求：\n 能限制只有叢集管理人員/開發團隊，能直接存取操作 OpenShift 叢集於內部網路中，因此叢集對應的內部服務、API、儲存等，能獲得更多更安全性。 將提供外部應用程式/服務，隔離於對外 Security Zones 區域，並限制只允許特定的 Router 使用某些特定功能(例如：單純網站服務運行, APP 後端服務)等。   主要目的：希望透過 Shard Route 負責，在不同 IP 地址，讓內部和外部使用者訪問不同的 Router 以達到 DMZ 與 OCP 訪問限制，來減少成本下能更加提升叢集運作安全性。\n 補充 - 更多場景情境，歡迎大家可以參考 Luis Javier Arizmendi Alonso 分享針對 OpenShift 於 Security Zones 系列文章 - Security Zones in OpenShift worker nodes\n/探討/ 相關架構考量 先探討結論，為什麼不採用「針對 Security Zones 進行實體隔離」來達到企業內部制定相對安全標準和法規，例如直接在 DMZ 區域多部署一座 OpenShift 叢集提供對外服務，更直接保護叢集運行產品的應用及服務更能確保安全性，其實重點包含成本考量還有 OpenShift維護成本高，可能企業內部需要更多維運人員來管理不同 OpenShift 叢集上版流程、應用程式、API及相關服務配置管理等，都是總總問題。\n所以或許另一種選擇，透過有一個大型的 OpenShift「單一叢集跨網段架構」能使更容易維運管理及同時達到提高安全性架構，就是此次架構分享重點。\n 這邊還是一個重點，取決於你對於當前環境規劃與架構設計需求而做選擇\n 以下針對架構圖做細部「單一叢集跨網段架構」探討：\n 每座 OpenShift 叢集都應被視為僅在內部管理，而不是暴露於外部 Internet 若劃分 Infra Nodes 創建在不同的 Zone 區域中，每個區域中提供使用者運行 Ingress 等應用及 Services 服務的 Pod 將限制使用者無法訪問或直接使用其他叢集區域中的資源，直接保護這些節點及組件。  例如：Master API, Logging, Metrics, Registry 等應該運行於內部 Internet   針對特定 Zone 區域運行應用，如果有特定服務需求，可以使用 node-selectors 將 Pod 實體隔離到特定 Worker 節點  例如：上圖 Pod 跑在特定「藍色節點」上，只能單獨在藍色區域節點運行 Pod    單一叢集 OpenSihft 運作於跨網段區域 下圖 - 為-驗證規劃 OpenShift 示意圖：\n操作步驟  需創建對外服務的 Ingress Controller 要如何配置規範   Infra 節點依據網段區域定義標籤 DNS 配置對外 External Service 使用的 WildCard 創建 Ingress Controller 指派為外部應用使用 設置 route 透過標籤指派運行於對外區域的 infra 節點  服務與應用程式部署完成後，對外的 Router 要定義經過哪一個 Ingress 節點去提供服務   修改預設 Ingress Controller - default 配置不讓其他 Service 服務進入 新增對外服務的 Ingress Controller - service 如何配置  1. DNS 配置對外 External Service 使用的 WildCard    Usage FQDN     Kubernetes API api.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;   Internal API api-int.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;   Ingress route *.apps.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;   Application, External Service *.service.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;    2. 將 Router 固定到 Infra Node  將 Infra Node 依據服務提供區 Zone 1, Zone 2，指定標籤 (範例配置, region=8, region=10) 在 IngressController 對象的 matchLabels 中指定 Label 設置為對應 Infra Node  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ oc label nodes \u0026lt;INFRA_NODE\u0026gt; region=8 $ oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodePlacement\u0026#34;: {\u0026#34;nodeSelector\u0026#34;: {\u0026#34;matchLabels\u0026#34;: {\u0026#34;region\u0026#34;: \u0026#34;8\u0026#34;}}}}}\u0026#39; apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: default namespace: openshift-ingress-operator spec: replicas: 3 nodePlacement: nodeSelector: matchLabels: region: \u0026#34;8\u0026#34; $ oc label nodes \u0026lt;INFRA_NODE\u0026gt; region=10    檢查節點 label 狀態  1 2 3 4 5 6 7  $ oc get node -A --show-labels NAME STATUS ROLES AGE VERSION LABELS infra-1.ocp4.lab.example.com Ready worker 60d v1.23.5+3afdacb region=8, beta.kubernetes.io/arch=amd64,... infra-2.ocp4.lab.example.com Ready worker 60d v1.23.5+3afdacb region=8, beta.kubernetes.io/arch=amd64,... infra-3.ocp4.lab.example.com Ready worker 60d v1.23.5+3afdacb region=8, beta.kubernetes.io/arch=amd64,... infra-4.ocp4.lab.example.com Ready worker 60d v1.23.5+3afdacb region=10, beta.kubernetes.io/arch=amd64,... infra-5.ocp4.lab.example.com Ready worker 60d v1.23.5+3afdacb region=10, beta.kubernetes.io/arch=amd64,...   3. 創建 Ingress Controller 指派為外部應用使用  創建命名為 “Service” 的 Ingress Controller(Router) 將允許從外部訪問的應用程式或服務，引流到對應的 Ingress 如下圖 (指定的 Label region: ‘10’ )為對外使用的網域  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ cat ingress-service.yaml apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: service namespace: openshift-ingress-operator spec: replicas: 2 domain: service.ocp4.lab.example.com nodePlacement: nodeSelector: matchLabels: region: \u0026#39;10\u0026#39; routeSelector: matchExpressions: - key: type operator: In values: - service    這邊指定對外 External Service 使用的網域\n  設置 ingress  1  $ oc apply -f ingress-service.yaml   4. 限制對內對外服務 Ingress Controller 配置  修改預設 Ingress Controller - default 配置不讓其他 Service 服務進入 新增對外服務的 Ingress Controller - service 如何配置   設定 Ingress Controller - service 指定 service 服務可以進入\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ oc edit ingresscontroller service -n openshift-ingress-operator apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: service namespace: openshift-ingress-operator spec: replicas: 2 domain: service.ocp4.lab.example.com nodePlacement: nodeSelector: matchLabels: region: \u0026#39;10\u0026#39; routeSelector: matchExpressions: - key: type operator: In values: - service    檢查 Ingress Controller - default 限制不讓 service 服務進入\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ oc edit ingresscontroller default -n openshift-ingress-operator apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: service namespace: openshift-ingress-operator spec: replicas: 3 domain: apps.ocp4.lab.example.com nodePlacement: nodeSelector: matchLabels: region: \u0026#34;8\u0026#34; routeSelector: matchExpressions: - key: type operator: NotIn values: - service   5. 部署應用，生成 Route 並指定其 Hostname和 Label (type=service) 1 2  $ oc new-project demo $ oc new-app rails-postgresql-example   1 2 3 4 5 6 7 8  $ oc get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE postgresql-1-deploy 0/1 Completed 0 2d20h 10.129.6.16 worker-3.ocp4.lab.example.com postgresql-1-hjztg 1/1 Running 0 2d20h 10.129.4.98 worker-2.ocp4.lab.example.com rails-postgresql-example-1-build 0/1 Completed 0 2d20h 10.129.4.97 worker-2.ocp4.lab.example.com rails-postgresql-example-1-deploy 0/1 Completed 0 2d19h 10.129.8.9 infra-1.ocp4.lab.example.com rails-postgresql-example-1-f8w4l 1/1 Running 0 2d19h 10.129.3.18 worker-1.ocp4.lab.example.com rails-postgresql-example-1-hook-pre 0/1 Completed 0 2d19h 10.129.3.17 worker-1.ocp4.lab.example.com    在創建 route 的時候需指定 hostname，不可使用預設名稱，此部分跳過說明，可以看到如以下 route domain 為 .service.ocp4.lab.example.com 結尾。\n 1 2 3  $ oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD rails-postgresql-example rails-postgresql-example-demo.service.ocp4.lab.example.com rails-postgresql-example \u0026lt;all\u0026gt; None    #指定 label 讓 rails-postgresql-example 的 route 地址與設置解析\n 1  $ oc label route rails-postgresql-example type=service   6. 訪問 AP 的 Route 地址，確認狀態可以訪問  預設應用及服務走 default 網頁會讀取不到  1  $ oc label route rails-postgresql-example type-    增加外部服務對應 label (如範例 type=service)  1  $ oc label route rails-postgresql-example type=service   Reference  Deep dive of Route Sharding in OpenShift 4 Security Zones in OpenShift worker nodes — Part I — Introduction OpenShift 4 - Ingress、Route與Shard  ","date":"2022-09-27T12:00:40+08:00","permalink":"https://blog.yylin.io/openshift/security-zones/","title":"Deep dive - OpenShift 叢集節點跨網段架構規劃"},{"content":"概述 SCAP (Security Content Automation Protocol)是一包資安標準的集合。實作 SCAP 的工具有好幾種，其中一個是 OpenSCAP 開源工具，運用 OpenSCAP 可做到全自動的檢查、script/ansible playbook的報告與修復。但「修復」這段不太可能全自動，需要人為判斷要不要修復會不會與公司定義的規則互相衝突，否則完全聽從某些嚴格的 SCAP Rules，派下去之後反而造成維運及開發團隊環境配置衝突，導致相關影響及不便性。\n透過 OpenSCAP 和 Satellite 實現系統合規檢查 在 Red Hat Satellite 中，包含可以配置 OpenSCAP 項目提供的工具用於實施安全合規性檢查。有關 OpenSCAP 的更多信息，可以參見Guide to the Secure Configuration of Red Hat Enterprise Linux 7 和 Guide to the Secure Configuration of Red Hat Enterprise Linux 8。通過 Satellite Web UI，可以在 Red Hat Satellite 管理的所有主機上進行計劃的合規性檢查和直接查看對應報告。\nRed Hat Ansible Automation(簡稱 AAP) 可以協助維運團使 IT 環境和流程的各個方面實現自動化，而 AAP 則可以與其他服務整合。在以下實驗中我們透過影片方式，介紹如何透過 AAP 與 Satellite 來達成OpenSCAP的自動合規邏輯檢查：\n執行主要步驟包含：\n 在 Red Hat Satellite 中定義 OpenSCAP 檢查策略 使用 Red Hat Ansible Automation Platform 於系統主機上大量運行 OpenSCAP 邏輯檢查掃描   可以參考實作詳細資訊步驟-參考： Automated Smart Management Workshop\n   /補充/ OpenSCAP與資安合規生命週期 為什麼需要系統安全檢查強化工具，OpenSCAP 可以完美的參與整個資安合規的生命週期：\n 評估當前系統不合規的地方 將找到的問題，依重要性分類 修復（需要與企業內部定義規則評估） 產生報告   重覆以上輪迴直到某主機的安全強度到一可接受的程度)\n 更棒的是，這四階段可以通通 Ansible Automation Platform 自動化，用來解決下面的資安老問題：\n 做資安檢測費時費工 判斷問題、修復問題，容易出錯 資安規則太多，很容易漏掉 資安規則細如牛毛，要做好不容易 人工檢測，即便做成 SOP，也不容易實踐  Reference  OpenSCAP on Github: https://github.com/OpenSCAP/openscap https://www.open-scap.org/ https://aap2.demoredhat.com/exercises/ansible_smart_mgmt/ https://blog.51cto.com/u_15127570/2712917  ","date":"2022-09-25T12:00:40+08:00","permalink":"https://blog.yylin.io/ansible/openscap/","title":"實現 OpenSCAP 自動化邏輯檢查"},{"content":"OpenShift 4.9 開始正式推出提供單節點部署（Single Node OpenShift，SNO），以支援小型、全功能的企業級Kubernetes叢集的應用，常見客戶 POC 應用需求如資料中心伺服器資源有限情況下，單節點 OpenShift 部署型態能夠更容易處理，可協助企業擴充既有應用程式開發與部署規模，以及管理相關工作流程，更支援邊緣資料資料中心的執行需求。\n單節點部署可以使用 RHACM 或者在線的安裝引導進行安装，此篇以安裝引導進行 SNO 部署說明：\n1. 準備好本地的安裝環境：  本文章使用 Red Hat Virtualization虛擬化環境，部署 SNO 叢集的最低配置要求如 Prerequisite 下表資訊 生成 SSH 密鑰(用於SSH登陸) 配置部署的虛擬機 IP 或使用DHCP配置叢集網路及 DNS 伺服器解析  Prerequisite 主機資源要求\n   CPU Memory Disk     8vCPU 16GB 120GB     Requirements for installing OpenShift on a single node - minimum resource requirements\n DNS 設定    Usage FQDN     Kubernetes API api.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;   Internal API api-int.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;   Ingress route *.apps.\u0026lt;cluster_name\u0026gt;.\u0026lt;base_domain\u0026gt;     參考 How to try out single-node OpenShift from Red Hat 安裝指南：     2. 登入安裝 console.redhat.com, 創建 OpenShift 單節點部署 Installation - Assisted Install   開啟 https://console.redhat.com，選擇 OpenShift   點擊上方 Create Cluster   選擇 Datacenter 並點擊 Create Cluster   輸入 Cluster 相關參數，勾選 Install single node OpenShift   Host network 部分選擇 Static network configuration   輸入相關網路參數\n   此部分範例設置為 192.168.10.0/24 網段\n   點選 Add host   選擇 Minimal image file，並輸入對應 ssh public key 內容   下載 Discovery ISO   啟動 VM 並掛載該 ISO，等待其自動安裝與設定，完成後應該看到 Host Inventory 顯示如下   針對錯誤部分進行修改，此處問題為 hostname 不能為 localhost   確認沒問題後即可進行下一步   最後檢查一遍設定，確認無誤後點擊 Install cluster   等待安裝完畢\n   單節點安裝時間大約 40 分鐘\n     安裝完成後即可根據以下連線資訊使用 Single Node OCP\n   最終登入 OCP 查看狀態  Reference  Demo: How to try out single-node OpenShift from Red Hat Preparing to install on a single node OpenShift 4.10 - Installing OpenShift on a single node OpenShift Virtualization on a Single Node Cluster Meet single node OpenShift: Our newest small OpenShift footprint for edge architectures https://github.com/lees07/tech-docs/blob/master/e1-sno-by-assisted-installer.md https://www.youtube.com/watch?v=leJa9HmvdI0\u0026ab_channel=RyanNix  ","date":"2022-09-24T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/sno-installer/","title":"單節點 OpenShift 部署與應用探討"},{"content":"COSCUP x KCD Taiwan，今年終於變回實體線下活動，能透過線下活動人與人交流到處尋覓老朋友、聊現況，真的是每年最熱鬧的一刻。\n這次我們社群 CNTUG 申請主辦台灣地區的 Kubernetes Community Day (KCD)，KCD 為 CNCF 官方的活動，性質跟 KubeConf 有點不太一樣，KCD 主要是藉由各地區當地的社群來推廣 Cloud Native 以及 Kubernetes 文化的一個活動，所以會由各國家地區的社群來主持這個活動，而台灣地區的就是 KCD Taiwan。\n今年 KCD Taiwan 跟 COSCUP 2022 一起合作，所以可以看到 COSCUP 2022 官網會有 COSCUP x KCD Taiwan 2022 以及吉祥物，而有一個主議程軌 (7/30 AU 視聽館)是專門給 KCD Taiwan 的，有興趣的朋友可以參考議程表。\nTalk - 那些年我們在開源社群的日子 Cloud Native Taiwan 今年很榮幸跟社群夥伴 Po-Hsien 一起投稿開源新手村，投稿題目是 「那些年我們在開源社群的日子 - Cloud Native Taiwan」，主要是以社群志工角度，介紹我們對於參與開源社群的一些想法，整理一些建議給想參與社群的會眾。\n(照片來源: COSCUP 2022 官方相簿)\n我們認為社群就是 一群志同道合宅宅聚集而成的團體，目的為分享、學習、推廣、交流以及貢獻開源技術，有點類似學校的社團一樣，大家對於共同領域有熱情，並一起投入進去的感覺。\n如何參與社群? 在此 Talk 中我們整理了四個參與社群的簡單步驟:\n 尋找動機 尋找管道 參與社群 支持與貢獻  尋找動機 顧名思義，就是尋找你想參與社群的目的或是動機，這裡可以再細分成兩步。第一步可以先找出你有興趣的領域，例如系統、程式、硬體、資安、開源、產品或是平台都可以，也可以多個。找到有興趣的領域後，第二步就是尋找你的動機跟目的，想增進技術？想擴展人脈? 想交流或是請教技術？想要貢獻技術？還是想要增加眼界？這些都可以變成你加入社群的動機，可以統整下來，並以這些目標前進，但還是需要注意，目標已不影響他人為主！\n尋找管道 有了動機跟目的後，可以開始尋找資源，透過各種管道找到社群的資訊，這邊推薦幾種常見的管道:\n 台灣開源社群推廣目錄 社群平台 (Facebook、Twitter、Slack、Telegram 等等) 社群官方網站 各式研討會 (SITCON/COSCUP/HITCON/iThome…) 親朋好友或是實驗室跟學校社團  可以根據關鍵字或是地區來進行搜索，目前台灣有非常多的社群，可以直接參與。在這些方法中我個人很推薦參與研討會，參與研討會可以讓你見識不同領域的新技術以及認識各式各樣的公司或是社群，是一個很好的機會與人交流以及尋找自己的方向的地方，除了可以聽聽演講以外，許多社群或是公司也會有擺攤、Workshop 活動，可以多加參與進一步暸解社群以及文化。\n如果自己想要的領域比較冷門或是還沒有相關社群時，可以考慮自己創立一個社群，也許有人覺得創立社群很困難，一開始就要多大多複雜的組織架構，但其實回歸到最原始的定義，社群只是『一群對同樣領域有熱情的人組成的團體』，所以其實是可以從小團體開始經營，例如小型讀書會、聚會或是可以從建立社群平台粉專開始，等到穩定之後，再慢慢擴大規模，例如從幾個朋友之間，變成實驗室，再變成系上或是學校，甚至還可以開始跟其他社群合作或是投稿研討會增加社群能見度，都是不錯的方式。\n參與社群 找到適合的社群後，就可以積極的參與各項社群活動，包括:\n 社群活動 (Meetup/Conference/Workshop…) 與其他會眾交流 於各式活動支持社群攤位 加入社群平台社團交流 旁聽社群定期會議 擔任講者分享經驗 (增加自己的能見度)  可以多參加不同的社群，了解不同社群文化以及經營，也可以多方學習不同種類的技術，多與人交流，也許可以獲得不錯的寶貴經驗。\n不過大多數人是比較害羞內向的，在參與社群時，可能比較難以跟其他人交流，在這裡分享一個小技巧，從請教問題開始切入。一開始可以先當聽眾就好，不用刻意要找人聊天，專心參與以及聽分享，也許某一次的分享的內容你剛好很有共鳴或是有疑問，就可以去請教講者，這就是一個很好的交流切入點，有時候在與講者討論時，其他會眾也加入一起討論，就可以趁這機會多認識人，慢慢的在交流上就會更有自信，也可以提升人脈。\n擔任志工 若有閒暇之餘，也可以擔任社群志工，為社群經營盡一份力，志工主要負責以下這些項目:\n 協助籌辦各式活動 協助尋找講者 參與社群定期會議 管理社群相關營運事務 (社團、官方網站)  由於經營社群會佔用到一些個人的時間，因此可以根據自身狀況盡力協助，志工之間彼此互相 Cover。\n如何支持及貢獻開源技術社群 在參與社群一段時間後，可以選擇協助支持或是貢獻開源技術社群，支持有許多種形式，可以選擇最適合自己的方式:\n 積極社群活動 (Meetup/Conference/Workshop…) 使用及推廣開源技術 貢獻專案，成為 Contributor 或是 Member 於各式活動分享相關專案或技術經驗 擔任志工協助經營社群 贊助社群  參與社群後對生活及職涯的影響 參與社群除了能提升專業、自信以及能見度以外，我個人認為最重要的是人脈，能認識許多志同道合的朋友以及業界的夥伴，這不管在未來職涯還是生活上都很有幫助。若是參與志工，更能學習到如何組織活動、與其他人溝通及協同作業，這些都是實用且寶貴的經歷。\n套用社群朋友的一句話 社群就是一個很大的「舞台」，這個舞台提供了許多能夠讓人分享、交流以及貢獻的機會，並在這些過程中，不斷學習精進，透過社群內部的交流以及社群與社群間交流，互相學習補足各領域不足的資訊，讓整個社群圈變成一個知識共同體，讓技術不斷向邁進，這是我認為社群最棒的文化。\nReference  開源社群推廣目錄 OCF - 社群專案 [王景弘] 談談COSCUP：讓整個社群圈變成一個知識共同體。 用社群實踐開源精神，Denny 不平凡的技術社群人生－專訪 SITCON 共同發起人 Denny Huang  ","date":"2022-08-28T03:40:40+08:00","permalink":"https://blog.yylin.io/community/coscup2022/","title":"COSCUP 2022 - 那些年我們在開源社群的日子 - Cloud Native Taiwan"},{"content":"先決條件：  Bastion 安裝 Helm 當前 OpenShift 版本為 4.10+ Nvidia GPU Operator 已經完成安裝  啟用 NVIDIA GPU Operator Usage 資訊  添加 helm repo:  1  $ helm repo add rh-ecosystem-edge https://rh-ecosystem-edge.github.io/console-plugin-nvidia-gpu   更新 repo:  1  $ helm repo update   安裝 helm chart 於預設 NVIDIA GPU Operator namespace:  1 2 3 4 5 6 7 8  $ helm install -n nvidia-gpu-operator console-plugin-nvidia-gpu rh-ecosystem-edge/console-plugin-nvidia-gpu $ kubectl -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu # 啟用 plugin 執行以下 command: $ kubectl patch consoles.operator.openshift.io cluster --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/plugins/-\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;console-plugin-nvidia-gpu\u0026#34; }]\u0026#39; --type=json   查看部署的資源：  1  $ oc -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu   驗證 plugins 是否已指定  1  $ oc get consoles.operator.openshift.io cluster --output=jsonpath=\u0026#34;{.spec.plugins}\u0026#34;    如果未指定，則運行以下 command 以啟用 plugin：  1  $ oc patch consoles.operator.openshift.io cluster --patch \u0026#39;{ \u0026#34;spec\u0026#34;: { \u0026#34;plugins\u0026#34;: [\u0026#34;console-plugin-nvidia-gpu\u0026#34;] } }\u0026#39; --type=merge    如果指定，則運行以下 command 以啟用 plugin：  1  $ oc patch consoles.operator.openshift.io cluster --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/plugins/-\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;console-plugin-nvidia-gpu\u0026#34; }]\u0026#39; --type=json   在 OCP Web Console 頁面中（Home \u0026gt; Overview）就可以查閱 GPU utilization:\n Reference  https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/enable-gpu-op-dashboard.html  ","date":"2022-06-28T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/gpu-utilization/","title":"OpenShift 啟用 GPU Operator Dashboard"},{"content":"概述 從 ACM 2.5 版開始正式 GA 功能，您可以透過 ApplicationSets 來配置 ArgoCD / OpenShift-GitOps ，通過單一管理平台以可擴展的方式管理您的所有 GitOps Applications。\nApplicationSet Controller 是一個 Kubernetes Controller，它增加對 ApplicationSet CustomResourceDefinition (CRD) 的支持。\nApplicationSet Controller 在與 Argo CD 一起安裝時，通過添加額外的功能來支持以叢集管理員為中心的場景來補充它。\nApplicationSet Ccontroller 提供：\n  使用單個 Kubernetes 清單通過 Argo CD 從一個或多個 Git 存儲庫部署多個 Application 的能力\n  改進了對 monorepos 的支持：在 Argo CD 的上下文中，monorepo 是在單個 Git 存儲庫中定義的多個 Argo CD Application 資源\n  我們將介紹如何將 ACM 與 OpenShift GitOps 的 ApplicationSets 連接起來，以便在託管 Cluster 中配置和部署 OpenShift GitOps Application 和 ApplicationSet。\n環境配置-先決條件  我們需要使用 Operator Hub 在 ACM Hub 那座 OpenShift Cluster 中安裝 OpenShift GitOps：  1  $ until oc apply -k https://github.com/RedHat-EMEA-SSA-Team/ns-gitops/tree/bootstrap/bootstrap ; do sleep 2; done     參考 OpenShift GitOps 的官方文件說明\n  另一方面，我們需要配置管理不同的 Cluster (e.g. Public Cloud)。在我的例子中，我使用我環境中部署 2 座 OCP Cluster 叢集，並將在這篇文章中用於部署我的 Application。\n  於 OpenShift GitOps / ArgoCD 配置託管叢集 要在 ACM 中配置和鏈接 OpenShift GitOps，我們可以將一組一個或多個託管 Cluster 註冊到 Argo CD 或 OpenShift GitOps Operator 的實例。\n註冊後，我們可以使用從 ACM Hub Application Controller 的 Application 和 ApplicationSets 將我們需要部署的應用程式部署到這些叢集。然後，我們可以設置一個連續的 GitOps 環境，以在開發、暫存和生產環境中跨叢集自動化配置 Application 的一致性。\n 首先，我們需要創建 cluster sets 並將託管 clusters 添加到這些 cluster sets：  1 2 3 4 5 6 7  $ cat acmgitops/managedclusterset.yaml apiVersion: cluster.open-cluster-management.io/v1alpha1 kind: ManagedClusterSet metadata: name: all-openshift-clusters spec: {}     將託管叢集作為導入 Cluster 添加到 ClusterSet。您可以使用 ACM Console 或 CLI 導入：\n  創建託管 Cluster 綁定到部署 Argo CD 或 OpenShift GitOps 的 namespace\n  1 2 3 4 5 6 7 8 9 10 11  $ cat managedclustersetbinding.yaml apiVersion: cluster.open-cluster-management.io/v1alpha1 kind: ManagedClusterSetBinding metadata: name: all-openshift-clusters namespace: openshift-gitops spec: clusterSet: all-openshift-clusters $ oc apply -f managedclustersetbinding.yaml    在託管叢集綁定中使用的 namespace 中，創建放置自定義資源以選擇一組託管叢集以註冊到 ArgoCD 或 OpenShift GitOps Operator instance：  1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion: cluster.open-cluster-management.io/v1alpha1 kind: Placement metadata: name: all-openshift-clusters namespace: openshift-gitops spec: predicates: - requiredClusterSelector: labelSelector: matchExpressions: - key: vendor operator: \u0026#34;In\u0026#34; values: - OpenShift    注意：只有 OpenShift 叢集註冊到 Argo CD 或 GitOps Operator instance，而不是其他 Kubernetes 叢集。\n  創建一個 GitOpsCluster 自定義資源以將託管叢集從放置註冊到 Argo CD 或 OpenShift GitOps 的指定的 instance：  1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion: apps.open-cluster-management.io/v1alpha1 kind: GitOpsCluster metadata: name: argo-acm-clusters namespace: openshift-gitops spec: argoServer: cluster: local-cluster argoNamespace: openshift-gitops placementRef: kind: Placement apiVersion: cluster.open-cluster-management.io/v1alpha1 name: all-openshift-clusters namespace: openshift-gitops   這使 Argo CD instance 能夠將 Application 部署到任何 ACM Hub 託管叢集中。\n正如我們從前面的示例中看到的，placementRef.name 被定義為 all-openshift-clusters，並被指定為安裝在 argoNamespace：openshift-gitops 中的 GitOps 實例的目標叢集。\n另一方面，argoServer.cluster 規範需要 local-cluster 值，因為將使用部署在 OpenShift 叢集中的 OpenShift GitOps，該叢集也是安裝 ACM Hub 的位置。\n 幾分鐘後，我們在 ACM Hub 中生成了 GitOps Cluster CRD，我們將能夠直接從 Application 部分的 ACM Hub 控制台定義 Application 和ApplicationSet。  從 ACM Hub 部署 ArgoCD/OpenShift GitOps ApplicationSet 一旦我們通過 ACM Hub 中的 GitOps Cluster CRD 啟用了 OpenShift GitOps 和 ACM 之間的整合，我們就可以直接在 ACM Hub 中部署 ApplicationSet，在一個單一的頁面中管理所有 ArgoCD Application。\n另一方面，我們還將受益於ArgoCD ApplicationSets 的不同生成器的特性。\n使用這些生成方式，我們可以從不同叢集中的單個 Repository 部署多個 Application，利用 ApplicationSet 為每個託管的 Cluster 的從Repository 中的配置不同對象及要部署的 Application。\n讓我們在 ACM Hub 中生成 ApplicationSet。\n 使用 UI 為Application 集生成一個 ApplicationSet 示例：  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: acm-appsets namespace: openshift-gitops spec: generators: - clusterDecisionResource: configMapRef: acm-placement labelSelector: matchLabels: cluster.open-cluster-management.io/placement: acm-appsets-placement requeueAfterSeconds: 180 template: metadata: name: \u0026#39;acm-appsets-\u0026#39; spec: destination: namespace: bgdk server: \u0026#39;\u0026#39; project: default source: path: apps/bgd/overlays/bgdk repoURL: \u0026#39;https://github.com/yylin1/ns-apps/\u0026#39; targetRevision: single-app syncPolicy: automated: prune: true selfHeal: true   注意：目標 namespace 可以是 openshift-gitops。BGDK 可能會更改，但它會以這種方式離開，因為我們需要放置一個目標 namespace，即ApplicationSet 本身不需要它（Applicatio bgdk 也不需要）\n 結果是在 OpenShift GitOps 中生成但由 ACM Hub 管理的 ApplicationSet：  正如我們所看到的，被分配到兩個不同的 Cluster ，bm-germany 這 local-cluster 將是 Application 部署的地方，由 ApplicationSet 管理\nApplication 在之前定義 ApplicationSet 期間為每個叢集生成了與定義為 acm-appsets-placement 的 Placement 匹配的 ApplicationSet。還可以匹配 Cluster 進行 label，而不僅僅依賴於 Placement 對象。\n 在生成的 Application 中，每個Application都有自己的Application、Placement 和 Cluster，我們可以檢查：  因為我們可以檢查 ArgoCD Application 是否正確部署並由 BM-Germany 叢集中的 ACM AppSets 的 ApplicationSet 自動管理。此外，另一個 ArgoCD Application 將用於在與 Placement 匹配的另一個 Cluster 中部署另一個 Application 。\n正如我們之前所描述的，兩個 ArgoCD Application 是由與定義的 Placement 匹配的 ApplicationSet 生成的。\n 在 OpenShift GitOps / ArgoCD argo-controller 實例中，ACM 生成的 ApplicationSet 也生成了兩個 Argo Application ，並且為與 Placement 匹配的 ClusterSet 中管理的每個 Cluster 生成了每個 ArgoCD Application ：   注意：檢查指向在早期步驟中定義的不同託管 Cluster 的目標。\n  每個 Argo ApplicationSet 管理每個託管 Cluster 中的Application ，例如在 BM-Germany Cluster 中部署 BGDK Application 。  此 Application 將在託管 Cluster 中部署Application 清單，在這種情況下部署 bgdk Application 清單（路由、服務、部署等）。\n 在 ArgoCD/OpenShift GitOps 的設置中，在 ACM 使用 ClusterSet 管理的這些叢集。  這些是由 ACM Hub 中生成的 GitOps CRD 自動生成和管理的，它與託管 Cluster 對應。\nReference  https://github.com/RedHat-EMEA-SSA-Team/ns-gitops/ https://rcarrata.com/openshift/argo-and-acm/  ","date":"2022-06-25T09:40:40+08:00","permalink":"https://blog.yylin.io/openshift/argocd-and-acm/","title":"在 ACM 透過 OpenShift-GitOps/ArgoCD 管理應用程式"},{"content":"OpenShift Pipelines 是一個基於 Kubernetes 資源的雲塊的持續和持續交付（持續集成和持續交付，簡稱 CI/CD）的解決方案。它通過執行執行的細節，使用 Tekton 進行跨平台的自動部署。Tekton 引入了多種標準的自定義資源定義 (CRD)，定義可跨 Kubernetes 分佈用於 CI/CD 管道。\n主要特性  OpenShift Pipelines 是一個無服務器的 CI/CD 系統，它在獨立的容器中運行 Pipelines，以及所有需要的依賴組件。 OpenShift Pipelines 是為開發微服務架構的非中心化團隊設計的。 OpenShift Pipelines 使用標準 CI（pipeline）定義，這些與現有的 Kubernetes 工具集成擴展可擴展和擴展，可讓您定義和擴展 Kubernetes。 您可以通過 OpenShift Pipelines 使用 Kubernetes （如 Source-to-Image (S2I)、Buildah、Buildpacks 和 Kaniko）構建鏡像，這些工具可以移植到任何 Kubernetes 平台。 您可以使用 OpenShift Container Platform 開發運行（Developer Console）來創建 Tekton 資源，查看 Pipeline 的日誌，並管理 OpenShift Container Platform 設計空間中的管道。  在 Tekton pipeline 中有以下幾個主要的組成要素，分別是：\n PipelineResource Task \u0026amp; ClusterTask TaskRun Pipeline PipelineRun  PipelineResource PipelineResource 簡單來說可以作為 task 的 input or output，而每個 task 可以有多個 input \u0026amp; output。\nSyntax PipelineResource 的定義中會有以下必要資訊：\n  apiVersion：目前固定是 tekton.dev/v1alpha1\n  kind：因為這是 CRD，所以是 PipelineResource\n  metadata：用來辨識此 TaskRun 用的資訊，例如 name\n  sepc：使用 resource 的詳細資訊(例如：路徑、位址)\n  type：用來指定 resource type，目前支援 git, pullRequest, image, cluster, storage, cloudevent … 等等\n  其他選填項目：\n params：不同的 resource type 可能會有的不同額外參數資訊  Resource Type 有了以上概念後，接著要知道的是 PipelineResources 共有以下幾種類型：\n Git Resource Pull Request Resource Image Resource Cluster Resource Storage Resource Cloud Event Resource  以下就針對比較常用的 Git \u0026amp; Image resource 說明，其他的部份可以參考官網的詳細文件。\nGit Resource 一般的 git repository，作為 task input 時，Tekton 執行 task 前會將程式碼 clone 回來，因此這邊就必須注意 git repository 存取的權限問題，若是 private repository 就要額外提供 credential 資訊才可以正常運作\n以下是一個標準的 Git PipelineResource 的定義：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: wizzbang-git namespace: default spec: type: git params: - name: url value: https://github.com/wizzbangcorp/wizzbang.git # 可用 branch, tag, commit SHA or ref # 沒指定就會拉 master branch - name: revision value: master # value: some_awesome_feature # value: refs/pull/52525/head   Task \u0026amp; ClusterTask Task(\u0026amp; ClusterTask) 中包含了一連串的 step，通常是使用者要用來執行 CI flow，而這些工作會在單一個 pod 中以多個 container 的形式逐一完成。\nTask \u0026amp; ClusterTask 兩者的不同在於 Task 是屬於 namespace level，而 ClusterTask 是屬於 cluster level\n而在 Task 的定義中，最重要的部份有以下三個項目：\n Input Output Steps  以下是一個 task 的標準定義內容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: deploy-using-kubectl spec: inputs: resources: - name: source type: git - name: image type: image params: - name: path type: string description: Path to the manifest to apply - name: yamlPathToImage type: string description: The path to the image to replace in the yaml manifest (arg to yq) steps: # step 中可以定義多個執行工作，會依照順序執行 - name: replace-image image: mikefarah/yq command: [\u0026#34;yq\u0026#34;] args: - \u0026#34;w\u0026#34; - \u0026#34;-i\u0026#34; - \u0026#34;$(inputs.params.path)\u0026#34; - \u0026#34;$(inputs.params.yamlPathToImage)\u0026#34; - \u0026#34;$(inputs.resources.image.url)\u0026#34; - name: run-kubectl image: lachlanevenson/k8s-kubectl command: [\u0026#34;kubectl\u0026#34;] args: - \u0026#34;apply\u0026#34; - \u0026#34;-f\u0026#34; - \u0026#34;$(inputs.params.path)\u0026#34; # 此 volume 在 task 中沒有用到，只是一個範例而已 # 用以表示可以在 task 中定義 volume 並使用 volumes: - name: example-volume emptyDir: {}   TaskRun 定義了 task 之後，Tekton 並不會主動執行任何 task，這時候就必須要搭配 TaskRun 才可以讓 task 真正的執行指定工作。\n以下是一個標準的 TaskRun 定義：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # 以下幾個(apiVersion, kind, metadata, spec)是必要資訊 apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: name: build-docker-image-from-git-source-task-run spec: serviceAccount: robot-docker-basic # 指定到已經預先定義好的 Task taskRef: name: build-docker-image-from-git-source inputs: resources: - name: docker-source # 指定到已經預先定義好的 PipelineResource resourceRef: name: git-tekton-test params: - name: pathToDockerFile value: Dockerfile - name: pathToContext value: /workspace/docker-source/examples/microservices/leeroy-web outputs: resources: - name: builtImage resourceRef: name: image-tekton-test   Pipeline Pipeline 其實可以把它簡單思考為前面 Task 的集合，有順序性的排列，並透過之後介紹的 PipelineRun 來運作。\n成功運行 Pipeline 結果:\n失敗運行 Pipeline 結果：\n結語 以上內容(PipelineResource, Task, TaskRun, Pipeline, PipelineRun) 是 Tekton 中執行工作的必要元素，實際上執行的 CI/CD 工作都會與這幾個部份有關。\nTekton 將所有的基本元素拆分成一個一個的 k8s CRD(Custom Resource Definition)，如果是稍微複雜一點的 CI/CD 工作，可能就會需要定義不少個 CRD 才能完成，而且在設計上相對於其他的 CI server(例如：GitLab CI, Drone CI)可能不是這麼直覺；但這樣的設計提供了以下優點：\n  原生的 k8s 使用經驗，不需要額外學習其他語法\n  定義好的 CRD(PipelineResource, Task, Pipeline) 可以被重複利用\n  原生整合 k8s\n  若是未來有考慮 workload 都跑在 k8s 上的使用者，在選擇 CI/CD 的工具時或許可以將 Tekton 考慮進行。\n接著可能會面臨到的問題可能是，如果希望作到 GitOps，光是以上項目好像不夠還有相關元件支援性。\nReference  pipeline/examples PipelineResources  ","date":"2022-06-21T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/pipelines/","title":"CI/CD: Tekton Pipeline 實戰"},{"content":"ARO4 深入探討 - Microsoft Azure Red Hat OpenShift 4\n讓我們深入挖掘！\nMicrosoft Azure Red Hat OpenShift 服務支持部署完全託管的 OpenShift 集群。\nAzure Red Hat OpenShift 由 Red Hat 和 Microsoft 聯合設計、運營和支持，以提供集成的支持體驗。沒有虛擬機可以運行，也不需要打補丁。主節點、基礎架構和應用程序節點由 Red Hat 和 Microsoft 代表您進行修補、更新和監控。您的 Azure Red Hat OpenShift 集群已部署到您的 Azure 訂閱中，並包含在您的 Azure 賬單中。\n在 OpenShift 4 上部署 Azure Red Hat 時，整個集群都包含在一個虛擬網絡中。在這個虛擬網絡中，您的主節點和工作節點都位於各自的子網中。每個子網都使用一個內部負載均衡器和一個公共負載均衡器。\n這是有關 Azure Red Hat OpenShift 4 的官方圖表（可在 ARO4 Microsoft 頁面中找到）：\n 關於 ARO4 部署和管理的網絡和資源的更多詳細信息，請查看ARO 圖詳細信息 - 官方文檔  讓我們安裝我們的第一個 ARO4 集群！\nAzure 帳戶先決條件 首先，我們需要在 Azure 帳戶中設置幾項內容，例如生成 ServicePrincipals、增加限制以及定義要使用的區域。\n 按照配置 Azure 帳戶先決條件來定義和分配適當的 RBAC 權限並增加對 Azure 帳戶的限制。  為 ARO 安裝配置 Azure 基礎結構先決條件 當我們準備好上一步後，就可以在 Azure 中為我們的 ARO4 集群生成基礎資源先決條件了：\n 定義 ARO4 安裝的基本參數：  1 2 3 4 5 6  export LOCATION=eastus export RESOURCEGROUP=aro-rg export CLUSTER=rcarrata export VNET_CIDR=\u0026#34;10.0.0.0/22\u0026#34; export MASTER_SUBNET_CIDR=\u0026#34;10.0.0.0/23\u0026#34; export WORKER_SUBNET_CIDR=\u0026#34;10.0.2.0/23\u0026#34;    使用 az cli 登錄到 Azure：  1  az login   注意：當登錄彈出時，您需要在 Azure Dashboard 中使用您的憑據進行身份驗證。\n 註冊資源提供者：  1 2 3  az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait   建立 Azure Red Hat OpenShift 4 叢集 開始之前 Azure Red Hat OpenShift 至少需要 40 個核心，才能建立和執行 OpenShift 叢集。 新 Azure 訂用帳戶的預設 Azure 資源配額不符合這項需求。 若要要求增加資源限制，請參閱標準配額：VM 系列的增加限制。\n 例如，若要檢查最小支援的虛擬機器系列 SKU 「標準 DSv3」的目前訂用帳戶配額：  1 2 3 4  LOCATION=eastus az vm list-usage -l $LOCATION \\ --query \u0026#34;[?contains(name.value, \u0026#39;standardDSv3Family\u0026#39;)]\u0026#34; \\ -o table   1 2 3  CurrentValue Limit LocalName -------------- ------- -------------------------- 40 1000 Standard DSv3 Family vCPUs   驗證權限 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  Azure CLI quickstart: export GUID=hzdnk export CLIENT_ID=bdf5480a-c661-451f-a2ce-81e448ce0ba8 export PASSWORD=Zz842xzC.7GWZS_xVZ3oQvg1~7PtoIcKVA export TENANT=1ce7852f-dcf3-42bc-afe6-3bf81ab984fb export SUBSCRIPTION=ede7f891-835c-4128-af5b-0e53848e54e7 export RESOURCEGROUP=openenv-hzdnk curl -L https://aka.ms/InstallAzureCli | bash az login --service-principal -u $CLIENT_ID -p $PASSWORD --tenant $TENANT [ { \u0026#34;cloudName\u0026#34;: \u0026#34;AzureCloud\u0026#34;, \u0026#34;homeTenantId\u0026#34;: \u0026#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ede7f891-835c-4128-af5b-0e53848e54e7\u0026#34;, \u0026#34;isDefault\u0026#34;: true, \u0026#34;managedByTenants\u0026#34;: [ { \u0026#34;tenantId\u0026#34;: \u0026#34;b5ce0030-ec42-4a62-bc94-3025993e790c\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;RHPDS Subscription - OpenTLC Tenant\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bdf5480a-c661-451f-a2ce-81e448ce0ba8\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;servicePrincipal\u0026#34; } } ]    創建資源組為 ARO4 對象（如 vnet 和子網，以及自己的 ARO4 對象）分配資源：  1  az group create --name $RESOURCEGROUP --location $LOCATION    創建虛擬網絡：  1 2  az network vnet create --resource-group $RESOURCEGROUP \\ --name aro-vnet --address-prefixes $VNET_CIDR    為主節點添加一個空子網：  1 2 3 4 5 6  az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes $MASTER_SUBNET_CIDR \\ --service-endpoints Microsoft.ContainerRegistry    為工作節點添加一個空子網：  1 2 3 4 5 6  az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes $WORKER_SUBNET_CIDR \\ --service-endpoints Microsoft.ContainerRegistry    在主子網上禁用子網專用終結點策略：  1 2 3 4 5  az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true   安裝 ARO4  使用 azure cli 創建 ARO 集群：  1 2 3 4 5 6  echo \u0026#34;Creating ARO Cluster... Please wait 40mins\u0026#34; az aro create --resource-group $RESOURCEGROUP \\ --name $CLUSTER --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @pull-secret.txt   注意：安裝需要一個有效的 pull-secret。請訪問您的Cloud OpenShift Doshboard並獲取您的 pull-secret 令牌。\n 然後 az aro cli 將在 Azure Dashboard 中預配一個 ARO 對象：   自動地，它使用用於安裝的 Azure 對象創建了一個額外的 Azure 資源組，由 RH 和 MSFT 的 ARO SRE 管理：   在此資源組中，我們生成了提供和配置 ARO4 集群所需的資源：  訪問 API 和控制台  大約 40 分鐘後。我們將使用控制台和 API 準備好 ARO4 集群：  \n 要訪問集群，請列出集群的憑據：  1 2 3 4 5  echo \u0026#34;List credentials for ARO Cluster\u0026#34; az aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP echo \u0026#34;\u0026#34;    要顯示 ARO4 控制台：  1 2 3 4 5  echo \u0026#34;List console for ARO cluster\u0026#34; az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \u0026#34;consoleProfile.url\u0026#34; -o tsv    檢查 ARO4 API：  1 2  apiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) echo \u0026#34;This is the API for your cluster: $apiServer\u0026#34;    查詢 ARO Cluster 狀態  1 2 3 4  [root@bastion ~]# az aro list -o table Name ResourceGroup Location ProvisioningState WorkerCount URL --------- --------------- ---------- ------------------- ------------- ----------------------------------------------------------------- aro-demo openenv-hdpnp eastus Succeeded 3 https://console-openshift-console.apps.yylin.io/   ","date":"2022-03-13T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/aro/","title":"從零開始快速建置 - Microsoft Azure Red Hat OpenShift (ARO)"},{"content":"本文件如何安裝 OpenShift 4.8 版本至虛擬機器上，這邊將以 RHEL 環境來進行測試。\n RHEL 8 or Fedora machine with podman v3.3 installed Fully qualified domain name for the Quay service (must resolve via DNS, or at least /etc/hosts) Passwordless sudo access on the target host (rootless install tbd) Key-based SSH connectivity on the target host (will be set up automatically for local installs, in case of remote hosts see here) make (only if compiling your own installer)  當我們的基礎設施處於離線環境時，需要創建容器鏡像倉庫(Registry)來託管安裝 OpenShift Container Platform 所需的相關鏡像檔，來正常部署 OpenShift 叢集。\n這是有關 mirror registry for Red Hat OpenShift OpenShift 4 的官方流程圖（可在 Red Hat Blog 頁面中找到）：\n情境與使用目標 OpenShift 通常是在受控制離線網路的情況下運行 Production Cluster。而客戶要使用 Red Hat Quay 在 OpenShift 上運行優勢包含，有更多擴充性與部署元件配置整合性。要在離線環境安裝 OpenShift 前提條件，需要一個存放的安裝所需的相關鏡像的鏡像倉庫 (e.g., Quay, Harbor, Docker registry)，這邊就會有雞生蛋蛋生雞問題。\n解決方案/目標：\nRed Hat 提供一個 “mirror registry”工具，只針對 OpenShift 部署的引導(Bootstrap)鏡像倉庫，透過自動化腳本安裝程序，提供在 RHEL 8 (or Fedora) 的系統環境，快速部署一個精簡版的 Quay ，提供保存下載特定版本 OpenShift、OpenShiftHub 鏡像檔。mirror registry 提供針對離線環境或只是單純 PoC OpenShift 情境使用的 Registry，可以透過自動化腳本，快速部署設置安裝單節點(all-in-one)的 Quay ，所需要的相關手動繁瑣設定，例如：完整網域名稱 FQDN、使用者自定義 SSL/TLS 憑證、訪問權權限 SSH key 及運行的環境選擇。\n情境: 離線環境，安裝 OpenShift 運行流程:\n 在可連線外部網路環境中，透過 mirror registry 部署第一座容器鏡像倉庫(Online Mirror) 進行 OpenShift、OpenShiftHub 必要鏡像檔存放。 同樣在離線環境中，透過 mirror registry 部署第二座容器鏡像倉庫(Air-gapped Mirror)，從 Online Mirror 拷貝並要的映像檔存放於 Air-gapped Mirror 透過 Air-gapped Mirror 部署 OpenShift Production/Infra Cluster OpenShift 安裝 Quay Operator 提供內部服務及應用所需的鏡像存放  事前準備  從 OpenShift console Downloads 下載最新版本的 mirror-registry.tar.gz 準備 pull-secret.json 檔案  安裝  在當前環境安裝 mirror registry  1 2 3 4 5 6 7 8  $ vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 registry.yylin.demolab $ export HOSTNAME=\u0026#34;registry.yylin.demolab\u0026#34; $ ./mirror-registry install --quayHostname ${HOSTNAME} --ssh-key \u0026lt;~/.ssh/my_id_rsa\u0026gt;    最後輸出顯示 registry host 與登入資訊  1 2  INFO[2022-03-01 00:52:38] Quay installed successfully, permanent data are stored in /etc/quay-install INFO[2022-03-01 00:52:38] Quay is available at https://registry.yylin.demolab:8443 with credentials (init, xxxxxxxxxxxxxxxxxxxxxx)    產生 Registry Basic Auth 之認證資訊  1 2  # (init, xxxxxxxxxxxxxxxxxxxxxx) echo -n \u0026#39;\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;\u0026#39; | base64 -w0    在 pull-secret 加入本地 Registry 認證資訊  1 2 3  \u0026#34;registry.yylin.demolab:8443\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;aW5pdDo3TzlNV1hBMDhQcTJKZGh0M0M2elZJNU5EMWlsYkJ3NA==\u0026#34; }    驗證登入registry  1 2  $ podman login -u init --authfile pull-secret.json registry.yylin.demolab:8443 Error: authenticating creds for \u0026#34;registry.yylin.demolab:8443\u0026#34;: pinging container registry registry.yylin.demolab:8443: Get \u0026#34;https://registry.yylin.demolab:8443/v2/\u0026#34;: x509: certificate signed by unknown authority (possibly because of \u0026#34;crypto/rsa: verification error\u0026#34; while trying to verify candidate authority certificate \u0026#34;bastion.redhat.kubedev.org\u0026#34;)    安裝更新 quay 憑證   mirror registry 預設會配置好 CA 憑證，請將 PEM 文件格式添加憑證到系統中信任的 CA 列表中，這邊複製到 /usr/share/pki/ca-trust-source/anchors/ 目錄中\n 1  $ cp /etc/quay-install/quay-rootCA/rootCA.pem /usr/share/pki/ca-trust-source/anchors/rootCA.cert    更新系統範圍的信任儲存配置，請使用 update-ca-trust 命令：\n 1  $ update-ca-trust    再次登入mirror registry - Quay 確認憑證狀態是不是已經更新  1 2 3  $ podman login -u init --authfile pull-secret.json bastion.redhat.kubedev.org:8443 Password: Login Succeeded!   開始備份 OpenShift images  建立備份路徑  1  $ mkdir -p $HOME/openshift4/registry/images   匯入環境變數  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # vim upgrade-env export OCP_RELEASE=$(oc version -o json --client | jq -r \u0026#39;.releaseClientVersion\u0026#39;) export LOCAL_REGISTRY=\u0026#34;registry.yylin.demolab:8443\u0026#34; export LOCAL_REPOSITORY=\u0026#34;ocp4/openshift4\u0026#34; export PRODUCT_REPO=\u0026#34;openshift-release-dev\u0026#34; # /註記/ 請確認 pull-secret.json 裡面有包含 mirror rigistry 資訊 export LOCAL_SECRET_JSON=$HOME/mirror-registry/pull-secret.json export RELEASE_NAME=\u0026#34;ocp-release\u0026#34; export ARCHITECTURE=x86_64 # 指定 images 存放路徑 export REMOVABLE_MEDIA_PATH=\u0026#34;$HOME/openshift4/registry/images\u0026#34; # 匯入環境變數 source upgrade-env   備份 Image   /#1/ Review the images and configuration manifests to mirror:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} --dry-run   1 2 3 4 5 6 7  imageContentSources: - mirrors: - registry.yylin.demolab:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - registry.yylin.demolab:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev    /#2/ Mirror the images to a directory on the removable media:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --to-dir=${REMOVABLE_MEDIA_PATH}/mirror quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE}   1 2 3 4 5 6 7 8 9 10  info: Mirroring completed in 3m6.3s (66.22MB/s) Success Update image: openshift/release:4.10.0-rc.3-x86_64 To upload local images to a registry, run: oc image mirror --from-dir=/root/openshift4/registry/images/mirror \u0026#39;file://openshift/release:4.10.0-rc.3-x86_64*\u0026#39; REGISTRY/REPOSITORY Configmap signature file /root/openshift4/registry/images/mirror/config/signature-sha256-3d4ada825f4aa4d2.yaml created    /#3/ Take the media to the restricted network environment and upload the images to the local container registry:  1  $ oc image mirror -a ${LOCAL_SECRET_JSON} --from-dir=${REMOVABLE_MEDIA_PATH}/mirror \u0026#34;file://openshift/release:${OCP_RELEASE}*\u0026#34; ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  phase 0: registry.redhat.demolab:8443 ocp4/openshift4 blobs=334 mounts=0 manifests=163 shared=0 info: Planning completed in 2.12s uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:7253c5d7bf339222334565e0ec7ac88dd017dc9b9ec01654d26e8dda07408548 81.38MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:8e549eaeb17902a2a516d03776112ad7a9bfd10cd2d1c298fe31b1c15d3b4166 154.4MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:11456a2e94a652afe44f2645d83a051160119fd953b2459167cc88232adbf9ed 31.45MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:4ceffbad995c4d6e76142c6b46e7f2e06fcd2ddb6eb2c3624cc0244ae018356f 106.9MiB ... ... sha256:2d80cc5beffc10607a53eb6c013e87d03e267cdf0b8fc678b40acf5432957714 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-multus-networkpolicy sha256:df3b0ec40395ea460fbf2728ca7adff79dbaebddcffce003e88b3fb9cb2c9759 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-must-gather sha256:a340ea3d86560de8cf9ee2c1afe42ad24e5eefc9903a0073abe9dc54815bf710 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-console-operator sha256:f532e7f50cadfc757a6d277ab5476a31a4f24aac0afc615da6ff9f7ce5e2b538 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-cluster-image-registry-operator info: Mirroring completed in 1m17.77s (5.572MB/s)    /#4/ Directly push the release images to the local registry by using following command:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --apply-release-image-signature    更多配置請參考 Mirroring the OpenShift Container Platform image repository\n mirror 後結果  Reference  Mirroring on a local host with mirror registry for Red Hat OpenShift https://cloud.redhat.com/blog/introducing-mirror-registry-for-red-hat-openshift https://access.redhat.com/support/policy/updates/openshift#omr https://www.youtube.com/watch?v=j5e4OT71N0A  ","date":"2022-03-07T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/mirror-registry-for-ocp4-install/","title":"Disconnected Environment - Introducing Mirror Registry for Red Hat OpenShift"},{"content":"Nvidia GPU Operator v1.9 on OpenShift 4.9.9 包含以上版本，安裝不用再進行額外權限配置]\nOpenShift 4.9.9 或更高的版本 [1] 針對 driver toolkit 取消必要安裝要求:\n Set up an entitlement Mirror the RPM packages in a disconnected environment Configure a proxy to access the package repository  [1] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions\nOpenShift 4.9.8 與以下版本 [2] 需手動操作獲取 OCP 憑證，創建 MachineConfig 來認證 OCP 叢集，擴大授權 Images 使用權限範圍，來安裝 Nvidia Operator :\n 從 Red Hat Customer Portal 下載 Red Hat OpenShift Container Platform 訂閱憑證 (啟用權限需要登入 OCP 憑證）。 創建一個 MachineConfig 啟用訂閱管理平台並提供有效訂閱憑證。等待 MachineConfigOperator 重啟節點並完成 MachineConfig。 驗證叢集所有節點更新權限是否正常。   補充 - NVIDIA GPU Operator 安裝會部署幾個 Pod 服務，用於管理和啟用 GPU 在 OpenShift 中運作。其中一些 Pod 需要 OpenShift 使用一些非 Universal Base Image (UBI) 默認授權的 Images。必須在 OpenShift Cluster 中啟用信任的授權 Images，來啟動 NVIDIA GPU 驅動程式的容器運行。\n [1] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions [2] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/cluster-entitlement.html#enabling-a-cluster-wide-entitlemenent\nInstall Operator - Node Feature Discovery 1 2 3 4 5 6 7  [lab-user@bastion ~]$ oc get no NAME STATUS ROLES AGE VERSION ip-10-0-135-51.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f ip-10-0-142-219.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f ip-10-0-167-35.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f ip-10-0-186-251.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f ip-10-0-213-103.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f    要驗證實例是否已創建，請運行：  1 2 3 4 5 6 7 8  [lab-user@bastion ~]$ oc get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-6f65f47cf6-tg6gj 2/2 Running 0 24m nfd-master-d7cqw 1/1 Running 0 35s nfd-master-j42m9 1/1 Running 0 35s nfd-master-r64nv 1/1 Running 0 35s nfd-worker-24tzn 1/1 Running 0 35s nfd-worker-5rsg2 1/1 Running 0 35s    成功的部署會顯示一個Running狀態。  Installing the NVIDIA GPU Operator With the Node Feature Discovery Operator installed you can continue with the final step and install the NVIDIA GPU Operator.\nAs a cluster administrator, you can install the NVIDIA GPU Operator using the OpenShift Container Platform CLI or the web console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  kind: ClusterPolicy apiVersion: nvidia.com/v1 metadata: name: gpu-cluster-policy spec: dcgmExporter: config: name: \u0026#39;\u0026#39; dcgm: enabled: true daemonsets: {} devicePlugin: {} driver: enabled: true use_ocp_driver_toolkit: true repoConfig: configMapName: \u0026#39;\u0026#39; certConfig: name: \u0026#39;\u0026#39; licensingConfig: nlsEnabled: false configMapName: \u0026#39;\u0026#39; virtualTopology: config: \u0026#39;\u0026#39; gfd: {} migManager: enabled: true nodeStatusExporter: enabled: true operator: defaultRuntime: crio deployGFD: true initContainer: {} mig: strategy: single toolkit: enabled: true validator: plugin: env: - name: WITH_WORKLOAD value: \u0026#39;true\u0026#39;   Create the ClusterPolicy custom resource. This CRD will create several OCP resources. It will evaluate all the labels for the each node in the cluster and look for this:\n1 2  $ oc project nvidia-gpu-operator $ oc get pod -o wide   Validating the GPU availability 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  [lab-user@bastion ~]$ oc get pod | grep nvidia-device-plugin-daemonset nvidia-device-plugin-daemonset-bspfh 1/1 Running 0 21m nvidia-device-plugin-daemonset-n62dm 1/1 Running 0 21m [lab-user@bastion ~]$ oc exec -ti nvidia-device-plugin-daemonset-bspfh -- nvidia-smi Defaulted container \u0026#34;nvidia-device-plugin-ctr\u0026#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init) Fri Jan 28 06:47:21 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 | | N/A 32C P0 23W / 300W | 0MiB / 16160MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ ... ... $ oc exec -ti nvidia-device-plugin-daemonset-n62dm -- nvidia-smi Defaulted container \u0026#34;nvidia-device-plugin-ctr\u0026#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init) Fri Jan 28 06:47:44 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 | | N/A 26C P0 24W / 300W | 0MiB / 16160MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+   Running a sample GPU Application Run a simple CUDA VectorAdd sample, which adds two vectors together to ensure the GPUs have bootstrapped correctly.\n Run the following:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  cat \u0026lt;\u0026lt; EOF | oc create -f - apiVersion: v1 kind: Pod metadata: name: cuda-vectoradd spec: restartPolicy: OnFailure containers: - name: cuda-vectoradd image: \u0026#34;nvidia/samples:vectoradd-cuda11.2.1\u0026#34; resources: limits: nvidia.com/gpu: 1 EOF pod/cuda-vectoradd created   Check the logs of the container:  1 2 3 4 5 6 7  [lab-user@bastion ~]$ oc logs cuda-vectoradd [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done   Getting information about the GPU¶ The nvidia-smi shows memory usage, GPU utilization and the temperature of the GPU. Test the GPU access by running the popular nvidia-smi command within the pod.\nTo view GPU utilization, run nvidia-smi from a pod in the GPU Operator daemonset.\n Change to the nvidia-gpu-operator project:  1  $ oc project nvidia-gpu-operator   Run the following command to view these new pods:  1  $ oc get pod -owide -lopenshift.driver-toolkit=true   1 2 3  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nvidia-driver-daemonset-49.84.202201102104-0-gl557 2/2 Running 0 26m 10.131.0.106 ip-10-0-167-35.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nvidia-driver-daemonset-49.84.202201102104-0-k9sg5 2/2 Running 0 26m 10.128.2.17 ip-10-0-135-51.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Run the nvidia-smi command within the pod:  1  $ oc exec -it nvidia-driver-daemonset-48.84.202110270303-0-9df9j -- nvidia-smi   ","date":"2022-02-28T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/gpu-operator/","title":"NVIDIA GPU Operator on OpenShift4"},{"content":"2021 年有非常多的轉變與抉擇，是讓自己很深刻的一年，雖然年初有一些個人因素，影響後續許多計畫與安排，但之後從個人職涯選擇以及決定，為自己嘗試更多各種不同的挑戰，雖然過程很不容易，但能能充實的過完這一年。\n2021 回顧  協助前公司部分產品 Containerized 建置與Kuberentes - IT 初期工作流程基礎規劃，學習很多寶貴的維運經驗。 下半年決定為自己職涯嘗試新的挑戰，轉換職務，雖然過程花很多時間為認知與技能做轉換，過程參與事件活動非常多，但忙得有意義，非常充實。 在 COSCUP 2021 協助 CNTUG 社群議程軌-開源運河上的雲原生號擔任主持人：首次 COSCUP 線上虛擬舉行，體會主持人如何引導議程進行時，與講者會眾互動 Q\u0026amp;A ，並完成整個整個議程軌活動，難得且充實的年會體驗。 擔任 HPC Summit 2021 講者：有機會分享抽空研究DeepOps 的專案，如何對於 AI/ML Infra 建置的部署工具與企業場景實際應用分析(簡報連結)。 擔任 Kuberentes Summit 2021 - WorkShop 講者: 學習如何用會眾角度規劃整個工作坊設計。 回歸山岳攝影-登山健行: 持續撰寫登山系列文章，跟一群好友組了登山興趣小組，開始嘗試(台灣中級山系列、台北大縱走路線完成4/7段[持續進行中])，脫離城市的大自然體驗。 自己動手做一顆手工肥皂，感謝 SU VIDA x 私 生活 開設的工作坊 攀岩(抱石) 維持 LV2 等級都順利完攀，持續往下一級前進。 終於換了新一台相機 Sony A7 IV  2022 展望  增強專業能力部分，提升各方面不足點 花更多時間在技術文章撰寫 完成先前未完成的證照 回歸開源專案貢獻，積極參與 Conference / Meetup 分享 持續健身運動維持飲食控制，為百岳做後續準備  Related Posts  2020-年度回顧 (舊 Blog 搬家中)   2022 繼續爬山   Photography Blog (https://medium.com/yiyang-lins-life)  ","date":"2021-12-31T13:33:28+08:00","permalink":"https://blog.yylin.io/year/2021-retrospect-and-prospect/","title":"2021 年度回顧"},{"content":"事前準備： 開始前，需要先準備以下資訊與要求。\n啟用 ROSA  開始配置 AWS 服務前，使用者必須以 IAM 身份(參考文件: Customer Requirements)\n 登入 AWS 管理控制台，確認當前使用者已啟用 Red Hat OpenShift服務 下載 AWS CLI 與 OpenShift CLI 工具: URL Dowload\n  選擇當前環境下載對應CLI ( Command-line interface (CLI) tools)\n  or\nCommand Dowload\n 下載 AWS/OpenShift CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:  1 2 3 4 5 6 7 8 9 10 11 12 13 14  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/rosa/latest/rosa-linux.tar.gz -O - | tar -xz $ mv rosa /usr/local/bin/ $ rosa version 1.1.1 $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 $ oc version Client Version: 4.8.10 Server Version: 4.8.12 Kubernetes Version: v1.21.1+d8043e1    創建 Red Hat 帳號 OpenShift 由 Red Hat SaaS 提供，因此您需要創建一個 Red Hat 帳戶。 https://cloud.redhat.com/\n  透過 ROSA 指令檢查憑證與資源配置 這邊以 Red Hat 部署 ROSA 文件部署為主 (Red Hat OpenShift Service on AWS - Creating a ROSA cluster)\n首先，使用 ROSA CLI 檢查 AWS 憑證\n1 2 3  $ rosa verify permissions I: Validating SCP policies... I: AWS SCP policies ok   第一次登錄時，你會需要一個 Token 來登入你的 Red Hat 帳號\n 需要創建一組 Red Hat 帳號\n 確認登入訊息：\n1 2  $ rosa login I: Logged in as \u0026#39;xxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39;   配置完成後可以透過 rosa whoami 來檢查你目前所有登錄憑證與狀態\n Red Hat 帳號會附帶 OpenShift Cluster Manager (OCM) 資訊與你的 AWS 帳戶顯示。\n 1 2 3 4 5 6 7 8 9 10 11 12  $ rosa whoami AWS Account ID: xxxxxxxxxxxx AWS Default Region: us-west-2 AWS ARN: arn:aws:iam::xxxxxxxxxxxx:user/user OCM API: https://api.openshift.com OCM Account ID: xxxxxxxxxxxxxxxxxxxxxx OCM Account Name: xxxxxx OCM Account Username: xxxxx@xxxxx.com OCM Account Email: xxxxx@xxxxx.com OCM Organization ID: xxxxxxxxxxxxxxxxxxxxxx OCM Organization Name: Red Hat OCM Organization External ID: xxxxxxxx   以上檢查沒問題後，接下運行 rosa init 運行指令來確保 ROSA 配置與相關資源狀態沒問題\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ rosa init I: Logged in as \u0026#39;xxxxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39; I: Validating AWS credentials... I: AWS credentials are valid! I: Validating SCP policies... I: AWS SCP policies ok I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html I: Ensuring cluster administrator user \u0026#39;osdCcsAdmin\u0026#39;... I: Admin user \u0026#39;osdCcsAdmin\u0026#39; already exists! I: Validating SCP policies for \u0026#39;osdCcsAdmin\u0026#39;... I: AWS SCP policies ok I: Validating cluster creation... I: Cluster creation valid I: Verifying whether OpenShift command-line tool is available... I: Current OpenShift Client Version: 4.8.10    需要驗證當前環境，是否已經登入 AWS並確認透過rosa init 來確認 AWS 資源與配置是否能足夠創建 OpenShift\n 申請 AWS 帳戶配額 在配置 ROSA 時作為預先檢查 $ rosa verify quota --region=${cluster} 叢集名稱。如果您在剛剛部署但未執行任何操作的帳戶上運行它，您將收到錯誤訊息 EC2 quota 配置不足。\n1 2 3 4 5  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... E: Insufficient AWS quotas E: Service quota is insufficient for the following service quota codes: - Service ec2 quota code L-1216C47A Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances not valid, expected quota of at least 100, but got 5   這邊需要額外申請 AWS EC2 限制增加配額 需要提供申請需求與相關資訊，才能申請限制配額成功\n相關配額資訊參考\n AWS service quotas Required AWS service quotas  以上完成申請後，再次驗證\n1 2 3  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.htm    透過 ROSA 指令快速創建 Red Hat OpenShift  透過 rosa create cluster 開始部署 OpenShift Cluster (部署Cluster 大約需要30-40分鐘)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  $ rosa create cluster --cluster-name=rosacluster I: Creating cluster \u0026#39;rosacluster\u0026#39; I: To view a list of clusters and their status, run \u0026#39;rosa list clusters\u0026#39; I: Cluster \u0026#39;rosacluster\u0026#39; has been created. I: Once the cluster is installed you will need to add an Identity Provider before you can login into the cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: To determine when your cluster is Ready, run \u0026#39;rosa describe cluster -c newsblogcluster\u0026#39;. I: To watch your cluster installation logs, run \u0026#39;rosa logs install -c newsblogcluster --watch\u0026#39;. Name: newsblogcluster ID: XXXXXXXXXXXXXXXXXXXX External ID: OpenShift Version: Channel Group: stable DNS: newsblogcluster.phnh.p1.openshiftapps.com AWS Account: 123456789012 API URL: Console URL: Region: xxxxxxxxxxx Multi-AZ: false Nodes: - Master: 3 - Infra: 2 - Compute: 2 (m5.xlarge) Network: - Service CIDR: 172.30.0.0/16 - Machine CIDR: 10.0.0.0/16 - Pod CIDR: 10.128.0.0/14 - Host Prefix: /23 State: pending (Preparing account) Private: No Created: xxxxxxxxxxxxxxxx Details Page: https://cloud.redhat.com/openshift/details/XXXXXXXXXXXXXXXXXXXX   執行命令後，您可以透過 rosa describe cluster 或 rosa logs install 指令檢查 OpenShift Cluster 狀態。您還可以從 URL 訪問 OpenShift Console 進行確認。\n1 2 3 4 5 6 7 8 9  $ rosa create admin -c rosacluster W: It is recommended to add an identity provider to login to this cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: Admin account has been added to cluster \u0026#39;rosacluster\u0026#39;. I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user. I: To login, run the following command: oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z I: It may take up to a minute for the account to become active.   1 2 3 4 5 6  $ oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z Login successful. You have access to 87 projects, the list has been suppressed. You can list all projects with \u0026#39;oc projects\u0026#39; Using project \u0026#34;default\u0026#34;.   1 2 3 4 5 6 7 8 9  $ oc get no NAME STATUS ROLES AGE VERSION ip-10-0-153-95.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1 ip-10-0-171-119.us-west-2.compute.internal Ready infra,worker 17m v1.21.1+d8043e1 ip-10-0-175-165.us-west-2.compute.internal Ready infra,worker 18m v1.21.1+d8043e1 ip-10-0-202-234.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-239-231.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-242-134.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-252-208.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1   刪除 Cluster rosa delete cluster 可以使用指令刪除使用 ROSA 創建的cluster。順便說一下，集群的刪除在大約 10 分鐘內完成。\n1 2 3 4  $ rosa delete cluster --cluster=rosacluster ? Are you sure you want to delete cluster rosacluster? Yes I: Cluster \u0026#39;rosacluster\u0026#39; will start uninstalling now I: To watch your cluster uninstallation logs, run \u0026#39;rosa logs uninstall -c rosacluster --watch   Reference  rosaworkshop.io  ","date":"2021-09-30T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/rosa/","title":"Red Hat OpenShift Platform on AWS (ROSA) 快速部署上手"},{"content":"延伸上一篇 OCP UPI 部署(OpenShift 4.8.x UPI install on Bare metal)過程遇到問題彙整。\n 備註: 由於資源有限情況下，透過單一伺服器(主機)透過 VM 模擬實際節點硬體資源環境，這邊主要透過檢測與除錯，來確保 OpenShift 部署成功。\n 部署常見問題整理 (持續更新) 超時等待 bootstrap 節點啟動  發現特定 Pod 持續RunningNotReady 狀態  1 2 3  W0830 11:16:37.087497 35214 reflector.go:436] k8s.io/client-go/tools/watch/informerwatcher.go:146: watch of *v1.ConfigMap ended with: very short watch: k8s.io/client-go/tools/watch/informerwatcher.go:146: Unexpected watch close - watch lasted less than a second and no items received apiserver - \u0026gt; RunningNotReady Aug 30 03:42:37 bootstrap.lab.yiylin.internal bootkube.sh[644199]: Pod Status:openshift-kube-apiserver/kube-apiserver RunningNotReady    此情境請透過收集bootstrap 節點日誌，來確認環境問題\n 1 2 3 4  $ ./openshift-install gather bootstrap --dir=\u0026lt;installation_directory\u0026gt; ... INFO Pulling debug logs from the bootstrap machine INFO Bootstrap gather logs captured here \u0026#34;\u0026lt;installation_directory\u0026gt;/log-bundle-\u0026lt;timestamp\u0026gt;.tar.gz\u0026#34;    蒐集包含 bootstrap process / bootstrap 上所有 Pod/Container 的 log 下載完後解壓縮 .tar.gz 檔案，針對可能部署或未啟動的Pod檢查log錯誤狀態，來找到對應的錯誤資訊   補充說明 bootstrap 主要分成兩個動作\n 安裝自己跟建立自己的 cluster(Pod) 引導安裝 master x 3 master bootstrap 全部安裝完，然後下 wait for complete 確認最終才可移除 bootstrap node   Reference:  Gathering logs from a failed installation Waiting for the bootstrap process to complete  Master Node 部署不起來，檢查並驗證 etcd 運行之硬碟是否正常運作 ?  大部分 master 無法建置情境，通常可能會有原因是硬碟速度問題(硬碟壞軌)   造成叢集有時候部署成功有時候會失敗的情況\n 透過簡單運行 fio 來測試 etcd benchamrk performance 檢測\n1 2 3 4  $ oc debug node/\u0026lt;master_node\u0026gt; [...] sh-4.4# chroot /host bash [root@\u0026lt;master_node\u0026gt; /]# podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf   Reference:  How to Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance in OCP  [參考影片說明] OpenShift 4 如何使用 fio 驗證 etcd 運行之硬碟是否可以正常運作 / Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance Hardware requirements and recommendations  補充|檢查 - 除錯流程   部署過程都透過 bootkube 確認部署狀態資訊，log 會不斷顯示錯誤然後 loop 確認環節，嘗試能不能從他錯誤訊息找出規則(也就是他跳錯誤的地方，通常是在 log restart之前)\n  嘗試 SSH 到 Master Node，透過 netstat -plnt 檢查三個服務，這三個通常只要好，這台 Master 就沒問題 (6443| 22623|2379|2380)，分別是 OpenShift API/Kubernetes API/ETCD/Machine Config Operator\n  如果以上幾個 Pod 有問題，透過 crictl ps 跟 crictl log 去查看 Container Log，問題有可能是連續的錯誤，所以可能是 API 有問題，可能是 Controller/Scheduler 引起，細節需要透過上面提到方式，下載節點所有 Log 日誌來比對檢查，找到正確的錯誤資訊來排除錯誤\n  Reference:  OpenShift Troubleshooting Resources  ","date":"2021-09-29T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/install-troubleshooting/","title":"OpenShift Intsall Troubleshooting Notes"},{"content":" [註記] 由於硬體環境資源有限，本實驗是透過 Proxmox Environment 單一伺服器節點來模擬部署 OpenShift 叢集，並依據官網配置建議硬體需求(Server sizing)資源。 [共同撰寫編輯]: Kyle Bai\n 部署版本說明：  Proxmox Environment v6.1-7  CPU: 8-Core, 16-Thread Unlocked / Memory: 128 GB   OpenShift v4.8.x.  Reference  參考官網文件 - Installing a user-provisioned cluster on bare metal  事前準備 開始前，需要先準備以下資訊與要求。\nDownload URLs  User-provisioned infrastructure: 可下載 OpenShift Installer、CLI 與 Pull secrets(需要登入 Red Hat 註冊帳號權限) Public OpenShift 4.8 Mirror: 可下載 OpenShift Installer、CLI 與 OPM。  openshift-client-linux.tar.gz: 包含 oc 與 kubectl CLI 工具。 openshift-install-linux.tar.gz: 包含 openshift-install 工具。用於建立 OpenShift auth、igntion 等設定檔案。 opm-linux.tar.gz: 用於管理 Operator Hub 與 mirror 相關 images。   Public RHCOS 4.8 Mirror: 可下載 RHCOS 4.8 相關 VM images。  若手動安裝以下載 rhcos-live.x86_64.iso 為主。若想要控管使用版本，請選擇有標示版本的 live iso。  FET vSphere 安裝請以這方式進行。   若以 PXE 方式安裝，則下載以下映像檔:  rhcos-live-initramfs.x86_64.img rhcos-live-kernel-x86_64 rhcos-live-rootfs.x86_64.img       過程中會透過 wget 下載，因此在有網路狀況下，不需要預先下載。\n  Network and Hosts Info  OpenShift subnet CIDR: 192.168.101.0/24 OpenShift Pod Network(CNI): 10.128.0.0/14 OpenShift Service Network: 172.3.0.0/16 Domain: lab.yiylin.internal     Hosts IP CPU RAM     bashtion 192.168.101.9 2 vCore 4G   bootstrap 192.168.101.10 4 vCore 8G   master-1 192.168.101.21 4 vCore 8G   master-2 192.168.191.22 4 vCore 8G   master-3 192.168.191.23 4 vCore 8G   worker-1 192.168.191.31 4 vCore 8G   worker-2 192.168.191.32 4 vCore 8G   infra-1 192.168.191.41 4 vCore 8G   infra-2 192.168.191.42 4 vCore 8G   infra-3 192.168.191.43 4 vCore 8G     這裡為了方便測試，將 PXE / HAProxy / DNS Server 等都放在 bastion上。 [額外參考資訊]各元件節點最小資源可參考 Minimum resource requirements。 請先透過 Proxmox Environment 開啟對應 hardware VM。\n 補充 Network 說明  其中 OpenShift subnet CIDR 請修改為自己當前 Proxmox Environment 網路配置)  Bastion 透過 Live CD 安裝 RHEL 8 版本(或使用 CentOS 8)，並確保網路設定正確。\n RHEL 需要確認 subscription 驗證，確保 repo 源可正常取得\n 事前準備 進入 bastion 機器，安裝 jq 與 wget 工具等:\n1 2  $ sudo su - $ yum install -y wget jq vim   關閉 bastion 防火牆:\n1 2  $ systemctl stop firewalld $ systemctl disable firewalld   \u0026gt; 這邊方便測試，所以直接關閉 :D。\n設定與關閉 SELinux:\n1 2 3 4  $ vim /etc/selinux/config SELINUX=disabled $ setenforce 0    這邊方便測試實驗，所以直接關閉。\n 建立 SSH Keys:\n1  $ ssh-keygen -t rsa    這邊方便測試，以 root 來建立。\n 導入以下環境變數:\n1 2  export DOMAIN=lab.yiylin.internal export OCP_NETWORK_CIDR=192.168.101.0/24   下載 OpenShift Installer 與 CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:\n1 2 3 4 5 6 7 8 9 10 11  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 # Installer $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-install-linux.tar.gz -O - | tar -xz $ mv openshift-install /usr/local/bin/ $ openshift-install version openshift-install 4.8.2   安裝與設定 DNS Server 安裝 DNS server 套件，這邊使用 dnsmasq 來提供服務:\n1  $ yum -y install dnsmasq bind-utils    這部分也可參考 OCP 官網安裝 BIND，關於不同 DNS Server 相關配置方法，會額外補充於後續文章介紹。\n 編輯 /etc/dnsmasq.d/openshift.conf 設定檔，以設定 A Record 與 PTR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/dnsmasq.d/openshift.conf server=8.8.8.8 domain=${DOMAIN},${OCP_NETWORK_CIDR},local # A Records(api) host-record=api.${DOMAIN}, 192.168.101.9 host-record=api-int.${DOMAIN}, host-record=api-int.${DOMAIN},192.168.101.9 # A Records(hosts) host-record=bastion.${DOMAIN},192.168.101.9 host-record=bootstrap.${DOMAIN},192.168.101.10 host-record=master-1.${DOMAIN},192.168.101.21 host-record=master-2.${DOMAIN},192.168.101.22 host-record=master-3.${DOMAIN},192.168.101.23 host-record=worker-1.${DOMAIN},192.168.101.31 host-record=worker-2.${DOMAIN},192.168.101.32 host-record=infra-1.${DOMAIN},192.168.101.41 host-record=infra-2.${DOMAIN},192.168.101.42 host-record=infra-3.${DOMAIN},192.168.101.43 # A Records(etcd) host-record=etcd-0.${DOMAIN},192.168.101.21 host-record=etcd-1.${DOMAIN},192.168.101.22 host-record=etcd-2.${DOMAIN},192.168.101.23 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-0.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-1.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-2.${DOMAIN},2380,0,10 # PTRs address=/apps.${DOMAIN}/192.168.101.9 address=/.apps.${DOMAIN}/192.168.101.9 address=/api.${DOMAIN}/192.168.101.9 address=/api-int.${DOMAIN}/192.168.101.9 address=/bastion.${DOMAIN}/192.168.101.9 address=/bootstrap.${DOMAIN}/192.168.101.10 address=/master-1.${DOMAIN}/192.168.101.21 address=/master-2.${DOMAIN}/192.168.101.22 address=/master-3.${DOMAIN}/192.168.101.23 address=/etcd-0.${DOMAIN}/192.168.101.21 address=/etcd-1.${DOMAIN}/192.168.101.22 address=/etcd-2.${DOMAIN}/192.168.101.23 address=/worker-1.${DOMAIN}/192.168.101.31 address=/worker-2.${DOMAIN}/192.168.101.32 address=/infra-1.${DOMAIN}/192.168.101.41 address=/infra-2.${DOMAIN}/192.168.101.42 address=/infra-3.${DOMAIN}/192.168.101.43 EOF   啟動 dnsmasq 服務，並檢測 dnsmasq 設定是否正確:\n1 2 3  $ systemctl enable dnsmasq; systemctl start dnsmasq $ dnsmasq --test dnsmasq: syntax check OK.   編輯 /etc/resolv.conf 檔案，修改 nameserver:\n1 2 3  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/resolv.conf nameserver 127.0.0.1 EOF   驗證 DNS server 正反解:\n1 2 3 4 5  # dig domain @dns_server $ dig bastion.${DOMAIN} @192.168.101.9 # dig -x ip @dns_server $ dig -x 192.168.101.9 @192.168.101.9    請驗證填寫自己對應的 ip address\n 安裝與設定 HTTP Server 安裝與設定 httpd 套件:\n1  $ yum -y install httpd   編輯 /etc/httpd/conf/httpd.conf 設定檔:\n1 2  #Listen 12.34.56.78:80 Listen 8080\t   因為 80 是 Ingress-http 使用，除非將 HAProxy 拆成不同節點或用 External LB。\n 啟動 httpd 服務:\n1  $ systemctl enable httpd; systemctl start httpd   產生 OpenShift Ignition fils 建立放置 OpenShift Config 檔案的目錄:\n1  $ cd $HOME ; mkdir $HOME/ocp4/ignition;cd $HOME/ocp4/ignition   建立與編輯 install-config.yaml 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v1baseDomain:yiylin.internalcompute:- hyperthreading:Enabledname:workerreplicas:0controlPlane:hyperthreading:Enabledname:masterreplicas:3metadata:name:labnetworking:clusterNetworks:- cidr:10.128.0.0/14hostPrefix:24networkType:OpenShiftSDNserviceNetwork:- 172.3.0.0/16platform:none:{}pullSecret:\u0026#39;{\u0026#34;auths\u0026#34;:{ YOUR_PULL_SECRETE }}\u0026#39;sshKey:\u0026#34;SSH_PUB_KEY\u0026#34;    Pull secret 可在 Create Cluster by user provisioned 取得。 Proxy 設定，參考 Configuring the cluster-wide proxy during installation。 SSH Key 請自行於 bastion 創建填寫，這裡不多做說明   產生 manifests 相關檔案:\n1 2 3 4  $ cp install-config.yaml install-config.yaml.bak $ openshift-install create manifests --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition manifests openshift   編輯 manifests/cluster-scheduler-02-config.yml 檔案，設定 mastersSchedulable 值，以確保 master 不作為工作負載節點:\n1 2 3 4  ...spec:mastersSchedulable:false...  產生 OCP 使用的 Ignition files:\n1 2 3  $ openshift-install create ignition-configs --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition auth bootstrap.ign master.ign metadata.json worker.ign   撰寫方面安裝 RHCOS 的腳本 $HOME/ocp4/ignition/coreos-install.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/bin/bash  set -eu export DEVICE_NAME=${DEVICE_NAME:-/dev/sda} export IGNITION_FILE=${IGNITION_FILE:-bootstrap.ign} # bootstrap.ign master.ign worker.ign  export IGNITION_URL=${IGNITION_URL:-http://192.168.101.9:8080/ignition/${IGNITION_FILE}} sudo coreos-installer install ${DEVICE_NAME} \\  --insecure-ignition \\  --ignition-url=${IGNITION_URL} \\  --firstboot-args rd.neednet=1 \\  --copy-network   複製 *.ign 與腳本 到 /var/www/html/ignition 目錄:\n1 2 3 4 5 6  $ mkdir /var/www/html/ignition $ cp -rp $HOME/ocp4/ignition/{*.ign,coreos-install.sh} /var/www/html/ignition $ ls /var/www/html/ignition bootstrap.ign master.ign worker.ign coreos-install.sh $ chmod 664 -R /var/www/html/ignition/*   Bootstrap 透過 PVE 建立 Bootstrap 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   設定網卡資訊:\n 確保網卡 IP address 設定為 xx.xx.xx.xx/24，若沒有設定 Network mask 的話，就會用預設 xx.xx.xx.xx/8。\n 重新啟動網卡設定:\n透過以下指令檢查設定結果:\n1  $ ip -4 a    確認網路與 DNS 都能夠被解析到。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Bootstrap 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Bootstrap 開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be INFO Waiting up to 20m0s for the Kubernetes API at https://api.lab.yiylin.internal:6443... INFO API v1.21.1+051ac4f up INFO Waiting up to 30m0s for bootstrapping to complete... # 確認 openshift-api-server 以及 machine-config-server LB 可以通 $ curl -k https://api-int.${DOMAIN}:6443 $ curl -k https://api-int.${DOMAIN}:22623/config/master | jq .   當看到此訊息時，就能並往下進行安裝 Masters。若等待過久，可以透過 bastion 進入 bootstrap 節點查看狀況:\n1 2 3 4 5  $ ssh core@bootstrap.lab.yiylin.internal core $ sudo crictl pods core $ sudo crictl ps -a core $ sudo journalctl -b -f -u bootkube.service   正常情況 Pods 要如以下:\n1 2 3 4 5 6 7 8  POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 8101e41b16f9e 52 seconds ago Ready bootstrap-kube-scheduler-localhost kube-system 0 (default) 907d0a933771a 52 seconds ago Ready bootstrap-kube-controller-manager-localhost kube-system 0 (default) a8b709f7caeaa 52 seconds ago Ready bootstrap-kube-apiserver-localhost kube-system 0 (default) 4eb793b3d1844 52 seconds ago Ready cloud-credential-operator-localhost openshift-cloud-credential-operator 0 (default) 49a1cdb2b25f9 52 seconds ago Ready bootstrap-cluster-version-operator-localhost openshift-cluster-version 0 (default) c5098e42b5aa5 About a minute ago Ready bootstrap-machine-config-operator-localhost default 0 (default) e7a3dbc6e0bc0 About a minute ago Ready etcd-bootstrap-member-localhost openshift-etcd 0 (default)   Maters 透過 vCenter 建立 Master 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   \u0026gt; 網路參考 Bootstrap 設定流程。\n確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Masters 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=master.ign $ ./coreos-install.sh # 安裝完後 $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Masters 都開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug ... INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Bootstrap status: complete INFO It is now safe to remove the bootstrap resources INFO Time elapsed: 0s   在 bastion 執行以下指令來查看叢集狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ cp $HOME/ocp4/ignition/auth/kubeconfig $HOME/ocp4/kubeconfig $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-9xqqd 25m kubernetes.io/kubelet-serving system:node:etcd-1 Approved,Issued csr-gzvwr 27m kubernetes.io/kubelet-serving system:node:etcd-0 Approved,Issued csr-j6npt 25m kubernetes.io/kubelet-serving system:node:etcd-2 Approved,Issued csr-mxqkf 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-tth9f 28m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-zx9q8 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued system:openshift:openshift-authenticator 25m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-authentication-operator:authentication-operator Approved,Issued $ oc get no NAME STATUS ROLES AGE VERSION master-1.lab.yiylin.internal Ready master 31m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 15m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 10m v1.21.1+051ac4f   完成後，即可進行 Workers 與 Infras 部署，並移除 Bootstrap。若等待過久，可以透過 bastion 進入 masters 節點查看狀況:\n1  $ ssh core@maste-1.lab.yiylin.internal   Workers \u0026amp; Infras 透過 vCenter 建立 Workers \u0026amp; Infras 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui    網路參考 Bootstrap 設定流程。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Workers \u0026amp; Infras 節點:\n1 2 3 4 5 6 7 8  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=worker.ign $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Workers \u0026amp; Infras 都開啟完成後，就可以回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ openshift-install wait-for install-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be DEBUG Loading Install Config... DEBUG Loading SSH Key... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Cluster Name... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Networking... DEBUG Loading Platform... DEBUG Loading Pull Secret... DEBUG Loading Platform... DEBUG Using Install Config loaded from state file INFO Waiting up to 40m0s for the cluster at https://api.lab.yiylin.internal:6443 to initialize... DEBUG Still waiting for the cluster to initialize: Some cluster operators are still updating: authentication, console, ingress, monitoring    出現 DEBUG 即可往下操作，因為這部分會需要下載 Images 會比較久\n 開啟一個新的 Terminal，在 bastion 節點 Approve workers CSR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr -ojson | jq -r .items[] | select(.status == {} ) | .metadata.name | xargs oc adm certificate approve # 確認機器都起來 $ oc get no NAME STATUS ROLES AGE VERSION infra0 Ready worker 12m v1.21.1+051ac4f infra1 Ready worker 12m v1.21.1+051ac4f infra2 Ready worker 12m v1.21.1+051ac4f master0 Ready master 11h v1.21.1+051ac4f master1 Ready master 11h v1.21.1+051ac4f master2 Ready master 11h v1.21.1+051ac4f worker0 Ready worker 12m v1.21.1+051ac4f worker1 Ready worker 12m v1.21.1+051ac4f worker2 Ready worker 12m v1.21.1+051ac4f worker3 Ready worker 12m v1.21.1+051ac4f   搬移資源至 Infra nodes 接著要將 Worker 與 Infra 角色做拆分，確保系統與 Operator 服務不影響業務服務的負載，參考 Creating infrastructure machine sets\n設定 infra nodes 的 labels:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ for i in {1..2}; do oc label node worker-${i} node-role.kubernetes.io/app= --overwrite done $ for i in {1..3}; do oc label node infra-${i} node-role.kubernetes.io/infra= --overwrite oc label node infra-${i} node-role.kubernetes.io/worker- done $ oc get no NAME STATUS ROLES AGE VERSION infra-1.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-2.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-3.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f master-1.lab.yiylin.internal Ready master 156m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 140m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 135m v1.21.1+051ac4f worker-1.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f worker-2.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f   編輯 Scheduler 物件，並調整預設 NodeSelector:\n1 2 3 4 5  $ oc edit scheduler cluster ... spec: defaultNodeSelector: node-role.kubernetes.io/app= ...   建立 Infra 機器的 Machine Config Pool 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ cat \u0026amp;lt;\u0026amp;lt;EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - key: machineconfiguration.openshift.io/role operator: In values: - worker - infra nodeSelector: matchLabels: node-role.kubernetes.io/infra: EOF $ oc get mcp    這時 Infra 節點會重新啟動。\n 搬移 Router 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10  $ oc edit IngressController default -n openshift-ingress-operator ... spec: nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/infra: replicas: 2 $ oc -n openshift-ingress get pod -o wide   搬移 Image Registry 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8  $ oc edit configs.imageregistry.operator.openshift.io/cluster ... spec: nodeSelector: node-role.kubernetes.io/infra: ... $ oc -n openshift-image-registry get pods -o wide   搬移 Monitoring 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: grafana: nodeSelector: node-role.kubernetes.io/infra: k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: telemeterClient: nodeSelector: node-role.kubernetes.io/infra: openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: EOF $ watch -n 0.1 oc -n openshift-monitoring get pod -o wide   \u0026gt; 其他資源請參考 Moving OpenShift Logging resources 搬移。\n叢集初始化會需要一點時間，完成後就可以透過 Console 查看整體狀況。\nWeb Console 透過 oc 取得 Web console 的 route URL:\n1 2  $ oc -n openshift-console get route console -o=jsonpath={.spec.host}{\\n} console-openshift-console.apps.lab.yiylin.internal   取得 OpenShift 的 kubeadmin 密碼:\n1  $ cat $HOME/ocp4/ignition/auth/kubeadmin-password    Terminal Login 開啟一個新的 Terminal，在 bastion 節點配置 KUBECONFIG\n1 2  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get no   References  https://github.com/openshift/machine-config-operator/blob/master/docs/custom-pools.md https://gist.github.com/kairen/894348c6281f33a981b528e69c3d06bf https://docs.openshift.com/container-platform/4.8/installing/installing_vsphere/installing-restricted-networks-vsphere.html Networking Guide - BIND (Berkeley Internet Name Domain) 參考 IPI Prerequisites  ","date":"2021-09-26T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/ocp4-install/","title":"OpenShift 4.8.x UPI install on Bare metal"}]