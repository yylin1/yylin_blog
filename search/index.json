[{"content":"先決條件：  Bastion 安裝 Helm 當前 OpenShift 版本為 4.10+ Nvidia GPU Operator 已經完成安裝  啟用 NVIDIA GPU Operator Usage 資訊  添加 helm repo:  1  $ helm repo add rh-ecosystem-edge https://rh-ecosystem-edge.github.io/console-plugin-nvidia-gpu   更新 repo:  1  $ helm repo update   安裝 helm chart 於預設 NVIDIA GPU Operator namespace:  1 2 3 4 5 6 7 8  $ helm install -n nvidia-gpu-operator console-plugin-nvidia-gpu rh-ecosystem-edge/console-plugin-nvidia-gpu $ kubectl -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu # 啟用 plugin 執行以下 command: $ kubectl patch consoles.operator.openshift.io cluster --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/plugins/-\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;console-plugin-nvidia-gpu\u0026#34; }]\u0026#39; --type=json   查看部署的資源：  1  $ oc -n nvidia-gpu-operator get all -l app.kubernetes.io/name=console-plugin-nvidia-gpu   驗證 plugins 是否已指定  1  $ oc get consoles.operator.openshift.io cluster --output=jsonpath=\u0026#34;{.spec.plugins}\u0026#34;    如果未指定，則運行以下 command 以啟用 plugin：  1  $ oc patch consoles.operator.openshift.io cluster --patch \u0026#39;{ \u0026#34;spec\u0026#34;: { \u0026#34;plugins\u0026#34;: [\u0026#34;console-plugin-nvidia-gpu\u0026#34;] } }\u0026#39; --type=merge    如果指定，則運行以下 command 以啟用 plugin：  1  $ oc patch consoles.operator.openshift.io cluster --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/plugins/-\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;console-plugin-nvidia-gpu\u0026#34; }]\u0026#39; --type=json   在 OCP Web Console 頁面中（Home \u0026gt; Overview）就可以查閱 GPU utilization:\n Reference  https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/enable-gpu-op-dashboard.html  ","date":"2022-06-28T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/gpu-utilization/","title":"OpenShift 啟用 GPU Operator Dashboard"},{"content":"","date":"2022-06-25T09:40:40+08:00","permalink":"https://blog.yylin.io/openshift/argocd-and-acm/","title":"在 ACM 透過 OpenShift-GitOps/ArgoCD 管理應用程式"},{"content":"OpenShift Pipelines 是一個基於 Kubernetes 資源的雲塊的持續和持續交付（持續集成和持續交付，簡稱 CI/CD）的解決方案。它通過執行執行的細節，使用 Tekton 進行跨平台的自動部署。Tekton 引入了多種標準的自定義資源定義 (CRD)，定義可跨 Kubernetes 分佈用於 CI/CD 管道。\n主要特性  OpenShift Pipelines 是一個無服務器的 CI/CD 系統，它在獨立的容器中運行 Pipelines，以及所有需要的依賴組件。 OpenShift Pipelines 是為開發微服務架構的非中心化團隊設計的。 OpenShift Pipelines 使用標準 CI（pipeline）定義，這些與現有的 Kubernetes 工具集成擴展可擴展和擴展，可讓您定義和擴展 Kubernetes。 您可以通過 OpenShift Pipelines 使用 Kubernetes （如 Source-to-Image (S2I)、Buildah、Buildpacks 和 Kaniko）構建鏡像，這些工具可以移植到任何 Kubernetes 平台。 您可以使用 OpenShift Container Platform 開發運行（Developer Console）來創建 Tekton 資源，查看 Pipeline 的日誌，並管理 OpenShift Container Platform 設計空間中的管道。  成功運行 Pipeline 結果:\n失敗運行 Pipeline 結果：\n","date":"2022-06-21T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/pipelines/","title":"CI/CD: Tekton Pipeline 實戰"},{"content":"ARO4 深入探討 - Microsoft Azure Red Hat OpenShift 4\n讓我們深入挖掘！\nMicrosoft Azure Red Hat OpenShift 服務支持部署完全託管的 OpenShift 集群。\nAzure Red Hat OpenShift 由 Red Hat 和 Microsoft 聯合設計、運營和支持，以提供集成的支持體驗。沒有虛擬機可以運行，也不需要打補丁。主節點、基礎架構和應用程序節點由 Red Hat 和 Microsoft 代表您進行修補、更新和監控。您的 Azure Red Hat OpenShift 集群已部署到您的 Azure 訂閱中，並包含在您的 Azure 賬單中。\n在 OpenShift 4 上部署 Azure Red Hat 時，整個集群都包含在一個虛擬網絡中。在這個虛擬網絡中，您的主節點和工作節點都位於各自的子網中。每個子網都使用一個內部負載均衡器和一個公共負載均衡器。\n這是有關 Azure Red Hat OpenShift 4 的官方圖表（可在 ARO4 Microsoft 頁面中找到）：\n 關於 ARO4 部署和管理的網絡和資源的更多詳細信息，請查看ARO 圖詳細信息 - 官方文檔  讓我們安裝我們的第一個 ARO4 集群！\nAzure 帳戶先決條件 首先，我們需要在 Azure 帳戶中設置幾項內容，例如生成 ServicePrincipals、增加限制以及定義要使用的區域。\n 按照配置 Azure 帳戶先決條件來定義和分配適當的 RBAC 權限並增加對 Azure 帳戶的限制。  為 ARO 安裝配置 Azure 基礎結構先決條件 當我們準備好上一步後，就可以在 Azure 中為我們的 ARO4 集群生成基礎資源先決條件了：\n 定義 ARO4 安裝的基本參數：  1 2 3 4 5 6  export LOCATION=eastus export RESOURCEGROUP=aro-rg export CLUSTER=rcarrata export VNET_CIDR=\u0026#34;10.0.0.0/22\u0026#34; export MASTER_SUBNET_CIDR=\u0026#34;10.0.0.0/23\u0026#34; export WORKER_SUBNET_CIDR=\u0026#34;10.0.2.0/23\u0026#34;    使用 az cli 登錄到 Azure：  1  az login   注意：當登錄彈出時，您需要在 Azure Dashboard 中使用您的憑據進行身份驗證。\n 註冊資源提供者：  1 2 3  az provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait   建立 Azure Red Hat OpenShift 4 叢集 開始之前 Azure Red Hat OpenShift 至少需要 40 個核心，才能建立和執行 OpenShift 叢集。 新 Azure 訂用帳戶的預設 Azure 資源配額不符合這項需求。 若要要求增加資源限制，請參閱標準配額：VM 系列的增加限制。\n 例如，若要檢查最小支援的虛擬機器系列 SKU 「標準 DSv3」的目前訂用帳戶配額：  1 2 3 4  LOCATION=eastus az vm list-usage -l $LOCATION \\ --query \u0026#34;[?contains(name.value, \u0026#39;standardDSv3Family\u0026#39;)]\u0026#34; \\ -o table   1 2 3  CurrentValue Limit LocalName -------------- ------- -------------------------- 40 1000 Standard DSv3 Family vCPUs   驗證權限 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  Azure CLI quickstart: export GUID=hzdnk export CLIENT_ID=bdf5480a-c661-451f-a2ce-81e448ce0ba8 export PASSWORD=Zz842xzC.7GWZS_xVZ3oQvg1~7PtoIcKVA export TENANT=1ce7852f-dcf3-42bc-afe6-3bf81ab984fb export SUBSCRIPTION=ede7f891-835c-4128-af5b-0e53848e54e7 export RESOURCEGROUP=openenv-hzdnk curl -L https://aka.ms/InstallAzureCli | bash az login --service-principal -u $CLIENT_ID -p $PASSWORD --tenant $TENANT [ { \u0026#34;cloudName\u0026#34;: \u0026#34;AzureCloud\u0026#34;, \u0026#34;homeTenantId\u0026#34;: \u0026#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ede7f891-835c-4128-af5b-0e53848e54e7\u0026#34;, \u0026#34;isDefault\u0026#34;: true, \u0026#34;managedByTenants\u0026#34;: [ { \u0026#34;tenantId\u0026#34;: \u0026#34;b5ce0030-ec42-4a62-bc94-3025993e790c\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;RHPDS Subscription - OpenTLC Tenant\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;1ce7852f-dcf3-42bc-afe6-3bf81ab984fb\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bdf5480a-c661-451f-a2ce-81e448ce0ba8\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;servicePrincipal\u0026#34; } } ]    創建資源組為 ARO4 對象（如 vnet 和子網，以及自己的 ARO4 對象）分配資源：  1  az group create --name $RESOURCEGROUP --location $LOCATION    創建虛擬網絡：  1 2  az network vnet create --resource-group $RESOURCEGROUP \\ --name aro-vnet --address-prefixes $VNET_CIDR    為主節點添加一個空子網：  1 2 3 4 5 6  az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes $MASTER_SUBNET_CIDR \\ --service-endpoints Microsoft.ContainerRegistry    為工作節點添加一個空子網：  1 2 3 4 5 6  az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes $WORKER_SUBNET_CIDR \\ --service-endpoints Microsoft.ContainerRegistry    在主子網上禁用子網專用終結點策略：  1 2 3 4 5  az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true   安裝 ARO4  使用 azure cli 創建 ARO 集群：  1 2 3 4 5 6  echo \u0026#34;Creating ARO Cluster... Please wait 40mins\u0026#34; az aro create --resource-group $RESOURCEGROUP \\ --name $CLUSTER --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @pull-secret.txt   注意：安裝需要一個有效的 pull-secret。請訪問您的Cloud OpenShift Doshboard並獲取您的 pull-secret 令牌。\n 然後 az aro cli 將在 Azure Dashboard 中預配一個 ARO 對象：   自動地，它使用用於安裝的 Azure 對象創建了一個額外的 Azure 資源組，由 RH 和 MSFT 的 ARO SRE 管理：   在此資源組中，我們生成了提供和配置 ARO4 集群所需的資源：  訪問 API 和控制台  大約 40 分鐘後。我們將使用控制台和 API 準備好 ARO4 集群：  \n 要訪問集群，請列出集群的憑據：  1 2 3 4 5  echo \u0026#34;List credentials for ARO Cluster\u0026#34; az aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP echo \u0026#34;\u0026#34;    要顯示 ARO4 控制台：  1 2 3 4 5  echo \u0026#34;List console for ARO cluster\u0026#34; az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \u0026#34;consoleProfile.url\u0026#34; -o tsv    檢查 ARO4 API：  1 2  apiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) echo \u0026#34;This is the API for your cluster: $apiServer\u0026#34;    查詢 ARO Cluster 狀態  1 2 3 4  [root@bastion ~]# az aro list -o table Name ResourceGroup Location ProvisioningState WorkerCount URL --------- --------------- ---------- ------------------- ------------- ----------------------------------------------------------------- aro-demo openenv-hdpnp eastus Succeeded 3 https://console-openshift-console.apps.yylin.io/   ","date":"2022-03-13T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/aro/","title":"從零開始快速建置 - Microsoft Azure Red Hat OpenShift (ARO)"},{"content":"本文件如何安裝 OpenShift 4.8 版本至虛擬機器上，這邊將以 RHEL 環境來進行測試。\n RHEL 8 or Fedora machine with podman v3.3 installed Fully qualified domain name for the Quay service (must resolve via DNS, or at least /etc/hosts) Passwordless sudo access on the target host (rootless install tbd) Key-based SSH connectivity on the target host (will be set up automatically for local installs, in case of remote hosts see here) make (only if compiling your own installer)  當我們的基礎設施處於離線環境時，需要創建容器鏡像倉庫(Registry)來託管安裝 OpenShift Container Platform 所需的相關鏡像檔，來正常部署 OpenShift 叢集。\n這是有關 mirror registry for Red Hat OpenShift OpenShift 4 的官方流程圖（可在 Red Hat Blog 頁面中找到）：\n情境與使用目標 OpenShift 通常是在受控制離線網路的情況下運行 Production Cluster。而客戶要使用 Red Hat Quay 在 OpenShift 上運行優勢包含，有更多擴充性與部署元件配置整合性。要在離線環境安裝 OpenShift 前提條件，需要一個存放的安裝所需的相關鏡像的鏡像倉庫 (e.g., Quay, Harbor, Docker registry)，這邊就會有雞生蛋蛋生雞問題。\n解決方案/目標：\nRed Hat 提供一個 “mirror registry”工具，只針對 OpenShift 部署的引導(Bootstrap)鏡像倉庫，透過自動化腳本安裝程序，提供在 RHEL 8 (or Fedora) 的系統環境，快速部署一個精簡版的 Quay ，提供保存下載特定版本 OpenShift、OpenShiftHub 鏡像檔。mirror registry 提供針對離線環境或只是單純 PoC OpenShift 情境使用的 Registry，可以透過自動化腳本，快速部署設置安裝單節點(all-in-one)的 Quay ，所需要的相關手動繁瑣設定，例如：完整網域名稱 FQDN、使用者自定義 SSL/TLS 憑證、訪問權權限 SSH key 及運行的環境選擇。\n情境: 離線環境，安裝 OpenShift 運行流程:\n 在可連線外部網路環境中，透過 mirror registry 部署第一座容器鏡像倉庫(Online Mirror) 進行 OpenShift、OpenShiftHub 必要鏡像檔存放。 同樣在離線環境中，透過 mirror registry 部署第二座容器鏡像倉庫(Air-gapped Mirror)，從 Online Mirror 拷貝並要的映像檔存放於 Air-gapped Mirror 透過 Air-gapped Mirror 部署 OpenShift Production/Infra Cluster OpenShift 安裝 Quay Operator 提供內部服務及應用所需的鏡像存放  事前準備  從 OpenShift console Downloads 下載最新版本的 mirror-registry.tar.gz 準備 pull-secret.json 檔案  安裝  在當前環境安裝 mirror registry  1 2 3 4 5 6 7 8  $ vim /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 registry.yylin.demolab $ export HOSTNAME=\u0026#34;registry.yylin.demolab\u0026#34; $ ./mirror-registry install --quayHostname ${HOSTNAME} --ssh-key \u0026lt;~/.ssh/my_id_rsa\u0026gt;    最後輸出顯示 registry host 與登入資訊  1 2  INFO[2022-03-01 00:52:38] Quay installed successfully, permanent data are stored in /etc/quay-install INFO[2022-03-01 00:52:38] Quay is available at https://registry.yylin.demolab:8443 with credentials (init, xxxxxxxxxxxxxxxxxxxxxx)    產生 Registry Basic Auth 之認證資訊  1 2  # (init, xxxxxxxxxxxxxxxxxxxxxx) echo -n \u0026#39;\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;\u0026#39; | base64 -w0    在 pull-secret 加入本地 Registry 認證資訊  1 2 3  \u0026#34;registry.yylin.demolab:8443\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;aW5pdDo3TzlNV1hBMDhQcTJKZGh0M0M2elZJNU5EMWlsYkJ3NA==\u0026#34; }    驗證登入registry  1 2  $ podman login -u init --authfile pull-secret.json registry.yylin.demolab:8443 Error: authenticating creds for \u0026#34;registry.yylin.demolab:8443\u0026#34;: pinging container registry registry.yylin.demolab:8443: Get \u0026#34;https://registry.yylin.demolab:8443/v2/\u0026#34;: x509: certificate signed by unknown authority (possibly because of \u0026#34;crypto/rsa: verification error\u0026#34; while trying to verify candidate authority certificate \u0026#34;bastion.redhat.kubedev.org\u0026#34;)    安裝更新 quay 憑證   mirror registry 預設會配置好 CA 憑證，請將 PEM 文件格式添加憑證到系統中信任的 CA 列表中，這邊複製到 /usr/share/pki/ca-trust-source/anchors/ 目錄中\n 1  $ cp /etc/quay-install/quay-rootCA/rootCA.pem /usr/share/pki/ca-trust-source/anchors/rootCA.cert    更新系統範圍的信任儲存配置，請使用 update-ca-trust 命令：\n 1  $ update-ca-trust    再次登入mirror registry - Quay 確認憑證狀態是不是已經更新  1 2 3  $ podman login -u init --authfile pull-secret.json bastion.redhat.kubedev.org:8443 Password: Login Succeeded!   開始備份 OpenShift images  建立備份路徑  1  $ mkdir -p $HOME/openshift4/registry/images   匯入環境變數  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # vim upgrade-env export OCP_RELEASE=$(oc version -o json --client | jq -r \u0026#39;.releaseClientVersion\u0026#39;) export LOCAL_REGISTRY=\u0026#34;registry.yylin.demolab:8443\u0026#34; export LOCAL_REPOSITORY=\u0026#34;ocp4/openshift4\u0026#34; export PRODUCT_REPO=\u0026#34;openshift-release-dev\u0026#34; # /註記/ 請確認 pull-secret.json 裡面有包含 mirror rigistry 資訊 export LOCAL_SECRET_JSON=$HOME/mirror-registry/pull-secret.json export RELEASE_NAME=\u0026#34;ocp-release\u0026#34; export ARCHITECTURE=x86_64 # 指定 images 存放路徑 export REMOVABLE_MEDIA_PATH=\u0026#34;$HOME/openshift4/registry/images\u0026#34; # 匯入環境變數 source upgrade-env   備份 Image   /#1/ Review the images and configuration manifests to mirror:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} --dry-run   1 2 3 4 5 6 7  imageContentSources: - mirrors: - registry.yylin.demolab:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - registry.yylin.demolab:8443/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev    /#2/ Mirror the images to a directory on the removable media:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --to-dir=${REMOVABLE_MEDIA_PATH}/mirror quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE}   1 2 3 4 5 6 7 8 9 10  info: Mirroring completed in 3m6.3s (66.22MB/s) Success Update image: openshift/release:4.10.0-rc.3-x86_64 To upload local images to a registry, run: oc image mirror --from-dir=/root/openshift4/registry/images/mirror \u0026#39;file://openshift/release:4.10.0-rc.3-x86_64*\u0026#39; REGISTRY/REPOSITORY Configmap signature file /root/openshift4/registry/images/mirror/config/signature-sha256-3d4ada825f4aa4d2.yaml created    /#3/ Take the media to the restricted network environment and upload the images to the local container registry:  1  $ oc image mirror -a ${LOCAL_SECRET_JSON} --from-dir=${REMOVABLE_MEDIA_PATH}/mirror \u0026#34;file://openshift/release:${OCP_RELEASE}*\u0026#34; ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  phase 0: registry.redhat.demolab:8443 ocp4/openshift4 blobs=334 mounts=0 manifests=163 shared=0 info: Planning completed in 2.12s uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:7253c5d7bf339222334565e0ec7ac88dd017dc9b9ec01654d26e8dda07408548 81.38MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:8e549eaeb17902a2a516d03776112ad7a9bfd10cd2d1c298fe31b1c15d3b4166 154.4MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:11456a2e94a652afe44f2645d83a051160119fd953b2459167cc88232adbf9ed 31.45MiB uploading: registry.redhat.demolab:8443/ocp4/openshift4 sha256:4ceffbad995c4d6e76142c6b46e7f2e06fcd2ddb6eb2c3624cc0244ae018356f 106.9MiB ... ... sha256:2d80cc5beffc10607a53eb6c013e87d03e267cdf0b8fc678b40acf5432957714 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-multus-networkpolicy sha256:df3b0ec40395ea460fbf2728ca7adff79dbaebddcffce003e88b3fb9cb2c9759 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-must-gather sha256:a340ea3d86560de8cf9ee2c1afe42ad24e5eefc9903a0073abe9dc54815bf710 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-console-operator sha256:f532e7f50cadfc757a6d277ab5476a31a4f24aac0afc615da6ff9f7ce5e2b538 registry.redhat.demolab:8443/ocp4/openshift4:4.10.0-rc.3-x86_64-cluster-image-registry-operator info: Mirroring completed in 1m17.77s (5.572MB/s)    /#4/ Directly push the release images to the local registry by using following command:  1  $ oc adm release mirror -a ${LOCAL_SECRET_JSON} --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --apply-release-image-signature    更多配置請參考 Mirroring the OpenShift Container Platform image repository\n mirror 後結果  Reference  Mirroring on a local host with mirror registry for Red Hat OpenShift https://cloud.redhat.com/blog/introducing-mirror-registry-for-red-hat-openshift https://access.redhat.com/support/policy/updates/openshift#omr https://www.youtube.com/watch?v=j5e4OT71N0A  ","date":"2022-03-07T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/mirror-registry-for-ocp4-install/","title":"Disconnected Environment - Introducing Mirror Registry for Red Hat OpenShift"},{"content":"Nvidia GPU Operator v1.9 on OpenShift 4.9.9 包含以上版本，安裝不用再進行額外權限配置]\nOpenShift 4.9.9 或更高的版本 [1] 針對 driver toolkit 取消必要安裝要求:\n Set up an entitlement Mirror the RPM packages in a disconnected environment Configure a proxy to access the package repository  [1] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions\nOpenShift 4.9.8 與以下版本 [2] 需手動操作獲取 OCP 憑證，創建 MachineConfig 來認證 OCP 叢集，擴大授權 Images 使用權限範圍，來安裝 Nvidia Operator :\n 從 Red Hat Customer Portal 下載 Red Hat OpenShift Container Platform 訂閱憑證 (啟用權限需要登入 OCP 憑證）。 創建一個 MachineConfig 啟用訂閱管理平台並提供有效訂閱憑證。等待 MachineConfigOperator 重啟節點並完成 MachineConfig。 驗證叢集所有節點更新權限是否正常。   補充 - NVIDIA GPU Operator 安裝會部署幾個 Pod 服務，用於管理和啟用 GPU 在 OpenShift 中運作。其中一些 Pod 需要 OpenShift 使用一些非 Universal Base Image (UBI) 默認授權的 Images。必須在 OpenShift Cluster 中啟用信任的授權 Images，來啟動 NVIDIA GPU 驅動程式的容器運行。\n [1] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions [2] https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/cluster-entitlement.html#enabling-a-cluster-wide-entitlemenent\nInstall Operator - Node Feature Discovery 1 2 3 4 5 6 7  [lab-user@bastion ~]$ oc get no NAME STATUS ROLES AGE VERSION ip-10-0-135-51.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f ip-10-0-142-219.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f ip-10-0-167-35.us-east-2.compute.internal Ready worker 3h5m v1.22.3+e790d7f ip-10-0-186-251.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f ip-10-0-213-103.us-east-2.compute.internal Ready master 3h14m v1.22.3+e790d7f    要驗證實例是否已創建，請運行：  1 2 3 4 5 6 7 8  [lab-user@bastion ~]$ oc get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-6f65f47cf6-tg6gj 2/2 Running 0 24m nfd-master-d7cqw 1/1 Running 0 35s nfd-master-j42m9 1/1 Running 0 35s nfd-master-r64nv 1/1 Running 0 35s nfd-worker-24tzn 1/1 Running 0 35s nfd-worker-5rsg2 1/1 Running 0 35s    成功的部署會顯示一個Running狀態。  Installing the NVIDIA GPU Operator With the Node Feature Discovery Operator installed you can continue with the final step and install the NVIDIA GPU Operator.\nAs a cluster administrator, you can install the NVIDIA GPU Operator using the OpenShift Container Platform CLI or the web console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  kind: ClusterPolicy apiVersion: nvidia.com/v1 metadata: name: gpu-cluster-policy spec: dcgmExporter: config: name: \u0026#39;\u0026#39; dcgm: enabled: true daemonsets: {} devicePlugin: {} driver: enabled: true use_ocp_driver_toolkit: true repoConfig: configMapName: \u0026#39;\u0026#39; certConfig: name: \u0026#39;\u0026#39; licensingConfig: nlsEnabled: false configMapName: \u0026#39;\u0026#39; virtualTopology: config: \u0026#39;\u0026#39; gfd: {} migManager: enabled: true nodeStatusExporter: enabled: true operator: defaultRuntime: crio deployGFD: true initContainer: {} mig: strategy: single toolkit: enabled: true validator: plugin: env: - name: WITH_WORKLOAD value: \u0026#39;true\u0026#39;   Create the ClusterPolicy custom resource. This CRD will create several OCP resources. It will evaluate all the labels for the each node in the cluster and look for this:\n1 2  $ oc project nvidia-gpu-operator $ oc get pod -o wide   Validating the GPU availability 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  [lab-user@bastion ~]$ oc get pod | grep nvidia-device-plugin-daemonset nvidia-device-plugin-daemonset-bspfh 1/1 Running 0 21m nvidia-device-plugin-daemonset-n62dm 1/1 Running 0 21m [lab-user@bastion ~]$ oc exec -ti nvidia-device-plugin-daemonset-bspfh -- nvidia-smi Defaulted container \u0026#34;nvidia-device-plugin-ctr\u0026#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init) Fri Jan 28 06:47:21 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 | | N/A 32C P0 23W / 300W | 0MiB / 16160MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ ... ... $ oc exec -ti nvidia-device-plugin-daemonset-n62dm -- nvidia-smi Defaulted container \u0026#34;nvidia-device-plugin-ctr\u0026#34; out of: nvidia-device-plugin-ctr, toolkit-validation (init) Fri Jan 28 06:47:44 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.82.01 Driver Version: 470.82.01 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla V100-SXM2... On | 00000000:00:1E.0 Off | 0 | | N/A 26C P0 24W / 300W | 0MiB / 16160MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+   Running a sample GPU Application Run a simple CUDA VectorAdd sample, which adds two vectors together to ensure the GPUs have bootstrapped correctly.\n Run the following:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  cat \u0026lt;\u0026lt; EOF | oc create -f - apiVersion: v1 kind: Pod metadata: name: cuda-vectoradd spec: restartPolicy: OnFailure containers: - name: cuda-vectoradd image: \u0026#34;nvidia/samples:vectoradd-cuda11.2.1\u0026#34; resources: limits: nvidia.com/gpu: 1 EOF pod/cuda-vectoradd created   Check the logs of the container:  1 2 3 4 5 6 7  [lab-user@bastion ~]$ oc logs cuda-vectoradd [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done   Getting information about the GPU¶ The nvidia-smi shows memory usage, GPU utilization and the temperature of the GPU. Test the GPU access by running the popular nvidia-smi command within the pod.\nTo view GPU utilization, run nvidia-smi from a pod in the GPU Operator daemonset.\n Change to the nvidia-gpu-operator project:  1  $ oc project nvidia-gpu-operator   Run the following command to view these new pods:  1  $ oc get pod -owide -lopenshift.driver-toolkit=true   1 2 3  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nvidia-driver-daemonset-49.84.202201102104-0-gl557 2/2 Running 0 26m 10.131.0.106 ip-10-0-167-35.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nvidia-driver-daemonset-49.84.202201102104-0-k9sg5 2/2 Running 0 26m 10.128.2.17 ip-10-0-135-51.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Run the nvidia-smi command within the pod:  1  $ oc exec -it nvidia-driver-daemonset-48.84.202110270303-0-9df9j -- nvidia-smi   ","date":"2022-02-28T21:40:40+08:00","permalink":"https://blog.yylin.io/openshift/gpu-operator/","title":"NVIDIA GPU Operator on OpenShift4"},{"content":"2021 年有非常多的轉變與抉擇，是讓自己很深刻的一年，雖然年初有一些個人因素，影響後續許多計畫與安排，但之後從個人職涯選擇以及決定，為自己嘗試更多各種不同的挑戰，雖然過程很不容易，但能能充實的過完這一年。\n2021 回顧  協助前公司部分產品 Containerized 建置與Kuberentes - IT 初期工作流程基礎規劃，學習很多寶貴的維運經驗。 下半年決定為自己職涯嘗試新的挑戰，轉換職務，雖然過程花很多時間為認知與技能做轉換，過程參與事件活動非常多，但忙得有意義，非常充實。 在 COSCUP 2021 協助 CNTUG 社群議程軌-開源運河上的雲原生號擔任主持人：首次 COSCUP 線上虛擬舉行，體會主持人如何引導議程進行時，與講者會眾互動 Q\u0026amp;A ，並完成整個整個議程軌活動，難得且充實的年會體驗。 擔任 HPC Summit 2021 講者：有機會分享抽空研究DeepOps 的專案，如何對於 AI/ML Infra 建置的部署工具與企業場景實際應用分析(簡報連結)。 擔任 Kuberentes Summit 2021 - WorkShop 講者: 學習如何用會眾角度規劃整個工作坊設計。 回歸山岳攝影-登山健行: 持續撰寫登山系列文章，跟一群好友組了登山興趣小組，開始嘗試(台灣中級山系列、台北大縱走路線完成4/7段[持續進行中])，脫離城市的大自然體驗。 自己動手做一顆手工肥皂，感謝 SU VIDA x 私 生活 開設的工作坊 攀岩(抱石) 維持 LV2 等級都順利完攀，持續往下一級前進。 終於換了新一台相機 Sony A7 IV  2022 展望  增強專業能力部分，提升各方面不足點 花更多時間在技術文章撰寫 完成先前未完成的證照 回歸開源專案貢獻，積極參與 Conference / Meetup 分享 持續健身運動維持飲食控制，為百岳做後續準備  Related Posts  2020-年度回顧 (舊 Blog 搬家中)   2022 繼續爬山   Photography Blog (https://medium.com/yiyang-lins-life)  ","date":"2021-12-31T13:33:28+08:00","permalink":"https://blog.yylin.io/year/2021-retrospect-and-prospect/","title":"2021 年度回顧"},{"content":"事前準備： 開始前，需要先準備以下資訊與要求。\n啟用 ROSA  開始配置 AWS 服務前，使用者必須以 IAM 身份(參考文件: Customer Requirements)\n 登入 AWS 管理控制台，確認當前使用者已啟用 Red Hat OpenShift服務 下載 AWS CLI 與 OpenShift CLI 工具: URL Dowload\n  選擇當前環境下載對應CLI ( Command-line interface (CLI) tools)\n  or\nCommand Dowload\n 下載 AWS/OpenShift CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:  1 2 3 4 5 6 7 8 9 10 11 12 13 14  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/rosa/latest/rosa-linux.tar.gz -O - | tar -xz $ mv rosa /usr/local/bin/ $ rosa version 1.1.1 $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 $ oc version Client Version: 4.8.10 Server Version: 4.8.12 Kubernetes Version: v1.21.1+d8043e1    創建 Red Hat 帳號 OpenShift 由 Red Hat SaaS 提供，因此您需要創建一個 Red Hat 帳戶。 https://cloud.redhat.com/\n  透過 ROSA 指令檢查憑證與資源配置 這邊以 Red Hat 部署 ROSA 文件部署為主 (Red Hat OpenShift Service on AWS - Creating a ROSA cluster)\n首先，使用 ROSA CLI 檢查 AWS 憑證\n1 2 3  $ rosa verify permissions I: Validating SCP policies... I: AWS SCP policies ok   第一次登錄時，你會需要一個 Token 來登入你的 Red Hat 帳號\n 需要創建一組 Red Hat 帳號\n 確認登入訊息：\n1 2  $ rosa login I: Logged in as \u0026#39;xxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39;   配置完成後可以透過 rosa whoami 來檢查你目前所有登錄憑證與狀態\n Red Hat 帳號會附帶 OpenShift Cluster Manager (OCM) 資訊與你的 AWS 帳戶顯示。\n 1 2 3 4 5 6 7 8 9 10 11 12  $ rosa whoami AWS Account ID: xxxxxxxxxxxx AWS Default Region: us-west-2 AWS ARN: arn:aws:iam::xxxxxxxxxxxx:user/user OCM API: https://api.openshift.com OCM Account ID: xxxxxxxxxxxxxxxxxxxxxx OCM Account Name: xxxxxx OCM Account Username: xxxxx@xxxxx.com OCM Account Email: xxxxx@xxxxx.com OCM Organization ID: xxxxxxxxxxxxxxxxxxxxxx OCM Organization Name: Red Hat OCM Organization External ID: xxxxxxxx   以上檢查沒問題後，接下運行 rosa init 運行指令來確保 ROSA 配置與相關資源狀態沒問題\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ rosa init I: Logged in as \u0026#39;xxxxxx@mail.com\u0026#39; on \u0026#39;https://api.openshift.com\u0026#39; I: Validating AWS credentials... I: AWS credentials are valid! I: Validating SCP policies... I: AWS SCP policies ok I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html I: Ensuring cluster administrator user \u0026#39;osdCcsAdmin\u0026#39;... I: Admin user \u0026#39;osdCcsAdmin\u0026#39; already exists! I: Validating SCP policies for \u0026#39;osdCcsAdmin\u0026#39;... I: AWS SCP policies ok I: Validating cluster creation... I: Cluster creation valid I: Verifying whether OpenShift command-line tool is available... I: Current OpenShift Client Version: 4.8.10    需要驗證當前環境，是否已經登入 AWS並確認透過rosa init 來確認 AWS 資源與配置是否能足夠創建 OpenShift\n 申請 AWS 帳戶配額 在配置 ROSA 時作為預先檢查 $ rosa verify quota --region=${cluster} 叢集名稱。如果您在剛剛部署但未執行任何操作的帳戶上運行它，您將收到錯誤訊息 EC2 quota 配置不足。\n1 2 3 4 5  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... E: Insufficient AWS quotas E: Service quota is insufficient for the following service quota codes: - Service ec2 quota code L-1216C47A Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances not valid, expected quota of at least 100, but got 5   這邊需要額外申請 AWS EC2 限制增加配額 需要提供申請需求與相關資訊，才能申請限制配額成功\n相關配額資訊參考\n AWS service quotas Required AWS service quotas  以上完成申請後，再次驗證\n1 2 3  $ rosa verify quota --region=us-west-2 I: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.htm    透過 ROSA 指令快速創建 Red Hat OpenShift  透過 rosa create cluster 開始部署 OpenShift Cluster (部署Cluster 大約需要30-40分鐘)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  $ rosa create cluster --cluster-name=rosacluster I: Creating cluster \u0026#39;rosacluster\u0026#39; I: To view a list of clusters and their status, run \u0026#39;rosa list clusters\u0026#39; I: Cluster \u0026#39;rosacluster\u0026#39; has been created. I: Once the cluster is installed you will need to add an Identity Provider before you can login into the cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: To determine when your cluster is Ready, run \u0026#39;rosa describe cluster -c newsblogcluster\u0026#39;. I: To watch your cluster installation logs, run \u0026#39;rosa logs install -c newsblogcluster --watch\u0026#39;. Name: newsblogcluster ID: XXXXXXXXXXXXXXXXXXXX External ID: OpenShift Version: Channel Group: stable DNS: newsblogcluster.phnh.p1.openshiftapps.com AWS Account: 123456789012 API URL: Console URL: Region: xxxxxxxxxxx Multi-AZ: false Nodes: - Master: 3 - Infra: 2 - Compute: 2 (m5.xlarge) Network: - Service CIDR: 172.30.0.0/16 - Machine CIDR: 10.0.0.0/16 - Pod CIDR: 10.128.0.0/14 - Host Prefix: /23 State: pending (Preparing account) Private: No Created: xxxxxxxxxxxxxxxx Details Page: https://cloud.redhat.com/openshift/details/XXXXXXXXXXXXXXXXXXXX   執行命令後，您可以透過 rosa describe cluster 或 rosa logs install 指令檢查 OpenShift Cluster 狀態。您還可以從 URL 訪問 OpenShift Console 進行確認。\n1 2 3 4 5 6 7 8 9  $ rosa create admin -c rosacluster W: It is recommended to add an identity provider to login to this cluster. See \u0026#39;rosa create idp --help\u0026#39; for more information. I: Admin account has been added to cluster \u0026#39;rosacluster\u0026#39;. I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user. I: To login, run the following command: oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z I: It may take up to a minute for the account to become active.   1 2 3 4 5 6  $ oc login https://api.rosacluster.hffh.p1.openshiftapps.com:6443 --username cluster-admin --password fTwWy-rYJJU-wUua7-W3G6Z Login successful. You have access to 87 projects, the list has been suppressed. You can list all projects with \u0026#39;oc projects\u0026#39; Using project \u0026#34;default\u0026#34;.   1 2 3 4 5 6 7 8 9  $ oc get no NAME STATUS ROLES AGE VERSION ip-10-0-153-95.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1 ip-10-0-171-119.us-west-2.compute.internal Ready infra,worker 17m v1.21.1+d8043e1 ip-10-0-175-165.us-west-2.compute.internal Ready infra,worker 18m v1.21.1+d8043e1 ip-10-0-202-234.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-239-231.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-242-134.us-west-2.compute.internal Ready master 49m v1.21.1+d8043e1 ip-10-0-252-208.us-west-2.compute.internal Ready worker 42m v1.21.1+d8043e1   刪除 Cluster rosa delete cluster 可以使用指令刪除使用 ROSA 創建的cluster。順便說一下，集群的刪除在大約 10 分鐘內完成。\n1 2 3 4  $ rosa delete cluster --cluster=rosacluster ? Are you sure you want to delete cluster rosacluster? Yes I: Cluster \u0026#39;rosacluster\u0026#39; will start uninstalling now I: To watch your cluster uninstallation logs, run \u0026#39;rosa logs uninstall -c rosacluster --watch   Reference  rosaworkshop.io  ","date":"2021-09-30T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/rosa/","title":"Red Hat OpenShift Platform on AWS (ROSA) 快速部署上手"},{"content":"延伸上一篇 OCP UPI 部署(OpenShift 4.8.x UPI install on Bare metal)過程遇到問題彙整。\n 備註: 由於資源有限情況下，透過單一伺服器(主機)透過 VM 模擬實際節點硬體資源環境，這邊主要透過檢測與除錯，來確保 OpenShift 部署成功。\n 部署常見問題整理 (持續更新) 超時等待 bootstrap 節點啟動  發現特定 Pod 持續RunningNotReady 狀態  1 2 3  W0830 11:16:37.087497 35214 reflector.go:436] k8s.io/client-go/tools/watch/informerwatcher.go:146: watch of *v1.ConfigMap ended with: very short watch: k8s.io/client-go/tools/watch/informerwatcher.go:146: Unexpected watch close - watch lasted less than a second and no items received apiserver - \u0026gt; RunningNotReady Aug 30 03:42:37 bootstrap.lab.yiylin.internal bootkube.sh[644199]: Pod Status:openshift-kube-apiserver/kube-apiserver RunningNotReady    此情境請透過收集bootstrap 節點日誌，來確認環境問題\n 1 2 3 4  $ ./openshift-install gather bootstrap --dir=\u0026lt;installation_directory\u0026gt; ... INFO Pulling debug logs from the bootstrap machine INFO Bootstrap gather logs captured here \u0026#34;\u0026lt;installation_directory\u0026gt;/log-bundle-\u0026lt;timestamp\u0026gt;.tar.gz\u0026#34;    蒐集包含 bootstrap process / bootstrap 上所有 Pod/Container 的 log 下載完後解壓縮 .tar.gz 檔案，針對可能部署或未啟動的Pod檢查log錯誤狀態，來找到對應的錯誤資訊   補充說明 bootstrap 主要分成兩個動作\n 安裝自己跟建立自己的 cluster(Pod) 引導安裝 master x 3 master bootstrap 全部安裝完，然後下 wait for complete 確認最終才可移除 bootstrap node   Reference:  Gathering logs from a failed installation Waiting for the bootstrap process to complete  Master Node 部署不起來，檢查並驗證 etcd 運行之硬碟是否正常運作 ?  大部分 master 無法建置情境，通常可能會有原因是硬碟速度問題(硬碟壞軌)   造成叢集有時候部署成功有時候會失敗的情況\n 透過簡單運行 fio 來測試 etcd benchamrk performance 檢測\n1 2 3 4  $ oc debug node/\u0026lt;master_node\u0026gt; [...] sh-4.4# chroot /host bash [root@\u0026lt;master_node\u0026gt; /]# podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf   Reference:  How to Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance in OCP  [參考影片說明] OpenShift 4 如何使用 fio 驗證 etcd 運行之硬碟是否可以正常運作 / Use \u0026lsquo;fio\u0026rsquo; to Check Etcd Disk Performance Hardware requirements and recommendations  補充|檢查 - 除錯流程   部署過程都透過 bootkube 確認部署狀態資訊，log 會不斷顯示錯誤然後 loop 確認環節，嘗試能不能從他錯誤訊息找出規則(也就是他跳錯誤的地方，通常是在 log restart之前)\n  嘗試 SSH 到 Master Node，透過 netstat -plnt 檢查三個服務，這三個通常只要好，這台 Master 就沒問題 (6443| 22623|2379|2380)，分別是 OpenShift API/Kubernetes API/ETCD/Machine Config Operator\n  如果以上幾個 Pod 有問題，透過 crictl ps 跟 crictl log 去查看 Container Log，問題有可能是連續的錯誤，所以可能是 API 有問題，可能是 Controller/Scheduler 引起，細節需要透過上面提到方式，下載節點所有 Log 日誌來比對檢查，找到正確的錯誤資訊來排除錯誤\n  Reference:  OpenShift Troubleshooting Resources  ","date":"2021-09-29T00:53:25+08:00","permalink":"https://blog.yylin.io/openshift/install-troubleshooting/","title":"OpenShift Intsall Troubleshooting Notes"},{"content":" [註記] 由於硬體環境資源有限，本實驗是透過 Proxmox Environment 單一伺服器節點來模擬部署 OpenShift 叢集，並依據官網配置建議硬體需求(Server sizing)資源。 [共同撰寫編輯]: Kyle Bai\n 部署版本說明：  Proxmox Environment v6.1-7  CPU: 8-Core, 16-Thread Unlocked / Memory: 128 GB   OpenShift v4.8.x.  Reference  參考官網文件 - Installing a user-provisioned cluster on bare metal  事前準備 開始前，需要先準備以下資訊與要求。\nDownload URLs  User-provisioned infrastructure: 可下載 OpenShift Installer、CLI 與 Pull secrets(需要登入 Red Hat 註冊帳號權限) Public OpenShift 4.8 Mirror: 可下載 OpenShift Installer、CLI 與 OPM。  openshift-client-linux.tar.gz: 包含 oc 與 kubectl CLI 工具。 openshift-install-linux.tar.gz: 包含 openshift-install 工具。用於建立 OpenShift auth、igntion 等設定檔案。 opm-linux.tar.gz: 用於管理 Operator Hub 與 mirror 相關 images。   Public RHCOS 4.8 Mirror: 可下載 RHCOS 4.8 相關 VM images。  若手動安裝以下載 rhcos-live.x86_64.iso 為主。若想要控管使用版本，請選擇有標示版本的 live iso。  FET vSphere 安裝請以這方式進行。   若以 PXE 方式安裝，則下載以下映像檔:  rhcos-live-initramfs.x86_64.img rhcos-live-kernel-x86_64 rhcos-live-rootfs.x86_64.img       過程中會透過 wget 下載，因此在有網路狀況下，不需要預先下載。\n  Network and Hosts Info  OpenShift subnet CIDR: 192.168.101.0/24 OpenShift Pod Network(CNI): 10.128.0.0/14 OpenShift Service Network: 172.3.0.0/16 Domain: lab.yiylin.internal     Hosts IP CPU RAM     bashtion 192.168.101.9 2 vCore 4G   bootstrap 192.168.101.10 4 vCore 8G   master-1 192.168.101.21 4 vCore 8G   master-2 192.168.191.22 4 vCore 8G   master-3 192.168.191.23 4 vCore 8G   worker-1 192.168.191.31 4 vCore 8G   worker-2 192.168.191.32 4 vCore 8G   infra-1 192.168.191.41 4 vCore 8G   infra-2 192.168.191.42 4 vCore 8G   infra-3 192.168.191.43 4 vCore 8G     這裡為了方便測試，將 PXE / HAProxy / DNS Server 等都放在 bastion上。 [額外參考資訊]各元件節點最小資源可參考 Minimum resource requirements。 請先透過 Proxmox Environment 開啟對應 hardware VM。\n 補充 Network 說明  其中 OpenShift subnet CIDR 請修改為自己當前 Proxmox Environment 網路配置)  Bastion 透過 Live CD 安裝 RHEL 8 版本(或使用 CentOS 8)，並確保網路設定正確。\n RHEL 需要確認 subscription 驗證，確保 repo 源可正常取得\n 事前準備 進入 bastion 機器，安裝 jq 與 wget 工具等:\n1 2  $ sudo su - $ yum install -y wget jq vim   關閉 bastion 防火牆:\n1 2  $ systemctl stop firewalld $ systemctl disable firewalld   \u0026gt; 這邊方便測試，所以直接關閉 :D。\n設定與關閉 SELinux:\n1 2 3 4  $ vim /etc/selinux/config SELINUX=disabled $ setenforce 0    這邊方便測試實驗，所以直接關閉。\n 建立 SSH Keys:\n1  $ ssh-keygen -t rsa    這邊方便測試，以 root 來建立。\n 導入以下環境變數:\n1 2  export DOMAIN=lab.yiylin.internal export OCP_NETWORK_CIDR=192.168.101.0/24   下載 OpenShift Installer 與 CLI 工具，並解壓縮檔案到 /usr/local/bin/ 底下:\n1 2 3 4 5 6 7 8 9 10 11  # CLI $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-client-linux.tar.gz -O - | tar -xz $ mv {oc,kubectl} /usr/local/bin/ $ oc version Client Version: 4.8.2 # Installer $ wget -c https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.8/openshift-install-linux.tar.gz -O - | tar -xz $ mv openshift-install /usr/local/bin/ $ openshift-install version openshift-install 4.8.2   安裝與設定 DNS Server 安裝 DNS server 套件，這邊使用 dnsmasq 來提供服務:\n1  $ yum -y install dnsmasq bind-utils    這部分也可參考 OCP 官網安裝 BIND，關於不同 DNS Server 相關配置方法，會額外補充於後續文章介紹。\n 編輯 /etc/dnsmasq.d/openshift.conf 設定檔，以設定 A Record 與 PTR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/dnsmasq.d/openshift.conf server=8.8.8.8 domain=${DOMAIN},${OCP_NETWORK_CIDR},local # A Records(api) host-record=api.${DOMAIN}, 192.168.101.9 host-record=api-int.${DOMAIN}, host-record=api-int.${DOMAIN},192.168.101.9 # A Records(hosts) host-record=bastion.${DOMAIN},192.168.101.9 host-record=bootstrap.${DOMAIN},192.168.101.10 host-record=master-1.${DOMAIN},192.168.101.21 host-record=master-2.${DOMAIN},192.168.101.22 host-record=master-3.${DOMAIN},192.168.101.23 host-record=worker-1.${DOMAIN},192.168.101.31 host-record=worker-2.${DOMAIN},192.168.101.32 host-record=infra-1.${DOMAIN},192.168.101.41 host-record=infra-2.${DOMAIN},192.168.101.42 host-record=infra-3.${DOMAIN},192.168.101.43 # A Records(etcd) host-record=etcd-0.${DOMAIN},192.168.101.21 host-record=etcd-1.${DOMAIN},192.168.101.22 host-record=etcd-2.${DOMAIN},192.168.101.23 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-0.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-1.${DOMAIN},2380,0,10 srv-host=_etcd-server-ssl._tcp.${DOMAIN},etcd-2.${DOMAIN},2380,0,10 # PTRs address=/apps.${DOMAIN}/192.168.101.9 address=/.apps.${DOMAIN}/192.168.101.9 address=/api.${DOMAIN}/192.168.101.9 address=/api-int.${DOMAIN}/192.168.101.9 address=/bastion.${DOMAIN}/192.168.101.9 address=/bootstrap.${DOMAIN}/192.168.101.10 address=/master-1.${DOMAIN}/192.168.101.21 address=/master-2.${DOMAIN}/192.168.101.22 address=/master-3.${DOMAIN}/192.168.101.23 address=/etcd-0.${DOMAIN}/192.168.101.21 address=/etcd-1.${DOMAIN}/192.168.101.22 address=/etcd-2.${DOMAIN}/192.168.101.23 address=/worker-1.${DOMAIN}/192.168.101.31 address=/worker-2.${DOMAIN}/192.168.101.32 address=/infra-1.${DOMAIN}/192.168.101.41 address=/infra-2.${DOMAIN}/192.168.101.42 address=/infra-3.${DOMAIN}/192.168.101.43 EOF   啟動 dnsmasq 服務，並檢測 dnsmasq 設定是否正確:\n1 2 3  $ systemctl enable dnsmasq; systemctl start dnsmasq $ dnsmasq --test dnsmasq: syntax check OK.   編輯 /etc/resolv.conf 檔案，修改 nameserver:\n1 2 3  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; /etc/resolv.conf nameserver 127.0.0.1 EOF   驗證 DNS server 正反解:\n1 2 3 4 5  # dig domain @dns_server $ dig bastion.${DOMAIN} @192.168.101.9 # dig -x ip @dns_server $ dig -x 192.168.101.9 @192.168.101.9    請驗證填寫自己對應的 ip address\n 安裝與設定 HTTP Server 安裝與設定 httpd 套件:\n1  $ yum -y install httpd   編輯 /etc/httpd/conf/httpd.conf 設定檔:\n1 2  #Listen 12.34.56.78:80 Listen 8080\t   因為 80 是 Ingress-http 使用，除非將 HAProxy 拆成不同節點或用 External LB。\n 啟動 httpd 服務:\n1  $ systemctl enable httpd; systemctl start httpd   產生 OpenShift Ignition fils 建立放置 OpenShift Config 檔案的目錄:\n1  $ cd $HOME ; mkdir $HOME/ocp4/ignition;cd $HOME/ocp4/ignition   建立與編輯 install-config.yaml 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v1baseDomain:yiylin.internalcompute:- hyperthreading:Enabledname:workerreplicas:0controlPlane:hyperthreading:Enabledname:masterreplicas:3metadata:name:labnetworking:clusterNetworks:- cidr:10.128.0.0/14hostPrefix:24networkType:OpenShiftSDNserviceNetwork:- 172.3.0.0/16platform:none:{}pullSecret:\u0026#39;{\u0026#34;auths\u0026#34;:{ YOUR_PULL_SECRETE }}\u0026#39;sshKey:\u0026#34;SSH_PUB_KEY\u0026#34;    Pull secret 可在 Create Cluster by user provisioned 取得。 Proxy 設定，參考 Configuring the cluster-wide proxy during installation。 SSH Key 請自行於 bastion 創建填寫，這裡不多做說明   產生 manifests 相關檔案:\n1 2 3 4  $ cp install-config.yaml install-config.yaml.bak $ openshift-install create manifests --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition manifests openshift   編輯 manifests/cluster-scheduler-02-config.yml 檔案，設定 mastersSchedulable 值，以確保 master 不作為工作負載節點:\n1 2 3 4  ...spec:mastersSchedulable:false...  產生 OCP 使用的 Ignition files:\n1 2 3  $ openshift-install create ignition-configs --dir=$HOME/ocp4/ignition $ ls $HOME/ocp4/ignition auth bootstrap.ign master.ign metadata.json worker.ign   撰寫方面安裝 RHCOS 的腳本 $HOME/ocp4/ignition/coreos-install.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/bin/bash  set -eu export DEVICE_NAME=${DEVICE_NAME:-/dev/sda} export IGNITION_FILE=${IGNITION_FILE:-bootstrap.ign} # bootstrap.ign master.ign worker.ign  export IGNITION_URL=${IGNITION_URL:-http://192.168.101.9:8080/ignition/${IGNITION_FILE}} sudo coreos-installer install ${DEVICE_NAME} \\  --insecure-ignition \\  --ignition-url=${IGNITION_URL} \\  --firstboot-args rd.neednet=1 \\  --copy-network   複製 *.ign 與腳本 到 /var/www/html/ignition 目錄:\n1 2 3 4 5 6  $ mkdir /var/www/html/ignition $ cp -rp $HOME/ocp4/ignition/{*.ign,coreos-install.sh} /var/www/html/ignition $ ls /var/www/html/ignition bootstrap.ign master.ign worker.ign coreos-install.sh $ chmod 664 -R /var/www/html/ignition/*   Bootstrap 透過 PVE 建立 Bootstrap 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   設定網卡資訊:\n 確保網卡 IP address 設定為 xx.xx.xx.xx/24，若沒有設定 Network mask 的話，就會用預設 xx.xx.xx.xx/8。\n 重新啟動網卡設定:\n透過以下指令檢查設定結果:\n1  $ ip -4 a    確認網路與 DNS 都能夠被解析到。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Bootstrap 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Bootstrap 開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be INFO Waiting up to 20m0s for the Kubernetes API at https://api.lab.yiylin.internal:6443... INFO API v1.21.1+051ac4f up INFO Waiting up to 30m0s for bootstrapping to complete... # 確認 openshift-api-server 以及 machine-config-server LB 可以通 $ curl -k https://api-int.${DOMAIN}:6443 $ curl -k https://api-int.${DOMAIN}:22623/config/master | jq .   當看到此訊息時，就能並往下進行安裝 Masters。若等待過久，可以透過 bastion 進入 bootstrap 節點查看狀況:\n1 2 3 4 5  $ ssh core@bootstrap.lab.yiylin.internal core $ sudo crictl pods core $ sudo crictl ps -a core $ sudo journalctl -b -f -u bootkube.service   正常情況 Pods 要如以下:\n1 2 3 4 5 6 7 8  POD ID CREATED STATE NAME NAMESPACE ATTEMPT RUNTIME 8101e41b16f9e 52 seconds ago Ready bootstrap-kube-scheduler-localhost kube-system 0 (default) 907d0a933771a 52 seconds ago Ready bootstrap-kube-controller-manager-localhost kube-system 0 (default) a8b709f7caeaa 52 seconds ago Ready bootstrap-kube-apiserver-localhost kube-system 0 (default) 4eb793b3d1844 52 seconds ago Ready cloud-credential-operator-localhost openshift-cloud-credential-operator 0 (default) 49a1cdb2b25f9 52 seconds ago Ready bootstrap-cluster-version-operator-localhost openshift-cluster-version 0 (default) c5098e42b5aa5 About a minute ago Ready bootstrap-machine-config-operator-localhost default 0 (default) e7a3dbc6e0bc0 About a minute ago Ready etcd-bootstrap-member-localhost openshift-etcd 0 (default)   Maters 透過 vCenter 建立 Master 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui   \u0026gt; 網路參考 Bootstrap 設定流程。\n確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Masters 節點:\n1 2 3 4 5 6 7  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=master.ign $ ./coreos-install.sh # 安裝完後 $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Masters 都開啟完成後，回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6  $ openshift-install wait-for bootstrap-complete --dir=$HOME/ocp4/ignition --log-level debug ... INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Bootstrap status: complete INFO It is now safe to remove the bootstrap resources INFO Time elapsed: 0s   在 bastion 執行以下指令來查看叢集狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ cp $HOME/ocp4/ignition/auth/kubeconfig $HOME/ocp4/kubeconfig $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-9xqqd 25m kubernetes.io/kubelet-serving system:node:etcd-1 Approved,Issued csr-gzvwr 27m kubernetes.io/kubelet-serving system:node:etcd-0 Approved,Issued csr-j6npt 25m kubernetes.io/kubelet-serving system:node:etcd-2 Approved,Issued csr-mxqkf 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-tth9f 28m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued csr-zx9q8 25m kubernetes.io/kube-apiserver-client-kubelet system:serviceaccount:openshift-machine-config-operator:node-bootstrapper Approved,Issued system:openshift:openshift-authenticator 25m kubernetes.io/kube-apiserver-client system:serviceaccount:openshift-authentication-operator:authentication-operator Approved,Issued $ oc get no NAME STATUS ROLES AGE VERSION master-1.lab.yiylin.internal Ready master 31m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 15m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 10m v1.21.1+051ac4f   完成後，即可進行 Workers 與 Infras 部署，並移除 Bootstrap。若等待過久，可以透過 bastion 進入 masters 節點查看狀況:\n1  $ ssh core@maste-1.lab.yiylin.internal   Workers \u0026amp; Infras 透過 vCenter 建立 Workers \u0026amp; Infras 節點，並掛載 RHCOS Live ISO，並進入 Console 執行以下操作。\n首先進入系統後，先設定 Network 資訊:\n1  $ sudo nmtui    網路參考 Bootstrap 設定流程。\n 確認沒問題後，執行以下指令來安裝 RHCOS，並設定為 Workers \u0026amp; Infras 節點:\n1 2 3 4 5 6 7 8  $ curl http://192.168.101.9:8080/ignition/coreos-install.sh --output coreos-install.sh $ chmod u+x coreos-install.sh $ export IGNITION_FILE=worker.ign $ ./coreos-install.sh # 安裝完成後 $ lsblk -f $ reboot    安裝沒問題後，重新啟動機器，並將 CD-ROM 移除。\n Workers \u0026amp; Infras 都開啟完成後，就可以回到 bastion 節點執行以下指令來檢查部署狀況:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ openshift-install wait-for install-complete --dir=$HOME/ocp4/ignition --log-level debug DEBUG OpenShift Installer 4.8.2 DEBUG Built from commit a5ddd2dd6c72d8a5ea0a5f17acd8b964b6a3d1be DEBUG Loading Install Config... DEBUG Loading SSH Key... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Cluster Name... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Networking... DEBUG Loading Platform... DEBUG Loading Pull Secret... DEBUG Loading Platform... DEBUG Using Install Config loaded from state file INFO Waiting up to 40m0s for the cluster at https://api.lab.yiylin.internal:6443 to initialize... DEBUG Still waiting for the cluster to initialize: Some cluster operators are still updating: authentication, console, ingress, monitoring    出現 DEBUG 即可往下操作，因為這部分會需要下載 Images 會比較久\n 開啟一個新的 Terminal，在 bastion 節點 Approve workers CSR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get csr -ojson | jq -r .items[] | select(.status == {} ) | .metadata.name | xargs oc adm certificate approve # 確認機器都起來 $ oc get no NAME STATUS ROLES AGE VERSION infra0 Ready worker 12m v1.21.1+051ac4f infra1 Ready worker 12m v1.21.1+051ac4f infra2 Ready worker 12m v1.21.1+051ac4f master0 Ready master 11h v1.21.1+051ac4f master1 Ready master 11h v1.21.1+051ac4f master2 Ready master 11h v1.21.1+051ac4f worker0 Ready worker 12m v1.21.1+051ac4f worker1 Ready worker 12m v1.21.1+051ac4f worker2 Ready worker 12m v1.21.1+051ac4f worker3 Ready worker 12m v1.21.1+051ac4f   搬移資源至 Infra nodes 接著要將 Worker 與 Infra 角色做拆分，確保系統與 Operator 服務不影響業務服務的負載，參考 Creating infrastructure machine sets\n設定 infra nodes 的 labels:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ for i in {1..2}; do oc label node worker-${i} node-role.kubernetes.io/app= --overwrite done $ for i in {1..3}; do oc label node infra-${i} node-role.kubernetes.io/infra= --overwrite oc label node infra-${i} node-role.kubernetes.io/worker- done $ oc get no NAME STATUS ROLES AGE VERSION infra-1.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-2.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f infra-3.lab.yiylin.internal Ready infra 31m v1.21.1+051ac4f master-1.lab.yiylin.internal Ready master 156m v1.21.1+051ac4f master-2.lab.yiylin.internal Ready master 140m v1.21.1+051ac4f master-3.lab.yiylin.internal Ready master 135m v1.21.1+051ac4f worker-1.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f worker-2.lab.yiylin.internal Ready app,worker 31m v1.21.1+051ac4f   編輯 Scheduler 物件，並調整預設 NodeSelector:\n1 2 3 4 5  $ oc edit scheduler cluster ... spec: defaultNodeSelector: node-role.kubernetes.io/app= ...   建立 Infra 機器的 Machine Config Pool 檔案:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ cat \u0026amp;lt;\u0026amp;lt;EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - key: machineconfiguration.openshift.io/role operator: In values: - worker - infra nodeSelector: matchLabels: node-role.kubernetes.io/infra: EOF $ oc get mcp    這時 Infra 節點會重新啟動。\n 搬移 Router 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10  $ oc edit IngressController default -n openshift-ingress-operator ... spec: nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/infra: replicas: 2 $ oc -n openshift-ingress get pod -o wide   搬移 Image Registry 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8  $ oc edit configs.imageregistry.operator.openshift.io/cluster ... spec: nodeSelector: node-role.kubernetes.io/infra: ... $ oc -n openshift-image-registry get pods -o wide   搬移 Monitoring 元件至 Infra 節點上:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  $ cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; \u0026gt; oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: grafana: nodeSelector: node-role.kubernetes.io/infra: k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: telemeterClient: nodeSelector: node-role.kubernetes.io/infra: openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: EOF $ watch -n 0.1 oc -n openshift-monitoring get pod -o wide   \u0026gt; 其他資源請參考 Moving OpenShift Logging resources 搬移。\n叢集初始化會需要一點時間，完成後就可以透過 Console 查看整體狀況。\nWeb Console 透過 oc 取得 Web console 的 route URL:\n1 2  $ oc -n openshift-console get route console -o=jsonpath={.spec.host}{\\n} console-openshift-console.apps.lab.yiylin.internal   取得 OpenShift 的 kubeadmin 密碼:\n1  $ cat $HOME/ocp4/ignition/auth/kubeadmin-password    Terminal Login 開啟一個新的 Terminal，在 bastion 節點配置 KUBECONFIG\n1 2  $ export KUBECONFIG=$HOME/ocp4/kubeconfig $ oc get no   References  https://github.com/openshift/machine-config-operator/blob/master/docs/custom-pools.md https://gist.github.com/kairen/894348c6281f33a981b528e69c3d06bf https://docs.openshift.com/container-platform/4.8/installing/installing_vsphere/installing-restricted-networks-vsphere.html Networking Guide - BIND (Berkeley Internet Name Domain) 參考 IPI Prerequisites  ","date":"2021-09-26T14:54:40+08:00","permalink":"https://blog.yylin.io/openshift/ocp4-install/","title":"OpenShift 4.8.x UPI install on Bare metal"}]